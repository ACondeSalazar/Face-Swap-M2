{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19fe4e92",
   "metadata": {},
   "source": [
    "# Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac13488",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "from facenet_pytorch import InceptionResnetV1\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c8d572",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568e4af4",
   "metadata": {},
   "source": [
    "### Check si on peut utiliser cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac21ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cuda compatible : \", torch.cuda.is_available())\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86e2968",
   "metadata": {},
   "source": [
    "# Chargement du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694ec5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 128\n",
    "NB_IMAGES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c87f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset, nb_classes = dataloader.make_dataset(\"../data/Face-Swap-M2-Dataset/dataset\", NB_IMAGES, IMAGE_SIZE, 0.8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c9016c",
   "metadata": {},
   "source": [
    "# Paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412f36a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EMBEDDING_SIZE = 128\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "EPOCHS = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcb3c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11670215",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in train_loader:\n",
    "    print(\"Batch shape:\", images.shape)\n",
    "    print(\"Min pixel value:\", images.min().item())\n",
    "    print(\"Max pixel value:\", images.max().item())\n",
    "    print(\"Mean pixel value:\", images.mean().item())\n",
    "    break  # only check one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80aa450f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"train samples: {len(train_dataset)}, test samples: {len(test_dataset)}, nb_classes: {nb_classes}\")\n",
    "\n",
    "class_counts = np.bincount([label for _, label in train_dataset])\n",
    "for cls, count in enumerate(class_counts):\n",
    "    print(f\"train Class {cls}: {count} samples\")\n",
    "\n",
    "class_counts = np.bincount([label for _, label in test_dataset])\n",
    "for cls, count in enumerate(class_counts):\n",
    "    print(f\"Test Class {cls}: {count} samples\")\n",
    "\n",
    "images, labels = next(iter(train_loader))\n",
    "images = images.cpu()\n",
    "labels = labels.cpu().numpy()\n",
    "\n",
    "n = min(8, images.size(0))\n",
    "imgs = images[:n]\n",
    "labs = labels[:n]\n",
    "\n",
    "fig, axs = plt.subplots(2, 4, figsize=(12, 6))\n",
    "axs = axs.flatten()\n",
    "for i, ax in enumerate(axs):\n",
    "    if i < n:\n",
    "        img = imgs[i].permute(1, 2, 0).numpy()\n",
    "        mi, ma = img.min(), img.max()\n",
    "        img_disp = (img - mi) / (ma - mi) if ma - mi > 1e-6 else img\n",
    "        ax.imshow(img_disp)\n",
    "        ax.set_title(f\"label: {labs[i]}\")\n",
    "    ax.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a2762a",
   "metadata": {},
   "source": [
    "# Définition des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174d6ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, 5), # 3 car image RGB, 6 en sortie (6 filtres) , taille noyau de 5\n",
    "            nn.BatchNorm2d(8), # empeche le collapse des embeddings\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # reduit par 2 la taille\n",
    "            nn.Conv2d(8, 16, 5),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(16 *29*29, EMBEDDING_SIZE )\n",
    "\n",
    "    def forward(self,x):\n",
    "        res = self.conv(x)\n",
    "        res = torch.flatten(res,1)\n",
    "        res = self.fc(res)\n",
    "        res = res / res.norm(dim= 1, keepdim=True) # normalise la sortie, donne de meilleur resultat\n",
    "        return res\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(EMBEDDING_SIZE, num_classes)\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "    \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            # Input: (128, 1, 1)\n",
    "            nn.ConvTranspose2d(EMBEDDING_SIZE, 512, kernel_size=4, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x.unflatten(1, (EMBEDDING_SIZE, 1, 1))\n",
    "        res = self.decoder(res)\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179a127e",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651e2551",
   "metadata": {},
   "source": [
    "## Entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9882f24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "faceEmbedder = Embedder().to(device)\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, faceEmbedder.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(f\"Embedder has {params} parameters\")\n",
    "\n",
    "triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2, eps=1e-7)\n",
    "optimizer = optim.Adam(faceEmbedder.parameters(), lr=0.0001)\n",
    "\n",
    "final_embedder_loss = 0.0\n",
    "\n",
    "faceEmbedder.train()\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        #images = torch.flatten(images, 1)\n",
    "\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        output = faceEmbedder(images)\n",
    "        anchor_idx, positive_idx, negative_idx = utils.get_triplets(labels)\n",
    "\n",
    "        anchors = output[anchor_idx]\n",
    "        positives = output[positive_idx]\n",
    "        negatives = output[negative_idx]\n",
    "\n",
    "        loss = triplet_loss(anchors,positives, negatives)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    total_loss = total_loss / len(train_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "    final_embedder_loss = total_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(\"Embedder final loss : \", final_embedder_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fe2983",
   "metadata": {},
   "source": [
    "## Analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1446f2",
   "metadata": {},
   "source": [
    "### Calcul tous les embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cae7e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "faceEmbedder.eval()\n",
    "embeddings = []\n",
    "avg_embeddings = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in train_loader:\n",
    "        #print(labels)\n",
    "        #images = torch.flatten(images, 1)\n",
    "\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        output = faceEmbedder(images)\n",
    "\n",
    "        for i in range(output.size(0)):\n",
    "            embeddings.append([output[i].cpu().tolist(), labels[i].cpu().item()])\n",
    "\n",
    "    emb_np = np.array([item[0] for item in embeddings])\n",
    "    lab_np = np.array([item[1] for item in embeddings])\n",
    "\n",
    "\n",
    "\n",
    "    for cls in range(nb_classes):\n",
    "        idx = np.where(lab_np == cls)[0]\n",
    "        avg_emb = emb_np[idx].mean(axis=0)\n",
    "\n",
    "        avg_embeddings.append(avg_emb)\n",
    "\n",
    "        print(f\"classe {cls}: avg embedding = {avg_emb.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871fffb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "faceEmbedder.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "avg_embeddings = torch.Tensor(avg_embeddings).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in DataLoader(test_dataset, batch_size=1):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        output = faceEmbedder(images)\n",
    "\n",
    "        min_dist = math.inf\n",
    "        i_min = -1\n",
    "        for i in range(len(avg_embeddings)):\n",
    "            d = torch.dist(torch.Tensor(output[0]).unsqueeze(0), avg_embeddings[i])\n",
    "            #print(d)\n",
    "            if (d.item() < min_dist):\n",
    "                i_min = i\n",
    "                min_dist = d\n",
    "        \n",
    "        #print(f\"closest is {i_min}, true value is {labels[0]}\")\n",
    "\n",
    "        if i_min == labels[0]:\n",
    "            correct += 1\n",
    "\n",
    "        total+= labels.size(0)\n",
    "\n",
    "print(f\"accuracy : {float(correct) / len(test_loader)}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e260c224",
   "metadata": {},
   "source": [
    "### Plot embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9e78ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embeddings():\n",
    "    embeddings_np = np.array([e[0] for e in embeddings])\n",
    "    labels_np = np.array([e[1] for e in embeddings])\n",
    "\n",
    "    pca = PCA(n_components=2) #reduction en 2D\n",
    "    reduced_embeddings = pca.fit_transform(embeddings_np)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for label in np.unique(labels_np):\n",
    "        idx = labels_np == label\n",
    "        plt.scatter(reduced_embeddings[idx, 0], reduced_embeddings[idx, 1], label=f\"classe {label}\")\n",
    "\n",
    "    plt.title(\"embeddings\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "plot_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a39c480",
   "metadata": {},
   "source": [
    "# Classifieur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dddb828",
   "metadata": {},
   "source": [
    "## Entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3137fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classifier = Classifier(nb_classes).to(device)\n",
    "crossLoss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n",
    "\n",
    "final_classifier_loss = [0.0,0.0]\n",
    "\n",
    "losses = []\n",
    "\n",
    "faceEmbedder.eval()\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0.0\n",
    "    classifier.train()\n",
    "    for images, labels in train_loader:\n",
    "        #images = torch.flatten(images, 1)\n",
    "\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        embeded = faceEmbedder(images)\n",
    "\n",
    "        output = classifier(embeded)\n",
    "        loss = crossLoss(output, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "\n",
    "    #test avec le set de validation\n",
    "    classifier.eval()\n",
    "    total_eval_loss = 0.0\n",
    "    for images, labels in test_loader:\n",
    "        #images = torch.flatten(images, 1)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        embeded = faceEmbedder(images)\n",
    "\n",
    "        output = classifier(embeded)\n",
    "        loss = crossLoss(output, labels)\n",
    "\n",
    "        #metrics\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "\n",
    "    total_loss = total_loss / len(train_loader)\n",
    "    total_eval_loss = total_eval_loss / len(test_loader)\n",
    "    print(f\"Epoch {epoch+1}, training loss: {total_loss:.4f}, test loss: {total_eval_loss:.4f}\")\n",
    "\n",
    "    losses.append((total_loss, total_eval_loss))\n",
    "\n",
    "    final_classifier_loss[0] = total_loss\n",
    "    final_classifier_loss[1] = total_eval_loss\n",
    "\n",
    "\n",
    "\n",
    "#print(\"Classifier final train loss : \", final_classifier_loss[0], \" test loss : \", final_classifier_loss[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a718a7",
   "metadata": {},
   "source": [
    "## Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55b9416",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses, label=\"Loss\")\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend([\"Train loss\", \"Test loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58ad2e6",
   "metadata": {},
   "source": [
    "### Plot la matrice de confusion et accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7cf16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix():\n",
    "    classifier.eval()\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            #images = torch.flatten(images, 1)\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            embeded = faceEmbedder(images)\n",
    "            output = classifier(embeded)\n",
    "            predictions = torch.argmax(output, dim=1)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_predictions, labels=range(nb_classes))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[f\"classe {i}\" for i in range(nb_classes)])\n",
    "    disp.plot()\n",
    "    plt.title(\"Matrice de confusion\")\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix()\n",
    "\n",
    "\n",
    "classifier.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        #images = torch.flatten(images, 1).to(device)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        logits = classifier(faceEmbedder(images))\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "accuracy = correct / total if total > 0 else 0.0\n",
    "print(f\"accuracy on test set: {accuracy * 100:.2f}% ({correct}/{total})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f357d2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "faceEmbedder.eval()\n",
    "classifier.eval()\n",
    "\n",
    "num_display = 16 \n",
    "cols = 4\n",
    "rows = math.ceil(num_display / cols)\n",
    "\n",
    "imgs_to_show = []\n",
    "trues = []\n",
    "preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images_cpu = images.cpu()\n",
    "        images_dev = images.to(device)\n",
    "        labels_dev = labels.to(device)\n",
    "\n",
    "        embeddings = faceEmbedder(images_dev)\n",
    "        logits = classifier(embeddings)\n",
    "        batch_preds = logits.argmax(dim=1).cpu().numpy()\n",
    "        batch_labels = labels_dev.cpu().numpy()\n",
    "\n",
    "        for i in range(images_cpu.size(0)):\n",
    "            imgs_to_show.append(images_cpu[i].permute(1, 2, 0).numpy())\n",
    "            trues.append(int(batch_labels[i]))\n",
    "            preds.append(int(batch_preds[i]))\n",
    "            if len(imgs_to_show) >= num_display:\n",
    "                break\n",
    "        if len(imgs_to_show) >= num_display:\n",
    "            break\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(4 * cols, 4 * rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.axis(\"off\")\n",
    "    if i < len(imgs_to_show):\n",
    "        img = imgs_to_show[i]\n",
    "        mi, ma = img.min(), img.max()\n",
    "        denom = (ma - mi) if (ma - mi) > 1e-6 else 1.0\n",
    "        img_disp = (img - mi) / denom\n",
    "        ax.imshow(img_disp)\n",
    "        true = trues[i]\n",
    "        pred = preds[i]\n",
    "        color = \"green\" if pred == true else \"red\"\n",
    "        ax.set_title(f\"P: {pred}  /  T: {true}\", color=color, fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b7cc41",
   "metadata": {},
   "source": [
    "# VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ad742f",
   "metadata": {},
   "source": [
    "## Entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcf4ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder().to(device)\n",
    "mseLoss = nn.MSELoss()\n",
    "optimizer = optim.Adam(decoder.parameters(), lr=0.0001)\n",
    "\n",
    "final_classifier_loss = [0.0,0.0]\n",
    "\n",
    "losses = []\n",
    "\n",
    "faceEmbedder.eval()\n",
    "for epoch in range(250):\n",
    "    total_loss = 0.0\n",
    "    decoder.train()\n",
    "    for images, labels in train_loader:\n",
    "        #images = torch.flatten(images, 1)\n",
    "\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        embeded = faceEmbedder(images)\n",
    "        #print(images.size())\n",
    "\n",
    "        output = decoder(embeded)\n",
    "        #print(output.size())\n",
    "\n",
    "        loss = mseLoss(output, images)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "\n",
    "    #test avec le set de validation\n",
    "    decoder.eval()\n",
    "    total_eval_loss = 0.0\n",
    "    for images, labels in test_loader:\n",
    "        #images = torch.flatten(images, 1)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        embeded = faceEmbedder(images)\n",
    "\n",
    "        output = decoder(embeded)\n",
    "        loss = mseLoss(output, images)\n",
    "\n",
    "        #metrics\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "\n",
    "    total_loss = total_loss / len(train_loader)\n",
    "    total_eval_loss = total_eval_loss / len(test_loader)\n",
    "    print(f\"Epoch {epoch+1}, training loss: {total_loss:.4f}, test loss: {total_eval_loss:.4f}\")\n",
    "\n",
    "    losses.append((total_loss, total_eval_loss))\n",
    "\n",
    "    final_classifier_loss[0] = total_loss\n",
    "    final_classifier_loss[1] = total_eval_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6fd47b",
   "metadata": {},
   "source": [
    "## Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da94aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses, label=\"Loss\")\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend([\"Train loss\", \"Test loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b58ea55",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.eval()\n",
    "faceEmbedder.eval()\n",
    "\n",
    "images, labels = next(iter(test_loader))\n",
    "images = images.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    embeddings = faceEmbedder(images)\n",
    "\n",
    "    reconstructed_images = decoder(embeddings)\n",
    "\n",
    "\n",
    "n = min(8, images.size(0))\n",
    "fig, axs = plt.subplots(2, n, figsize=(15, 5))\n",
    "\n",
    "for i in range(n):\n",
    "    img = images[i].permute(1, 2, 0).cpu().numpy()\n",
    "    mi, ma = img.min(), img.max()\n",
    "    img_disp = (img - mi) / (ma - mi) if ma - mi > 1e-6 else img\n",
    "    axs[0, i].imshow(img_disp)\n",
    "    axs[0, i].set_title(f\"Original (Label: {labels[i].item()})\")\n",
    "    axs[0, i].axis(\"off\")\n",
    "\n",
    "    recon_img = reconstructed_images[i].permute(1, 2, 0).cpu().numpy()\n",
    "    mi, ma = recon_img.min(), recon_img.max()\n",
    "    recon_disp = (recon_img - mi) / (ma - mi) if ma - mi > 1e-6 else recon_img\n",
    "    axs[1, i].imshow(recon_disp)\n",
    "    axs[1, i].set_title(\"Reconstructed\")\n",
    "    axs[1, i].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2747f3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.eval()\n",
    "\n",
    "random_embeddings = torch.tensor(avg_embeddings, device=device)  # Convert avg_embeddings to a PyTorch tensor\n",
    "\n",
    "random_embeddings = random_embeddings.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_images = decoder(random_embeddings)\n",
    "\n",
    "n = generated_images.size(0)\n",
    "fig, axs = plt.subplots(1, n, figsize=(15, 5))\n",
    "\n",
    "for i in range(n):\n",
    "    img = generated_images[i].permute(1, 2, 0).cpu().numpy()\n",
    "    mi, ma = img.min(), img.max()\n",
    "    img_disp = (img - mi) / (ma - mi) if ma - mi > 1e-6 else img\n",
    "    axs[i].imshow(img_disp)\n",
    "    axs[i].axis(\"off\")\n",
    "    axs[i].set_title(f\"Generated {i+1}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
