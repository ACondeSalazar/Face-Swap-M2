{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26a46ac4",
   "metadata": {},
   "source": [
    "# FSGAN Pipeline Setup & Testing\n",
    "\n",
    "This notebook sets up and tests the pretrained FSGAN models for face swapping.\n",
    "\n",
    "**Contents:**\n",
    "1. Environment setup and imports\n",
    "2. Load pretrained models (reenactment, segmentation, landmarks)\n",
    "3. Test individual components\n",
    "4. Run full face swap pipeline\n",
    "5. Analyze pipeline quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7b0892",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038d4b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ffmpeg-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb60f2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, subprocess, importlib\n",
    "pkgs = ['torch', 'torchvision', 'numpy', 'opencv_python', 'tqdm', 'matplotlib']\n",
    "missing = []\n",
    "for p in pkgs:\n",
    "    try:\n",
    "        importlib.import_module(p)\n",
    "    except Exception:\n",
    "        missing.append(p)\n",
    "\n",
    "if missing:\n",
    "    print('Missing packages:', missing)\n",
    "    print('Installing missing packages into the current Python environment. This may take a few minutes.')\n",
    "    cmd = [sys.executable, '-m', 'pip', 'install'] + missing\n",
    "    subprocess.check_call(cmd)\n",
    "else:\n",
    "    print('All minimal packages present')\n",
    "\n",
    "\n",
    "import os\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from fsgan.utils.utils import load_model\n",
    "from fsgan.utils import img_utils, landmarks_utils\n",
    "from fsgan.models.hrnet import hrnet_wlfw\n",
    "from fsgan.inference import swap as swap_mod\n",
    "\n",
    "from fsgan.notebook_helpers.reenact_preprocess import run_full_pipeline\n",
    "\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "ROOT = Path('.')\n",
    "WEIGHTS_DIR = ROOT / 'fsgan' / 'weights'\n",
    "OUT_DIR = ROOT / 'outputs'\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "IMG_A = ROOT / 'j.jpg'\n",
    "IMG_J = ROOT / 'a.jpg'\n",
    "print('Weights directory:', WEIGHTS_DIR)\n",
    "print('Expecting images at:', IMG_A, IMG_J)\n",
    "print('Outputs will be saved to:', OUT_DIR)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device', device)\n",
    "\n",
    "def load_image_as_tensor(p, size=256, device=None):\n",
    "    im = cv2.imread(str(p))\n",
    "    if im is None:\n",
    "        raise FileNotFoundError(f'Image not found: {p}')\n",
    "    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "    im = cv2.resize(im, (size, size), interpolation=cv2.INTER_AREA)\n",
    "    im = im.astype('float32') / 255.0\n",
    "    t = torch.from_numpy(im.transpose(2,0,1)).unsqueeze(0)\n",
    "    if device is not None:\n",
    "        t = t.to(device)\n",
    "    return t\n",
    "\n",
    "def read_bgr_tensor(p, device=device, size=256):\n",
    "    im = cv2.imread(str(p))\n",
    "    if im is None:\n",
    "        raise FileNotFoundError(f'Image not found: {p}')\n",
    "    if size is not None:\n",
    "        im = cv2.resize(im, (size, size), interpolation=cv2.INTER_AREA)\n",
    "    return img_utils.bgr2tensor(im, normalize=False).to(device)\n",
    "\n",
    "def save_mask(mask, outpath):\n",
    "    cv2.imwrite(str(outpath), mask)\n",
    "\n",
    "print('Top-level imports and helpers are ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cf0622",
   "metadata": {},
   "source": [
    "## 2. Test Individual Components\n",
    "\n",
    "### Segmentation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4427d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_w = WEIGHTS_DIR / 'celeba_unet_256_1_2_segmentation_v2.pth'\n",
    "out_seg_dir = OUT_DIR / 'segmentation'\n",
    "out_seg_dir.mkdir(exist_ok=True)\n",
    "try:\n",
    "    model_seg = load_model(str(seg_w), 'segmentation', device=device)\n",
    "    model_seg.eval()\n",
    "    t = load_image_as_tensor(IMG_A, size=256, device=device)\n",
    "    with torch.no_grad():\n",
    "        pred = model_seg(t)\n",
    "\n",
    "    pred_np = pred.detach().cpu().numpy()\n",
    "    if pred_np.ndim == 4:\n",
    "        mask = pred_np.argmax(1)[0].astype('uint8') * 85 \n",
    "    else:\n",
    "        mask = (pred_np[0,0] * 255).astype('uint8')\n",
    "    out_path = out_seg_dir / 'a_seg_mask.png'\n",
    "    save_mask(mask, out_path)\n",
    "    print('✓ Segmentation saved to', out_path)\n",
    "except Exception as e:\n",
    "    print('✗ Segmentation test failed:', e)\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4baa76",
   "metadata": {},
   "source": [
    "### Landmarks Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488b133e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lms_w = WEIGHTS_DIR / 'hr18_wflw_landmarks.pth'\n",
    "out_lms_dir = OUT_DIR / 'landmarks'\n",
    "out_lms_dir.mkdir(exist_ok=True)\n",
    "try:\n",
    "    model_lms = load_model(str(lms_w), 'landmarks', device=device)\n",
    "    model_lms.eval()\n",
    "    t = load_image_as_tensor(IMG_A, size=256, device=device)\n",
    "    with torch.no_grad():\n",
    "        out = model_lms(t)\n",
    "    print('Landmarks model forward output shape:', getattr(out, 'shape', None))\n",
    "\n",
    "    try:\n",
    "        out_np = out.detach().cpu().numpy()\n",
    "        if out_np.ndim == 4:\n",
    "            hm = out_np[0,0]\n",
    "            hm = (255 * (hm - hm.min()) / (hm.max() - hm.min() + 1e-8)).astype('uint8')\n",
    "            cv2.imwrite(str(out_lms_dir / 'a_landmark_heatmap_ch0.png'), hm)\n",
    "            print('✓ Saved example landmark heatmap to', out_lms_dir / 'a_landmark_heatmap_ch0.png')\n",
    "    except Exception as e:\n",
    "        print('Could not save landmark heatmap:', e)\n",
    "except Exception as e:\n",
    "    print('✗ Landmarks test failed:', e)\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecc930a",
   "metadata": {},
   "source": [
    "## 3. Full Reenactment Pipeline Test\n",
    "\n",
    "Test the complete face reenactment pipeline with visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceb06ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "OUT_DIR = Path('outputs')\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "src = str(Path('input/tim.jpg'))\n",
    "tgt = str(Path('input/tom.jpg'))\n",
    "out_file = str(OUT_DIR / 'reenact_full_composited.png')\n",
    "\n",
    "print(f\"Running full pipeline: {src} → {tgt}\")\n",
    "result_bgr, intermediates, src_crop, tgt_crop = run_full_pipeline(\n",
    "    src, tgt, \n",
    "    out_path=out_file, \n",
    "    reenact=True, \n",
    "    use_detector=True, \n",
    "    device=device,\n",
    "    crop_scale=1.5, \n",
    "    resolution=256\n",
    ")\n",
    "\n",
    "print('✓ Saved full result to', out_file)\n",
    "\n",
    "try:\n",
    "    img = cv2.cvtColor(result_bgr, cv2.COLOR_BGR2RGB)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "    plt.title('Face Swap Result')\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print('Could not display image inline:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464f2adc",
   "metadata": {},
   "source": [
    "## 4. Visualize Pipeline Intermediates\n",
    "\n",
    "Show all intermediate steps: segmentation, inpainting, blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9a3324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from fsgan.utils.img_utils import tensor2bgr\n",
    "\n",
    "def tensor_to_bgr_uint8(x):\n",
    "    import numpy as _np\n",
    "    import torch as _torch\n",
    "    if isinstance(x, _torch.Tensor):\n",
    "        t = x.detach().cpu()\n",
    "        if t.ndim == 4:\n",
    "            t = t[0]\n",
    "        if t.min() >= -1.1 and t.max() <= 1.1:\n",
    "            return tensor2bgr(t).astype('uint8')\n",
    "        else:\n",
    "            arr = (t.numpy().transpose(1,2,0) * 255.0).clip(0,255).astype('uint8')\n",
    "            return arr[:, :, ::-1]\n",
    "    else:\n",
    "        arr = x.copy()\n",
    "        if arr.dtype != _np.uint8:\n",
    "            arr = arr.astype('uint8')\n",
    "        return arr\n",
    "\n",
    "def seg_to_color(seg_arr):\n",
    "    import numpy as _np\n",
    "    h,w = seg_arr.shape\n",
    "    cmap = _np.array([[0,0,0],[0,255,0],[0,0,255],[255,0,0],[255,255,0]], dtype='uint8')\n",
    "    out = _np.zeros((h,w,3), dtype='uint8')\n",
    "    labels = _np.clip(seg_arr, 0, cmap.shape[0]-1)\n",
    "    for i in range(cmap.shape[0]):\n",
    "        out[labels==i] = cmap[i]\n",
    "    return out\n",
    "\n",
    "# Gather images to display\n",
    "imgs = []\n",
    "titles = []\n",
    "\n",
    "# Preprocessing crops\n",
    "try:\n",
    "    imgs.append(src_crop)\n",
    "    titles.append('Source crop')\n",
    "except Exception:\n",
    "    pass\n",
    "    \n",
    "try:\n",
    "    imgs.append(tgt_crop)\n",
    "    titles.append('Target crop')\n",
    "except Exception:\n",
    "    pass\n",
    "    \n",
    "# Reenact result\n",
    "try:\n",
    "    imgs.append(result_bgr)\n",
    "    titles.append('Final composited')\n",
    "except Exception:\n",
    "    pass\n",
    "    \n",
    "# Reenactment-only (generator output) if available\n",
    "if 'reenact_tensor' in intermediates:\n",
    "    try:\n",
    "        imgs.append(tensor_to_bgr_uint8(intermediates['reenact_tensor']))\n",
    "        titles.append('Reenact')\n",
    "    except Exception:\n",
    "        pass\n",
    "        \n",
    "# Segmentation maps\n",
    "if 'reenact_seg' in intermediates:\n",
    "    s = intermediates['reenact_seg']\n",
    "    try:\n",
    "        s_np = s.detach().cpu().numpy() if hasattr(s, 'detach') else s\n",
    "        if s_np.ndim == 4:\n",
    "            lab = s_np.argmax(1)[0].astype('int')\n",
    "        else:\n",
    "            lab = s_np.astype('int')\n",
    "        imgs.append(seg_to_color(lab)[:,:,::-1])\n",
    "        titles.append('Reenact seg')\n",
    "    except Exception:\n",
    "        pass\n",
    "        \n",
    "if 'tgt_seg' in intermediates:\n",
    "    s = intermediates['tgt_seg']\n",
    "    try:\n",
    "        s_np = s.detach().cpu().numpy() if hasattr(s, 'detach') else s\n",
    "        if s_np.ndim == 4:\n",
    "            lab = s_np.argmax(1)[0].astype('int')\n",
    "        else:\n",
    "            lab = s_np.astype('int')\n",
    "        imgs.append(seg_to_color(lab)[:,:,::-1])\n",
    "        titles.append('Target seg')\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Plot grid\n",
    "n = len(imgs)\n",
    "if n == 0:\n",
    "    print('No intermediates available to display')\n",
    "else:\n",
    "    cols = min(4, n)\n",
    "    rows = math.ceil(n / cols)\n",
    "    plt.figure(figsize=(4*cols, 3*rows))\n",
    "    for i, im in enumerate(imgs):\n",
    "        plt.subplot(rows, cols, i+1)\n",
    "        plt.axis('off')\n",
    "        if im.ndim == 2:\n",
    "            plt.imshow(im, cmap='gray')\n",
    "        else:\n",
    "            plt.imshow(im[:,:,::-1])\n",
    "        plt.title(titles[i])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6c830e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook tested the pretrained FSGAN pipeline:\n",
    "- ✓ Segmentation model working\n",
    "- ✓ Landmarks model working  \n",
    "- ✓ Full reenactment pipeline working\n",
    "\n",
    "**Next steps:**\n",
    "- For multi-subject finetuning: Open `02_multisubject_finetuning.ipynb`\n",
    "- For per-subject finetuning: Open `03_per_subject_finetuning.ipynb`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
