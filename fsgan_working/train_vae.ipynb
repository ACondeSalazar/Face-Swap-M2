{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c75c5a15",
   "metadata": {},
   "source": [
    "# VAE Image Enhancement (Degradation -> Enhance)\n",
    "\n",
    "This notebook trains a small convolutional VAE to \n",
    " images: the model takes a degraded (blurred / downscaled-upscaled) image and reconstructs a higher-quality version at the same resolution.\n",
    "\n",
    "Key parts:\n",
    "- Degradation pipeline: random Gaussian blur + downscale/upscale to mimic low quality.\n",
    "- Dataset returns (low_quality_input, high_quality_target) normalized to [-1, 1].\n",
    "- Simple Conv VAE (encoder -> mu/logvar -> reparam -> decoder).\n",
    "- Loss = L1(recon, target) + beta * KL. For enhancement, beta is small so reconstruction dominates.\n",
    "\n",
    "Edit `DATA_DIR`, `OUT_DIR` and training params below to your needs. Run the cells in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0a991d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "DATA_DIR: ../data/Face-Swap-M2-Dataset/dataset/smaller\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: imports and paths\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "# Paths (edit these)\n",
    "ROOT = Path('.')\n",
    "DATA_DIR = ROOT / '../data' / 'Face-Swap-M2-Dataset' / 'dataset' / 'smaller'  # adjust to your dataset\n",
    "OUT_DIR = ROOT / 'outputs' / 'vae_enhance'\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_DIR = OUT_DIR / 'checkpoints'\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SAMPLES_DIR = OUT_DIR / 'samples'\n",
    "SAMPLES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import dataloader\n",
    "print('Device:', device)\n",
    "print('DATA_DIR:', DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e1f8c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degrader ready\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: dataset + degradation utilities\n",
    "class DegradeTransforms:\n",
    "    def __init__(self, min_scale=0.5, max_scale=0.9, blur_prob=0.5, max_blur_ks=7):\n",
    "        self.min_scale = min_scale\n",
    "        self.max_scale = max_scale\n",
    "        self.blur_prob = blur_prob\n",
    "        self.max_blur_ks = max_blur_ks\n",
    "\n",
    "    def random_down_up(self, img, scale):\n",
    "        # img: HxWxC uint8 RGB\n",
    "        h, w = img.shape[:2]\n",
    "        new_h = max(2, int(h * scale))\n",
    "        new_w = max(2, int(w * scale))\n",
    "        # downscale then upscale\n",
    "        small = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "        up = cv2.resize(small, (w, h), interpolation=cv2.INTER_LINEAR)\n",
    "        return up\n",
    "\n",
    "    def random_blur(self, img):\n",
    "        # random odd kernel size\n",
    "        k = random.randrange(1, self.max_blur_ks, 2) if random.random() < self.blur_prob else 1\n",
    "        if k > 1:\n",
    "            return cv2.GaussianBlur(img, (k, k), 0)\n",
    "        return img\n",
    "\n",
    "    def degrade(self, img):\n",
    "        # img: HxWxC uint8 RGB\n",
    "        scale = random.uniform(self.min_scale, self.max_scale)\n",
    "        out = self.random_down_up(img, scale)\n",
    "        out = self.random_blur(out)\n",
    "        return out\n",
    "\n",
    "class VaeImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, size=256, transform=None, degrade=None, extensions=['.jpg', '.png', '.jpeg']):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.files = [p for p in self.root_dir.rglob('*') if p.suffix.lower() in extensions]\n",
    "        self.size = size\n",
    "        self.transform = transform\n",
    "        self.degrade = degrade\n",
    "        if len(self.files) == 0:\n",
    "            raise RuntimeError(f'No images found in {root_dir}')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def load_image(self, p):\n",
    "        im = cv2.imread(str(p))\n",
    "        if im is None:\n",
    "            raise RuntimeError(f'Could not read {p}')\n",
    "        im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "        im = cv2.resize(im, (self.size, self.size), interpolation=cv2.INTER_AREA)\n",
    "        return im\n",
    "\n",
    "    def to_tensor(self, img):\n",
    "        # img: HxWxC uint8 RGB -> Tensor in [-1,1] CxHxW\n",
    "        t = torch.from_numpy(img.astype('float32') / 255.0).permute(2,0,1)\n",
    "        t = (t - 0.5) / 0.5\n",
    "        return t\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        p = self.files[idx]\n",
    "        tgt = self.load_image(p)\n",
    "        inp_img = tgt.copy()\n",
    "        if self.degrade is not None:\n",
    "            inp_img = self.degrade.degrade(inp_img)\n",
    "        if self.transform is not None:\n",
    "            # allow extra transforms if provided (PIL or numpy aware)\n",
    "            inp_img_pil = Image.fromarray(inp_img)\n",
    "            tgt_pil = Image.fromarray(tgt)\n",
    "            inp_t = self.transform(inp_img_pil)\n",
    "            tgt_t = self.transform(tgt_pil)\n",
    "            # transform expected to keep range in [0,1] or [-1,1] depending on implementation\n",
    "            return inp_t, tgt_t\n",
    "        inp_t = self.to_tensor(inp_img)\n",
    "        tgt_t = self.to_tensor(tgt)\n",
    "        return inp_t, tgt_t\n",
    "\n",
    "# quick test: create dataset instance (not executed until user runs cell)\n",
    "degrader = DegradeTransforms(min_scale=0.25, max_scale=0.5, blur_prob=0.0, max_blur_ks=7)\n",
    "print('Degrader ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce7d62b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model classes defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: VAE model definition\n",
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, in_ch=3, base_ch=64, latent_dim=256):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, base_ch, 4, 2, 1)\n",
    "        self.conv2 = nn.Conv2d(base_ch, base_ch*2, 4, 2, 1)\n",
    "        self.conv3 = nn.Conv2d(base_ch*2, base_ch*4, 4, 2, 1)\n",
    "        self.conv4 = nn.Conv2d(base_ch*4, base_ch*8, 4, 2, 1)\n",
    "        self.act = nn.LeakyReLU(0.2, inplace=True)\n",
    "        # pool to 16x16 so decoder with 4 upsampling layers returns to 256x256\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((16,16))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc_mu = nn.Linear(base_ch*8*16*16, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(base_ch*8*16*16, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.conv1(x))\n",
    "        x = self.act(self.conv2(x))\n",
    "        x = self.act(self.conv3(x))\n",
    "        x = self.act(self.conv4(x))\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = self.flatten(x)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "class ConvDecoder(nn.Module):\n",
    "    def __init__(self, out_ch=3, base_ch=64, latent_dim=256):\n",
    "        super().__init__()\n",
    "        # match encoder's pooled feature size\n",
    "        self.fc = nn.Linear(latent_dim, base_ch*8*16*16)\n",
    "        self.deconv1 = nn.ConvTranspose2d(base_ch*8, base_ch*4, 4, 2, 1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(base_ch*4, base_ch*2, 4, 2, 1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(base_ch*2, base_ch, 4, 2, 1)\n",
    "        self.deconv4 = nn.ConvTranspose2d(base_ch, out_ch, 4, 2, 1)\n",
    "        self.act = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.fc(z)\n",
    "        # reshape to (B, base_ch*8, 16, 16)\n",
    "        x = x.view(x.size(0), -1, 16, 16)\n",
    "        x = self.act(self.deconv1(x))   # -> 32x32\n",
    "        x = self.act(self.deconv2(x))   # -> 64x64\n",
    "        x = self.act(self.deconv3(x))   # -> 128x128\n",
    "        x = self.tanh(self.deconv4(x))  # -> 256x256\n",
    "        return x\n",
    "\n",
    "class SimpleVAE(nn.Module):\n",
    "    def __init__(self, in_ch=3, base_ch=64, latent_dim=256):\n",
    "        super().__init__()\n",
    "        self.encoder = ConvEncoder(in_ch, base_ch, latent_dim)\n",
    "        self.decoder = ConvDecoder(in_ch, base_ch, latent_dim)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = (0.5 * logvar).exp()  # logvar/2 -> std (approx)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon = self.decoder(z)\n",
    "        return recon, mu, logvar\n",
    "\n",
    "print('Model classes defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27a69f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3b: GAN models and training loop (conditional GAN: generator + PatchGAN discriminator)\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_ch=3, out_ch=3, base_ch=64):\n",
    "        super().__init__()\n",
    "        # encoder\n",
    "        self.e1 = nn.Conv2d(in_ch, base_ch, 4, 2, 1)       # 128\n",
    "        self.e2 = nn.Conv2d(base_ch, base_ch*2, 4, 2, 1)   # 64\n",
    "        self.e3 = nn.Conv2d(base_ch*2, base_ch*4, 4, 2, 1) # 32\n",
    "        self.e4 = nn.Conv2d(base_ch*4, base_ch*8, 4, 2, 1) # 16\n",
    "        self.act = nn.LeakyReLU(0.2, inplace=True)\n",
    "        # decoder\n",
    "        self.d1 = nn.ConvTranspose2d(base_ch*8, base_ch*4, 4, 2, 1)   # 32\n",
    "        self.d2 = nn.ConvTranspose2d(base_ch*8, base_ch*2, 4, 2, 1)   # 64 (concat)\n",
    "        self.d3 = nn.ConvTranspose2d(base_ch*4, base_ch, 4, 2, 1)     # 128\n",
    "        self.d4 = nn.ConvTranspose2d(base_ch*2, out_ch, 4, 2, 1)      # 256\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.act(self.e1(x))\n",
    "        e2 = self.act(self.e2(e1))\n",
    "        e3 = self.act(self.e3(e2))\n",
    "        e4 = self.act(self.e4(e3))\n",
    "        d1 = self.act(self.d1(e4))\n",
    "        d1_cat = torch.cat([d1, e3], dim=1)\n",
    "        d2 = self.act(self.d2(d1_cat))\n",
    "        d2_cat = torch.cat([d2, e2], dim=1)\n",
    "        d3 = self.act(self.d3(d2_cat))\n",
    "        d3_cat = torch.cat([d3, e1], dim=1)\n",
    "        out = self.tanh(self.d4(d3_cat))\n",
    "        return out\n",
    "\n",
    "class PatchDiscriminator(nn.Module):\n",
    "    def __init__(self, in_ch=6, base_ch=64):\n",
    "        super().__init__()\n",
    "        # input is concat(input, target) -> 6 channels\n",
    "        self.conv1 = nn.Conv2d(in_ch, base_ch, 4, 2, 1)   # 128\n",
    "        self.conv2 = nn.Conv2d(base_ch, base_ch*2, 4, 2, 1) #64\n",
    "        self.conv3 = nn.Conv2d(base_ch*2, base_ch*4, 4, 2, 1) #32\n",
    "        self.conv4 = nn.Conv2d(base_ch*4, base_ch*8, 4, 1, 1) #31->30-ish\n",
    "        self.conv5 = nn.Conv2d(base_ch*8, 1, 4, 1, 1)\n",
    "        self.act = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.conv1(x))\n",
    "        x = self.act(self.conv2(x))\n",
    "        x = self.act(self.conv3(x))\n",
    "        x = self.act(self.conv4(x))\n",
    "        x = self.conv5(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train_gan(dataset_root=None, out_dir=OUT_DIR, epochs=100, batch_size=16, lr=2e-4, base_ch=64,\n",
    "              adv_weight=1.0, lambda_l1=100.0, num_workers=4, save_every=1, \n",
    "              train_loader=None, use_make_dataset=True, nb_images=10, trainsplit=0.8):\n",
    "    \"\"\"\n",
    "    Simple conditional GAN training loop. Generator maps degraded input -> enhanced output.\n",
    "    Discriminator is a PatchGAN that sees (input, real) or (input, fake).\n",
    "    \n",
    "    Args:\n",
    "        train_loader: If provided, use this DataLoader instead of creating one. Expected to return (target, label) batches.\n",
    "        dataset_root: Path to dataset (only used if train_loader is None).\n",
    "        use_make_dataset: If True and train_loader is None, use dataloader.make_dataset; else VaeImageDataset.\n",
    "    \"\"\"\n",
    "    if train_loader is not None:\n",
    "        loader = train_loader\n",
    "        use_external_ds = True\n",
    "    elif use_make_dataset:\n",
    "        train_ds, _, _ = dataloader.make_dataset(dataset_root, nb_images, image_size=256, trainsplit=trainsplit, crop_faces=False)\n",
    "        loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "        use_external_ds = True\n",
    "    else:\n",
    "        ds = VaeImageDataset(dataset_root, size=256, transform=None, degrade=degrader)\n",
    "        loader = DataLoader(ds, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "        use_external_ds = False\n",
    "\n",
    "    G = Generator(in_ch=3, out_ch=3, base_ch=base_ch).to(device)\n",
    "    D = PatchDiscriminator(in_ch=6, base_ch=base_ch).to(device)\n",
    "    opt_G = torch.optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    opt_D = torch.optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "    bce = nn.BCEWithLogitsLoss()\n",
    "    l1 = nn.L1Loss()\n",
    "\n",
    "    iters = 0\n",
    "    for epoch in range(1, epochs+1):\n",
    "        G.train(); D.train()\n",
    "        t0 = time.time()\n",
    "        epoch_loss_G = 0.0\n",
    "        epoch_loss_D = 0.0\n",
    "        for batch in loader:\n",
    "            if use_external_ds:\n",
    "                tgt, _ = batch\n",
    "                tgt = tgt.to(device)\n",
    "                # convert to uint8 and degrade\n",
    "                tgt_uint8 = ((tgt + 1.0) * 127.5).clamp(0,255).permute(0,2,3,1).detach().cpu().numpy().astype('uint8')\n",
    "                inp_imgs = [degrader.degrade(img) for img in tgt_uint8]\n",
    "                inp = torch.stack([((torch.from_numpy(img.astype('float32')/255.0).permute(2,0,1)-0.5)/0.5) for img in inp_imgs]).to(device)\n",
    "            else:\n",
    "                inp, tgt = batch\n",
    "                inp = inp.to(device); tgt = tgt.to(device)\n",
    "\n",
    "            # ------------------ update D ------------------\n",
    "            with torch.no_grad():\n",
    "                fake = G(inp)\n",
    "            real_pair = torch.cat([inp, tgt], dim=1)\n",
    "            fake_pair = torch.cat([inp, fake], dim=1)\n",
    "\n",
    "            opt_D.zero_grad()\n",
    "            pred_real = D(real_pair)\n",
    "            pred_fake = D(fake_pair)\n",
    "            real_labels = torch.ones_like(pred_real, device=device)\n",
    "            fake_labels = torch.zeros_like(pred_fake, device=device)\n",
    "            loss_D_real = bce(pred_real, real_labels)\n",
    "            loss_D_fake = bce(pred_fake, fake_labels)\n",
    "            loss_D = (loss_D_real + loss_D_fake) * 0.5\n",
    "            loss_D.backward()\n",
    "            opt_D.step()\n",
    "\n",
    "            # ------------------ update G ------------------\n",
    "            opt_G.zero_grad()\n",
    "            fake = G(inp)\n",
    "            fake_pair = torch.cat([inp, fake], dim=1)\n",
    "            pred_fake_for_G = D(fake_pair)\n",
    "            adv_loss = bce(pred_fake_for_G, real_labels)\n",
    "            recon_loss = l1(fake, tgt)\n",
    "            loss_G = adv_weight * adv_loss + lambda_l1 * recon_loss\n",
    "            loss_G.backward()\n",
    "            opt_G.step()\n",
    "\n",
    "            epoch_loss_G += loss_G.item()\n",
    "            epoch_loss_D += loss_D.item()\n",
    "            iters += 1\n",
    "\n",
    "            if iters % 200 == 0:\n",
    "                # save sample\n",
    "                sample_and_save(G, inp.detach().cpu(), tgt.detach().cpu(), iters, SAMPLES_DIR)\n",
    "\n",
    "        dt = time.time() - t0\n",
    "        print(f'Epoch {epoch}/{epochs} G_loss={epoch_loss_G/len(loader):.6f} D_loss={epoch_loss_D/len(loader):.6f} time={dt:.1f}s')\n",
    "        if epoch % save_every == 0:\n",
    "            torch.save({'epoch': epoch, 'G': G.state_dict(), 'D': D.state_dict(), 'optG': opt_G.state_dict(), 'optD': opt_D.state_dict()}, CHECKPOINT_DIR / f'gan_epoch_{epoch}.pth')\n",
    "\n",
    "    return G, D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4780ab05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training utilities defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: training utilities and loop\n",
    "def loss_function(recon, target, mu, logvar, l1_weight=1.0, beta=1e-4):\n",
    "    # recon, target in [-1,1]\n",
    "    recon_loss = F.l1_loss(recon, target) * l1_weight\n",
    "    # KL per batch (sum over latent dims, mean over batch)\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)\n",
    "    kl = kl.mean()\n",
    "    return recon_loss + beta * kl, recon_loss.item(), kl.item()\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, path):\n",
    "    torch.save({'epoch': epoch, 'model_state': model.state_dict(), 'optim_state': optimizer.state_dict()}, str(path))\n",
    "\n",
    "def sample_and_save(model, inp, tgt, step, out_dir):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        recon, _, _ = model(inp.to(device))\n",
    "    # recon, inp, tgt ranges: model uses [-1,1] -> save_image will map to [0,1] if we transform back\n",
    "    # Expect inp and tgt to be CPU tensors in [-1,1]\n",
    "    grid = make_grid(torch.cat([inp.cpu(), recon.cpu(), tgt.cpu()], dim=0), nrow=inp.size(0))\n",
    "    save_image((grid + 1) / 2.0, out_dir / f'sample_step_{step}.png')\n",
    "    model.train()\n",
    "\n",
    "\n",
    "def train_vae(dataset_root=None, out_dir=OUT_DIR, epochs=10, batch_size=16, lr=1e-4, latent_dim=512,\n",
    "              l1_weight=1.0, beta=1e-4, num_workers=4, save_every=1, \n",
    "              train_loader=None, use_make_dataset=True, nb_images=10, trainsplit=0.8):\n",
    "    \"\"\"\n",
    "    Train VAE. If train_loader is provided, use it; otherwise build dataset from dataset_root.\n",
    "    \n",
    "    Args:\n",
    "        train_loader: If provided, use this DataLoader instead of creating one. Expected to return (target, label) batches.\n",
    "        dataset_root: Path to dataset (only used if train_loader is None).\n",
    "        use_make_dataset: If True and train_loader is None, use dataloader.make_dataset; else VaeImageDataset.\n",
    "    \"\"\"\n",
    "    if train_loader is not None:\n",
    "        loader = train_loader\n",
    "        use_external_ds = True\n",
    "    elif use_make_dataset:\n",
    "        # use the project's dataloader to build train/test datasets. We set crop_faces=False so tensors are raw\n",
    "        # resized images normalized to [-1,1] by the dataloader; we will convert them back to uint8 to apply our degrader\n",
    "        train_ds, test_ds, _ = dataloader.make_dataset(dataset_root, nb_images, image_size=256, trainsplit=trainsplit, crop_faces=False)\n",
    "        loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "        use_external_ds = True\n",
    "    else:\n",
    "        ds = VaeImageDataset(dataset_root, size=256, transform=None, degrade=degrader)\n",
    "        loader = DataLoader(ds, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "        use_external_ds = False\n",
    "\n",
    "    model = SimpleVAE(in_ch=3, base_ch=64, latent_dim=latent_dim).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    n_iter = 0\n",
    "    for epoch in range(1, epochs+1):\n",
    "        epoch_loss = 0.0\n",
    "        recon_l_sum = 0.0\n",
    "        kl_sum = 0.0\n",
    "        start = time.time()\n",
    "        for batch_idx, batch in enumerate(loader, 1):\n",
    "            if use_external_ds:\n",
    "                # dataloader.make_dataset -> TensorDataset of (faces, labels)\n",
    "                tgt, _labels = batch\n",
    "                tgt = tgt.to(device)\n",
    "                # convert target tensors (assumed in [-1,1]) back to uint8 HWC for degradation\n",
    "                tgt_uint8 = ((tgt + 1.0) * 127.5).clamp(0, 255).permute(0, 2, 3, 1).detach().cpu().numpy().astype('uint8')\n",
    "                # apply degrader per-sample\n",
    "                inp_imgs = [degrader.degrade(img) for img in tgt_uint8]\n",
    "                # convert degraded inputs back to tensors in [-1,1]\n",
    "                inp = torch.stack([((torch.from_numpy(img.astype('float32') / 255.0).permute(2, 0, 1) - 0.5) / 0.5) for img in inp_imgs]).to(device)\n",
    "            else:\n",
    "                inp, tgt = batch\n",
    "                inp = inp.to(device)\n",
    "                tgt = tgt.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            recon, mu, logvar = model(inp)\n",
    "            loss, recon_l, kl_l = loss_function(recon, tgt, mu, logvar, l1_weight=l1_weight, beta=beta)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            recon_l_sum += recon_l\n",
    "            kl_sum += kl_l\n",
    "            n_iter += 1\n",
    "            if n_iter % 200 == 0:\n",
    "                # save a sample (use a small minibatch from current batch)\n",
    "                # prepare CPU copies of inp/tgt consistent with sample_and_save expectations\n",
    "                sample_inp = inp.detach().cpu()\n",
    "                sample_tgt = tgt.detach().cpu()\n",
    "                sample_and_save(model, sample_inp, sample_tgt, n_iter, SAMPLES_DIR)\n",
    "\n",
    "        elapsed = time.time() - start\n",
    "        print(f'Epoch {epoch}/{epochs} avg_loss={epoch_loss/len(loader):.6f} recon={recon_l_sum/len(loader):.6f} kl={kl_sum/len(loader):.6f} time={elapsed:.1f}s')\n",
    "\n",
    "        if epoch % save_every == 0:\n",
    "            ckpt_path = CHECKPOINT_DIR / f'vae_epoch_{epoch}.pth'\n",
    "            save_checkpoint(model, optimizer, epoch, ckpt_path)\n",
    "            print('Saved checkpoint to', ckpt_path)\n",
    "\n",
    "    return model\n",
    "\n",
    "print('Training utilities defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ca3ee27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000 G_loss=45.015999 D_loss=0.667705 time=0.5s\n",
      "Epoch 2/1000 G_loss=39.550528 D_loss=0.482376 time=0.5s\n",
      "Epoch 2/1000 G_loss=39.550528 D_loss=0.482376 time=0.5s\n",
      "Epoch 3/1000 G_loss=23.253866 D_loss=0.842958 time=0.5s\n",
      "Epoch 3/1000 G_loss=23.253866 D_loss=0.842958 time=0.5s\n",
      "Epoch 4/1000 G_loss=15.226611 D_loss=0.709538 time=0.5s\n",
      "Epoch 4/1000 G_loss=15.226611 D_loss=0.709538 time=0.5s\n",
      "Epoch 5/1000 G_loss=13.547769 D_loss=0.686390 time=0.5s\n",
      "Epoch 5/1000 G_loss=13.547769 D_loss=0.686390 time=0.5s\n",
      "Epoch 6/1000 G_loss=11.229188 D_loss=0.694160 time=0.5s\n",
      "Epoch 6/1000 G_loss=11.229188 D_loss=0.694160 time=0.5s\n",
      "Epoch 7/1000 G_loss=10.704336 D_loss=0.681595 time=0.5s\n",
      "Epoch 7/1000 G_loss=10.704336 D_loss=0.681595 time=0.5s\n",
      "Epoch 8/1000 G_loss=9.936435 D_loss=0.684250 time=0.5s\n",
      "Epoch 8/1000 G_loss=9.936435 D_loss=0.684250 time=0.5s\n",
      "Epoch 9/1000 G_loss=9.417183 D_loss=0.691413 time=0.5s\n",
      "Epoch 9/1000 G_loss=9.417183 D_loss=0.691413 time=0.5s\n",
      "Epoch 10/1000 G_loss=10.007603 D_loss=0.674935 time=0.5s\n",
      "Epoch 10/1000 G_loss=10.007603 D_loss=0.674935 time=0.5s\n",
      "Epoch 11/1000 G_loss=8.644685 D_loss=0.689470 time=0.5s\n",
      "Epoch 11/1000 G_loss=8.644685 D_loss=0.689470 time=0.5s\n",
      "Epoch 12/1000 G_loss=8.337909 D_loss=0.687198 time=0.5s\n",
      "Epoch 12/1000 G_loss=8.337909 D_loss=0.687198 time=0.5s\n",
      "Epoch 13/1000 G_loss=8.030304 D_loss=0.691432 time=0.5s\n",
      "Epoch 13/1000 G_loss=8.030304 D_loss=0.691432 time=0.5s\n",
      "Epoch 14/1000 G_loss=8.830230 D_loss=0.677284 time=0.5s\n",
      "Epoch 14/1000 G_loss=8.830230 D_loss=0.677284 time=0.5s\n",
      "Epoch 15/1000 G_loss=7.693588 D_loss=0.697338 time=0.5s\n",
      "Epoch 15/1000 G_loss=7.693588 D_loss=0.697338 time=0.5s\n",
      "Epoch 16/1000 G_loss=7.979729 D_loss=0.691479 time=0.5s\n",
      "Epoch 16/1000 G_loss=7.979729 D_loss=0.691479 time=0.5s\n",
      "Epoch 17/1000 G_loss=7.742142 D_loss=0.684939 time=0.5s\n",
      "Epoch 17/1000 G_loss=7.742142 D_loss=0.684939 time=0.5s\n",
      "Epoch 18/1000 G_loss=8.901831 D_loss=0.680343 time=0.5s\n",
      "Epoch 18/1000 G_loss=8.901831 D_loss=0.680343 time=0.5s\n",
      "Epoch 19/1000 G_loss=7.170583 D_loss=0.690173 time=0.5s\n",
      "Epoch 19/1000 G_loss=7.170583 D_loss=0.690173 time=0.5s\n",
      "Epoch 20/1000 G_loss=7.434531 D_loss=0.680711 time=0.5s\n",
      "Epoch 20/1000 G_loss=7.434531 D_loss=0.680711 time=0.5s\n",
      "Epoch 21/1000 G_loss=7.332942 D_loss=0.671292 time=0.5s\n",
      "Epoch 21/1000 G_loss=7.332942 D_loss=0.671292 time=0.5s\n",
      "Epoch 22/1000 G_loss=6.821917 D_loss=0.697220 time=0.5s\n",
      "Epoch 22/1000 G_loss=6.821917 D_loss=0.697220 time=0.5s\n",
      "Epoch 23/1000 G_loss=6.988303 D_loss=0.690236 time=0.5s\n",
      "Epoch 23/1000 G_loss=6.988303 D_loss=0.690236 time=0.5s\n",
      "Epoch 24/1000 G_loss=6.666219 D_loss=0.684580 time=0.5s\n",
      "Epoch 24/1000 G_loss=6.666219 D_loss=0.684580 time=0.5s\n",
      "Epoch 25/1000 G_loss=7.148981 D_loss=0.679375 time=0.5s\n",
      "Epoch 25/1000 G_loss=7.148981 D_loss=0.679375 time=0.5s\n",
      "Epoch 26/1000 G_loss=6.419465 D_loss=0.697480 time=0.5s\n",
      "Epoch 26/1000 G_loss=6.419465 D_loss=0.697480 time=0.5s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mERROR: train_loader not found. Please run Cell 7 first to load the dataset!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# run GAN training using the pre-loaded train_loader\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     G, D \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_gan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOUT_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mbase_ch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madv_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_l1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGAN training finished (smoke run)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[15], line 130\u001b[0m, in \u001b[0;36mtrain_gan\u001b[0;34m(dataset_root, out_dir, epochs, batch_size, lr, base_ch, adv_weight, lambda_l1, num_workers, save_every, train_loader, use_make_dataset, nb_images, trainsplit)\u001b[0m\n\u001b[1;32m    127\u001b[0m loss_G\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    128\u001b[0m opt_G\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 130\u001b[0m epoch_loss_G \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss_G\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m epoch_loss_D \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_D\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    132\u001b[0m iters \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Cell 6: quick smoke-run / example usage (adjust params and run)\n",
    "# NOTE: this cell will run training; set epochs small for a quick smoke test.\n",
    "# Make sure you've run Cell 7 first to create train_loader and test_loader!\n",
    "\n",
    "EPOCHS = 1000\n",
    "BATCH_SIZE = 8\n",
    "LR = 2e-4\n",
    "LATENT_DIM = 256\n",
    "\n",
    "# Use the pre-created train_loader from Cell 7 (much cleaner!)\n",
    "if 'train_loader' not in dir():\n",
    "    print('ERROR: train_loader not found. Please run Cell 7 first to load the dataset!')\n",
    "else:\n",
    "    # run GAN training using the pre-loaded train_loader\n",
    "    G, D = train_gan(train_loader=train_loader, out_dir=OUT_DIR, epochs=EPOCHS, lr=LR, \n",
    "                     base_ch=64, adv_weight=1.0, lambda_l1=100.0, save_every=200)\n",
    "    print('GAN training finished (smoke run)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2fe807df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataset \n",
      "\n",
      "Loaded dataloaders: train_loader, test_loader\n",
      "Loaded dataloaders: train_loader, test_loader\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Load datasets using project dataloader\n",
    "IMAGE_SIZE = 256\n",
    "NB_IMAGES = 10\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset, test_dataset, nb_classes = dataloader.make_dataset(\"../data/Face-Swap-M2-Dataset/dataset/smaller\", NB_IMAGES, IMAGE_SIZE, 0.8, crop_faces=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print('Loaded dataloaders: train_loader, test_loader')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e24d96a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 1 is not equal to len(dims) = 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Example usage (run after training completes)\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[43mplot_gan_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 16\u001b[0m, in \u001b[0;36mplot_gan_results\u001b[0;34m(generator, loader, num_samples)\u001b[0m\n\u001b[1;32m     13\u001b[0m     fake \u001b[38;5;241m=\u001b[39m generator(inp)\n\u001b[1;32m     15\u001b[0m inp \u001b[38;5;241m=\u001b[39m inp\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m  \u001b[38;5;66;03m# Denormalize to [0, 1]\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m tgt \u001b[38;5;241m=\u001b[39m \u001b[43mtgt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m     17\u001b[0m fake \u001b[38;5;241m=\u001b[39m fake\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m     19\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(num_samples, \u001b[38;5;241m3\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, num_samples \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 1 is not equal to len(dims) = 4"
     ]
    }
   ],
   "source": [
    "# Cell 8: Plot GAN results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_gan_results(generator, loader, num_samples=5):\n",
    "    generator.eval()\n",
    "    inp, tgt = next(iter(loader))\n",
    "    inp, tgt = inp.to(device), tgt.to(device)\n",
    "    \n",
    "    # Ensure num_samples does not exceed the batch size\n",
    "    num_samples = min(num_samples, inp.size(0))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        fake = generator(inp)\n",
    "    \n",
    "    # tensors\n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(12, num_samples * 4))\n",
    "    for i in range(num_samples):\n",
    "        axes[i, 0].imshow(inp[i])\n",
    "        axes[i, 0].set_title(\"Input (Degraded)\")\n",
    "        axes[i, 0].axis(\"off\")\n",
    "        \n",
    "        axes[i, 1].imshow(fake[i])\n",
    "        axes[i, 1].set_title(\"Generated (Enhanced)\")\n",
    "        axes[i, 1].axis(\"off\")\n",
    "        \n",
    "        axes[i, 2].imshow(tgt[i])\n",
    "        axes[i, 2].set_title(\"Target (Original)\")\n",
    "        axes[i, 2].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage (run after training completes)\n",
    "plot_gan_results(G, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b9a165",
   "metadata": {},
   "source": [
    "## Notes and next steps\n",
    "- For better perceptual quality try adding a perceptual loss (VGG features) or L1 in Lab space.\n",
    "- You can increase `latent_dim`, `base_ch`, and number of epochs for higher capacity.\n",
    "- Save intermediate sample images to `outputs/vae_enhance/samples` and checkpoints to `outputs/vae_enhance/checkpoints`.\n",
    "- If training on many images, increase `num_workers` and `batch_size` and use an LR schedule.\n",
    "- To evaluate perceptual similarity, compute LPIPS or use a pre-trained face embedding model (you already have FaceNet code in `main.ipynb`)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
