{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f052892",
   "metadata": {},
   "source": [
    "# Multi-Subject Reenactment Finetuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c299219b",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1633b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from fsgan.utils.utils import load_model\n",
    "from fsgan.utils.img_utils import bgr2tensor, create_pyramid, tensor2bgr\n",
    "from fsgan.utils.landmarks_utils import LandmarksHeatMapDecoder, filter_landmarks\n",
    "from fsgan.criterions.vgg_loss import VGGLoss\n",
    "from fsgan.notebook_helpers.reenact_preprocess import run_full_pipeline\n",
    "from fsgan.utils.obj_factory import obj_factory\n",
    "\n",
    "import dataloader\n",
    "\n",
    "ROOT = Path('.')\n",
    "WEIGHTS_DIR = ROOT / 'fsgan' / 'weights'\n",
    "OUT_DIR = ROOT / 'outputs'\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b57c9d",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b02348",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 256\n",
    "NB_IMAGES = 10\n",
    "DATASET_PATH = \"../data/Face-Swap-M2-Dataset/dataset/smaller\"\n",
    "\n",
    "\n",
    "FINETUNE_EPOCHS = 100\n",
    "FINETUNE_LR = 1e-5\n",
    "FINETUNE_BATCH_SIZE = 2 \n",
    "GRADIENT_ACCUMULATION_STEPS = 2  # Accumulate gradients to simulate batch_size=4\n",
    "SAVE_EVERY = 200\n",
    "\n",
    "\n",
    "WEIGHT_PIXEL = 0.1\n",
    "WEIGHT_PERCEPTUAL = 1.0\n",
    "WEIGHT_REC = 1.0\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Dataset: {DATASET_PATH}\")\n",
    "print(f\"  Image size: {IMAGE_SIZE}\")\n",
    "print(f\"  Images per person: {NB_IMAGES}\")\n",
    "print(f\"  Epochs: {FINETUNE_EPOCHS}\")\n",
    "print(f\"  Batch size: {FINETUNE_BATCH_SIZE}\")\n",
    "print(f\"  Gradient accumulation: {GRADIENT_ACCUMULATION_STEPS} (effective batch: {FINETUNE_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS})\")\n",
    "print(f\"  Learning rate: {FINETUNE_LR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce91c324",
   "metadata": {},
   "source": [
    "## 3. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cbfdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset, test_dataset, nb_classes = dataloader.make_dataset(\n",
    "    DATASET_PATH, \n",
    "    NB_IMAGES, \n",
    "    IMAGE_SIZE, \n",
    "    0.8, \n",
    "    crop_faces=False\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Number of identities: {nb_classes}\")\n",
    "\n",
    "class_counts = np.bincount([label for _, label in train_dataset])\n",
    "for cls, count in enumerate(class_counts):\n",
    "    print(f\"  Identity {cls}: {count} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7c9d11",
   "metadata": {},
   "source": [
    "## 4. Create Paired Dataset for Reenactment\n",
    "\n",
    "For reenactment training, we need pairs of images from the same person with differents poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4e5ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReenactmentPairDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Creates pairs of images from the same person for reenactment training.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_dataset, resolution=256):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.resolution = resolution\n",
    "        \n",
    "        # Group samples by label (person identity)\n",
    "        self.label_to_indices = {}\n",
    "        for idx in range(len(base_dataset)):\n",
    "            _, label = base_dataset[idx]\n",
    "            if isinstance(label, torch.Tensor):\n",
    "                label = label.item()\n",
    "            if label not in self.label_to_indices:\n",
    "                self.label_to_indices[label] = []\n",
    "            self.label_to_indices[label].append(idx)\n",
    "        \n",
    "        # Filter labels with at least 2 samples\n",
    "        self.valid_labels = [l for l, indices in self.label_to_indices.items() if len(indices) >= 2]\n",
    "        \n",
    "        # Create list of valid indices\n",
    "        self.valid_indices = []\n",
    "        for label in self.valid_labels:\n",
    "            self.valid_indices.extend(self.label_to_indices[label])\n",
    "        \n",
    "        print(f\"ReenactmentPairDataset: {len(self.valid_labels)} identities with 2+ images\")\n",
    "        print(f\"Total valid samples: {len(self.valid_indices)}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get source sample\n",
    "        src_idx = self.valid_indices[idx]\n",
    "        src_img, src_label = self.base_dataset[src_idx]\n",
    "        if isinstance(src_label, torch.Tensor):\n",
    "            src_label = src_label.item()\n",
    "        \n",
    "        # Get a different sample from the same person\n",
    "        same_person_indices = self.label_to_indices[src_label]\n",
    "        tgt_idx = src_idx\n",
    "        while tgt_idx == src_idx:\n",
    "            tgt_idx = same_person_indices[np.random.randint(len(same_person_indices))]\n",
    "        \n",
    "        tgt_img, _ = self.base_dataset[tgt_idx]\n",
    "        \n",
    "        return src_img, tgt_img, src_label\n",
    "\n",
    "# Create paired datasets\n",
    "finetune_dataset = ReenactmentPairDataset(train_dataset, resolution=IMAGE_SIZE)\n",
    "finetune_loader = DataLoader(\n",
    "    finetune_dataset, \n",
    "    batch_size=FINETUNE_BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "print(f\"\\nFinetune dataloader: {len(finetune_loader)} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cef8d78",
   "metadata": {},
   "source": [
    "## 5. Load Pretrained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2f6747",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading pretrained models...\")\n",
    "\n",
    "# Load reenactment generator\n",
    "reenact_w = WEIGHTS_DIR / 'nfv_msrunet_256_1_2_reenactment_v2.1.pth'\n",
    "Gr_finetune, ckpt = load_model(str(reenact_w), 'reenactment', device=device, return_checkpoint=True)\n",
    "Gr_finetune.train()\n",
    "print(f\"Loaded Reenactment generator: {ckpt.get('arch', 'unknown')}\")\n",
    "\n",
    "# Load landmarks model (frozen)\n",
    "lms_w = WEIGHTS_DIR / 'hr18_wflw_landmarks.pth'\n",
    "L_frozen, _ = load_model(str(lms_w), 'landmarks', device=device, return_checkpoint=True)\n",
    "L_frozen.eval()\n",
    "for param in L_frozen.parameters():\n",
    "    param.requires_grad = False\n",
    "print(\"Loaded Landmarks model (frozen)\")\n",
    "\n",
    "n_local = getattr(Gr_finetune, 'n_local_enhancers', None)\n",
    "if n_local is None and hasattr(Gr_finetune, 'module'):\n",
    "    n_local = getattr(Gr_finetune.module, 'n_local_enhancers', None)\n",
    "n_local = n_local if n_local is not None else 1\n",
    "n_levels = n_local + 1\n",
    "print(f\"Pyramid levels: {n_levels}\")\n",
    "\n",
    "# Normalisation for landmark model\n",
    "imagenet_mean = torch.tensor([0.485, 0.456, 0.406], device=device).view(1, 3, 1, 1)\n",
    "imagenet_std = torch.tensor([0.229, 0.224, 0.225], device=device).view(1, 3, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8128d884",
   "metadata": {},
   "source": [
    "## 6. Setup Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd874aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing loss functions...\")\n",
    "\n",
    "\n",
    "criterion_pixel = nn.L1Loss().to(device)\n",
    "\n",
    "\n",
    "try:\n",
    "    vgg_id_path = str(WEIGHTS_DIR / 'vggface2_vgg19_256_1_2_id.pth')\n",
    "    criterion_id = VGGLoss(vgg_id_path).to(device)\n",
    "    criterion_id.eval()\n",
    "    print(\"loaded VGG identity loss\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load VGG identity loss: {e}\")\n",
    "    criterion_id = None\n",
    "\n",
    "try:\n",
    "    vgg_attr_path = str(WEIGHTS_DIR / 'celeba_vgg19_256_2_0_28_attr.pth')\n",
    "    criterion_attr = VGGLoss(vgg_attr_path).to(device)\n",
    "    criterion_attr.eval()\n",
    "    print(\"VGG attribute loss loaded\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load VGG attribute loss: {e}\")\n",
    "    criterion_attr = None\n",
    "\n",
    "\n",
    "optimizer_G = optim.Adam(Gr_finetune.parameters(), lr=FINETUNE_LR, betas=(0.5, 0.999))\n",
    "print(f\"Optimizer: Adam, LR={FINETUNE_LR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb0ef17",
   "metadata": {},
   "source": [
    "## 7. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292a250f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_reenactment_input(src_batch, tgt_batch, L_model, n_pyramid_levels, device):\n",
    "    \"\"\"\n",
    "    Prepare source image pyramid with target landmarks.\n",
    "    \"\"\"\n",
    "\n",
    "    if src_batch.min() >= 0:\n",
    "        src_normalized = src_batch * 2 - 1\n",
    "    else:\n",
    "        src_normalized = src_batch\n",
    "    \n",
    "    if tgt_batch.min() >= 0:\n",
    "        tgt_normalized = tgt_batch * 2 - 1\n",
    "    else:\n",
    "        tgt_normalized = tgt_batch\n",
    "    \n",
    "\n",
    "    tgt_01 = (tgt_normalized + 1) / 2\n",
    "    tgt_for_lms = (tgt_01 - imagenet_mean) / imagenet_std\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        tgt_landmarks = L_model(tgt_for_lms)\n",
    "        tgt_landmarks = filter_landmarks(tgt_landmarks)\n",
    "\n",
    "        del tgt_01, tgt_for_lms\n",
    "    \n",
    "\n",
    "    src_pyd = create_pyramid(src_normalized, n_pyramid_levels)\n",
    "    \n",
    "\n",
    "    input_list = []\n",
    "    for p in range(len(src_pyd)):\n",
    "        pyd_h, pyd_w = src_pyd[p].shape[2:]\n",
    "        context = F.interpolate(tgt_landmarks, size=(pyd_h, pyd_w), mode='bilinear', align_corners=False)\n",
    "        context = filter_landmarks(context)\n",
    "        inp = torch.cat((src_pyd[p], context), dim=1)\n",
    "        input_list.append(inp)\n",
    "    \n",
    "\n",
    "    del src_pyd, tgt_landmarks, context\n",
    "    \n",
    "    return input_list, tgt_normalized\n",
    "\n",
    "print(\"Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461f835d",
   "metadata": {},
   "source": [
    "## 8. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b32d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_reenactment(model, dataloader, optimizer, epochs, save_dir, resume_from=None):\n",
    "    \"\"\"\n",
    "    Finetune the reenactment generator on paired face data.\n",
    "    \n",
    "    Args:\n",
    "        resume_from: Path to checkpoint to resume from, or None to start fresh\n",
    "    \"\"\"\n",
    "    save_path = Path(save_dir)\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    model.train()\n",
    "    history = {'loss': [], 'loss_pixel': [], 'loss_id': [], 'loss_attr': []}\n",
    "    start_epoch = 0\n",
    "    \n",
    "\n",
    "    if resume_from is not None:\n",
    "        print(f\"Resuming from checkpoint: {resume_from}\")\n",
    "        resume_ckpt = torch.load(resume_from, map_location=device)\n",
    "        model.load_state_dict(resume_ckpt['state_dict'])\n",
    "        optimizer.load_state_dict(resume_ckpt['optimizer'])\n",
    "        start_epoch = resume_ckpt.get('epoch', 0)\n",
    "        if 'history' in resume_ckpt:\n",
    "            history = resume_ckpt['history']\n",
    "        print(f\"✓ Resumed from epoch {start_epoch}\")\n",
    "        print(f\"  Previous best loss: {history['loss'][-1] if history['loss'] else 'N/A'}\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"STARTING FINETUNING\" if start_epoch == 0 else \"RESUMING FINETUNING\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Epochs: {start_epoch + 1} to {epochs}\")\n",
    "    print(f\"Batches per epoch: {len(dataloader)}\")\n",
    "    print(f\"Save directory: {save_path}\")\n",
    "    print(f\"Gradient accumulation steps: {GRADIENT_ACCUMULATION_STEPS}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        epoch_losses = {'total': 0, 'pixel': 0, 'id': 0, 'attr': 0}\n",
    "        n_batches = 0\n",
    "        accumulation_step = 0\n",
    "        \n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        for batch_idx, (src_img, tgt_img, labels) in enumerate(pbar):\n",
    "            src_img = src_img.to(device)\n",
    "            tgt_img = tgt_img.to(device)\n",
    "\n",
    "            \n",
    "\n",
    "            input_list, tgt_normalized = prepare_reenactment_input(\n",
    "                src_img, tgt_img, L_frozen, n_levels, device\n",
    "            )\n",
    "            \n",
    "\n",
    "            output = model(input_list)\n",
    "            pred = output[-1] if isinstance(output, (list, tuple)) else output\n",
    "            \n",
    "\n",
    "            loss_pixel = criterion_pixel(pred, tgt_normalized)\n",
    "            \n",
    "            if criterion_id is not None:\n",
    "                loss_id = criterion_id(pred, tgt_normalized)\n",
    "            else:\n",
    "                loss_id = torch.tensor(0.0, device=device)\n",
    "            \n",
    "            if criterion_attr is not None:\n",
    "                loss_attr = criterion_attr(pred, tgt_normalized)\n",
    "            else:\n",
    "                loss_attr = torch.tensor(0.0, device=device)\n",
    "            \n",
    "\n",
    "            loss_rec = WEIGHT_PIXEL * loss_pixel + 0.5 * loss_id + 0.5 * loss_attr\n",
    "            loss_total = WEIGHT_REC * loss_rec / GRADIENT_ACCUMULATION_STEPS\n",
    "            \n",
    "\n",
    "            loss_total.backward()\n",
    "            \n",
    "\n",
    "            total_val = loss_total.item() * GRADIENT_ACCUMULATION_STEPS\n",
    "            pixel_val = loss_pixel.item()\n",
    "            id_val = loss_id.item() if criterion_id else 0\n",
    "            attr_val = loss_attr.item() if criterion_attr else 0\n",
    "            \n",
    "            epoch_losses['total'] += total_val\n",
    "            epoch_losses['pixel'] += pixel_val\n",
    "            epoch_losses['id'] += id_val\n",
    "            epoch_losses['attr'] += attr_val\n",
    "            n_batches += 1\n",
    "            \n",
    "            del input_list, output, pred, loss_pixel, loss_id, loss_attr, loss_rec, loss_total\n",
    "            del src_img, tgt_img, tgt_normalized\n",
    "            \n",
    "            accumulation_step += 1\n",
    "            if accumulation_step % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                accumulation_step = 0\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': f\"{total_val:.4f}\",\n",
    "                'pix': f\"{pixel_val:.4f}\"\n",
    "            })\n",
    "            \n",
    "            if (n_batches % 5) == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        if accumulation_step > 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        avg_loss = epoch_losses['total'] / n_batches\n",
    "        avg_pixel = epoch_losses['pixel'] / n_batches\n",
    "        avg_id = epoch_losses['id'] / n_batches\n",
    "        avg_attr = epoch_losses['attr'] / n_batches\n",
    "        \n",
    "        history['loss'].append(avg_loss)\n",
    "        history['loss_pixel'].append(avg_pixel)\n",
    "        history['loss_id'].append(avg_id)\n",
    "        history['loss_attr'].append(avg_attr)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Loss={avg_loss:.4f}, Pixel={avg_pixel:.4f}, ID={avg_id:.4f}\")\n",
    "        \n",
    "        # Save\n",
    "        if (epoch + 1) % SAVE_EVERY == 0 or epoch == epochs - 1:\n",
    "            ckpt_path = save_path / f'reenact_finetuned_epoch{epoch+1}.pth'\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'arch': ckpt.get('arch', 'unknown'),\n",
    "                'history': history,\n",
    "            }, ckpt_path)\n",
    "            print(f\"  → Saved checkpoint: {ckpt_path}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINETUNING COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return history\n",
    "\n",
    "print(\"Training function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc98d03",
   "metadata": {},
   "source": [
    "## 9. Run Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3ba3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"reenact_finetuned_epoch100.pth\"\n",
    "finetuned_dir = OUT_DIR / 'finetuned_models'\n",
    "ckpts = [finetuned_dir / file_name] if (finetuned_dir / file_name).exists() else []\n",
    "\n",
    "if not ckpts:\n",
    "    print(\"No finetuned checkpoints found!\")\n",
    "else:\n",
    "    latest_ckpt = ckpts[-1]\n",
    "    print(f\"Loading: {latest_ckpt.name}\")\n",
    "    \n",
    "    ckpt_data = torch.load(str(latest_ckpt), map_location=device)\n",
    "    Gr_finetune.load_state_dict(ckpt_data['state_dict'])\n",
    "    print(f\"Loaded finetuned model from epoch {ckpt_data.get('epoch', '?')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c126c3",
   "metadata": {},
   "source": [
    "### 9.1 Configure Resume (Optional)\n",
    "If you have a checkpoint from a previous training run, you can resume from it instead of starting from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3996458",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RESUME_FROM =  'outputs/finetuned_models/reenact_finetuned_epoch300.pth'\n",
    "\n",
    "\n",
    "if RESUME_FROM and Path(RESUME_FROM).exists():\n",
    "    print(f\"Will resume from: {RESUME_FROM}\")\n",
    "else:\n",
    "    if RESUME_FROM:\n",
    "        print(f\"Checkpoint not found: {RESUME_FROM}\")\n",
    "        print(\"  Starting fresh training instead\")\n",
    "    RESUME_FROM = None\n",
    "    print(\"Starting fresh training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bff326",
   "metadata": {},
   "outputs": [],
   "source": [
    "allocated_memory = torch.cuda.memory_allocated()\n",
    "print(f\"CUDA memory allocated: {allocated_memory / (1024 ** 2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554faca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(\"CUDA cache emptied.\")\n",
    "\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "print(\"GPU memory stats reset.\")\n",
    "\n",
    "os.system('nvidia-smi --gpu-reset')\n",
    "print(\"GPU reset command executed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e66b4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "allocated_memory = torch.cuda.memory_allocated()\n",
    "print(f\"CUDA memory allocated: {allocated_memory / (1024 ** 2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e081c26",
   "metadata": {},
   "source": [
    "### 9.2 Execute Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3454594a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history = finetune_reenactment(\n",
    "    model=Gr_finetune,\n",
    "    dataloader=finetune_loader,\n",
    "    optimizer=optimizer_G,\n",
    "    epochs=1500,\n",
    "    save_dir=finetuned_dir,\n",
    "    resume_from=RESUME_FROM\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a5b7f0",
   "metadata": {},
   "source": [
    "**How Resume Works:**\n",
    "- If `RESUME_FROM=None`: Starts fresh training from epoch 0\n",
    "- If `RESUME_FROM` points to a checkpoint: Loads model weights, optimizer state, and continues from that epoch\n",
    "- Training history is preserved and extended\n",
    "- New checkpoints will continue the epoch numbering (e.g., if resuming from epoch 50, next save is epoch 60, 70, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f69e83",
   "metadata": {},
   "source": [
    "## 10. Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776b1099",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "if 'history' in dir() and history:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    axes[0].plot(history['loss'], 'b-', linewidth=2)\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Total Loss')\n",
    "    axes[0].set_title('Total Loss')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1].plot(history['loss_pixel'], 'g-', linewidth=2)\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Pixel L1 Loss')\n",
    "    axes[1].set_title('Pixel Loss')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[2].plot(history['loss_id'], 'r-', linewidth=2, label='Identity')\n",
    "    axes[2].plot(history['loss_attr'], 'm-', linewidth=2, label='Attribute')\n",
    "    axes[2].set_xlabel('Epoch')\n",
    "    axes[2].set_ylabel('Perceptual Loss')\n",
    "    axes[2].set_title('Perceptual Losses')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(str(OUT_DIR / 'finetuning_curves.png'), dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Final losses - Total: {history['loss'][-1]:.4f}, Pixel: {history['loss_pixel'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919e2171",
   "metadata": {},
   "source": [
    "## 11. Test Finetuned Model with Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7859e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def analyze_dataset_identities(dataset):\n",
    "    \"\"\"Analyze dataset to find all identities.\"\"\"\n",
    "    label_to_indices = {}\n",
    "    for idx in range(len(dataset)):\n",
    "        _, label = dataset[idx]\n",
    "        if isinstance(label, torch.Tensor):\n",
    "            label = label.item()\n",
    "        if label not in label_to_indices:\n",
    "            label_to_indices[label] = []\n",
    "        label_to_indices[label].append(idx)\n",
    "    return label_to_indices\n",
    "\n",
    "NUM_SWAPS = 5\n",
    "FULL_SWAP_DIR = OUT_DIR / 'finetuned_full_swaps'\n",
    "FULL_SWAP_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "\n",
    "print(\"Loading finetuned model...\")\n",
    "finetuned_dir = OUT_DIR / 'finetuned_models'\n",
    "ckpts = sorted(finetuned_dir.glob('*.pth'))\n",
    "\n",
    "if not ckpts:\n",
    "    print(\"No finetuned checkpoints found!\")\n",
    "else:\n",
    "    latest_ckpt = ckpts[-1]\n",
    "    print(f\"Loading: {latest_ckpt.name}\")\n",
    "    \n",
    "    ckpt_data = torch.load(str(latest_ckpt), map_location=device)\n",
    "    arch = ckpt_data.get('arch', 'res_unet.MultiScaleResUNet(in_nc=101,out_nc=3)')\n",
    "    Gr_finetuned = obj_factory(arch).to(device)\n",
    "    Gr_finetuned.load_state_dict(ckpt_data['state_dict'])\n",
    "    Gr_finetuned.eval()\n",
    "    \n",
    "    print(f\"✓ Loaded epoch {ckpt_data.get('epoch', '?')}\")\n",
    "    \n",
    "    # Get available identities\n",
    "    train_identities = analyze_dataset_identities(train_dataset)\n",
    "    available_labels = list(train_identities.keys())\n",
    "    \n",
    "    if len(available_labels) < 2:\n",
    "        print(\"Need at least 2 identities!\")\n",
    "    else:\n",
    "        print(f\"\\nPerforming {NUM_SWAPS} full pipeline face swaps...\")\n",
    "        \n",
    "        all_sources = []\n",
    "        all_targets = []\n",
    "        all_results = []\n",
    "        swap_info = []\n",
    "        \n",
    "        for i in range(NUM_SWAPS):\n",
    "            src_label, tgt_label = random.sample(available_labels, 2)\n",
    "            src_idx = random.choice(train_identities[src_label])\n",
    "            tgt_idx = random.choice(train_identities[tgt_label])\n",
    "            \n",
    "            src_img, _ = train_dataset[src_idx]\n",
    "            tgt_img, _ = train_dataset[tgt_idx]\n",
    "            \n",
    "            # Save temp images\n",
    "            src_path = FULL_SWAP_DIR / f'temp_src_{i}.png'\n",
    "            tgt_path = FULL_SWAP_DIR / f'temp_tgt_{i}.png'\n",
    "            out_path = FULL_SWAP_DIR / f'full_swap_{i}_id{src_label}_pose{tgt_label}.png'\n",
    "            \n",
    "            src_np = ((src_img.permute(1, 2, 0).numpy() + 1) / 2 * 255).clip(0, 255).astype('uint8')\n",
    "            tgt_np = ((tgt_img.permute(1, 2, 0).numpy() + 1) / 2 * 255).clip(0, 255).astype('uint8')\n",
    "            \n",
    "            cv2.imwrite(str(src_path), cv2.cvtColor(src_np, cv2.COLOR_RGB2BGR))\n",
    "            cv2.imwrite(str(tgt_path), cv2.cvtColor(tgt_np, cv2.COLOR_RGB2BGR))\n",
    "            \n",
    "            # Run full pipeline with FINETUNED MODEL\n",
    "            try:\n",
    "                result_bgr, intermediates, src_crop, tgt_crop = run_full_pipeline(\n",
    "                    str(src_path), \n",
    "                    str(tgt_path), \n",
    "                    out_path=str(out_path),\n",
    "                    reenact=True, \n",
    "                    use_detector=True, \n",
    "                    device=device,\n",
    "                    crop_scale=1.2, \n",
    "                    resolution=256,\n",
    "                    G_model=Gr_finetuned\n",
    "                )\n",
    "                \n",
    "                result_rgb = cv2.cvtColor(result_bgr, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                all_sources.append(src_np)\n",
    "                all_targets.append(tgt_np)\n",
    "                all_results.append(result_rgb)\n",
    "                swap_info.append(f\"ID {src_label} → Pose {tgt_label}\")\n",
    "                \n",
    "                print(f\"  {i+1}/{NUM_SWAPS}: ID {src_label} → Pose {tgt_label} ✓\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  {i+1}/{NUM_SWAPS}: Error - {e}\")\n",
    "        \n",
    "        print(f\"\\n✓ Done! Results in: {FULL_SWAP_DIR}\")\n",
    "        \n",
    "        # Visualize results\n",
    "        if all_results:\n",
    "            n_swaps = len(all_results)\n",
    "            fig, axes = plt.subplots(n_swaps, 3, figsize=(12, 4*n_swaps))\n",
    "            \n",
    "            if n_swaps == 1:\n",
    "                axes = axes.reshape(1, -1)\n",
    "            \n",
    "            for i in range(n_swaps):\n",
    "                axes[i, 0].imshow(all_sources[i])\n",
    "                axes[i, 0].set_title(f'Source\\n{swap_info[i].split(\"→\")[0].strip()}', fontsize=11)\n",
    "                axes[i, 0].axis('off')\n",
    "                \n",
    "                axes[i, 1].imshow(all_targets[i])\n",
    "                axes[i, 1].set_title(f'Target\\n{swap_info[i].split(\"→\")[1].strip()}', fontsize=11)\n",
    "                axes[i, 1].axis('off')\n",
    "                \n",
    "                axes[i, 2].imshow(all_results[i])\n",
    "                axes[i, 2].set_title(f'Result\\n{swap_info[i]}', fontsize=11)\n",
    "                axes[i, 2].axis('off')\n",
    "            \n",
    "            plt.suptitle('Finetuned Model - Face Swap Results', fontsize=14, fontweight='bold')\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            viz_path = FULL_SWAP_DIR / 'all_swaps_visualization.png'\n",
    "            plt.savefig(str(viz_path), dpi=150, bbox_inches='tight')\n",
    "            print(f\"\\nSaved visualization: {viz_path}\")\n",
    "            \n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fe97ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "568a4d3c",
   "metadata": {},
   "source": [
    "## 12. Compare Base vs Finetuned Models\n",
    "\n",
    "Compare face swap results between the pretrained base model and finetuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c476b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FaceNet for identity similarity measurement\n",
    "print(\"\\n4. Loading FaceNet for identity verification...\")\n",
    "\n",
    "try:\n",
    "    from facenet_pytorch import InceptionResnetV1\n",
    "    \n",
    "    facenet = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "    \n",
    "    for param in facenet.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    print(\"   ✓ FaceNet loaded successfully\")\n",
    "    \n",
    "    from torchvision import transforms\n",
    "    facenet_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((160, 160)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    \n",
    "    def get_facenet_embedding(img_rgb):\n",
    "        \"\"\"Extract FaceNet embedding from RGB image (HWC numpy array).\"\"\"\n",
    "        img_tensor = facenet_transform(img_rgb).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            embedding = facenet(img_tensor)\n",
    "        return embedding\n",
    "    \n",
    "    def cosine_similarity(emb1, emb2):\n",
    "        \"\"\"Compute cosine similarity between two embeddings.\"\"\"\n",
    "        return F.cosine_similarity(emb1, emb2).item()\n",
    "    \n",
    "    print(\"   ✓ FaceNet helper functions defined\")\n",
    "    FACENET_AVAILABLE = True\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"   ⚠ facenet-pytorch not installed. Run: pip install facenet-pytorch\")\n",
    "    print(\"   Skipping identity similarity metrics...\")\n",
    "    FACENET_AVAILABLE = False\n",
    "except Exception as e:\n",
    "    print(f\"   ⚠ Error loading FaceNet: {e}\")\n",
    "    print(\"   Skipping identity similarity metrics...\")\n",
    "    FACENET_AVAILABLE = False\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3e3255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "NUM_COMPARISONS = 10\n",
    "COMPARISON_DIR = OUT_DIR / 'model_comparison'\n",
    "COMPARISON_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL COMPARISON: Base vs Finetuned\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load base pretrained model\n",
    "print(\"\\n1. Loading BASE pretrained model...\")\n",
    "reenact_base_w = WEIGHTS_DIR / 'nfv_msrunet_256_1_2_reenactment_v2.1.pth'\n",
    "Gr_base, ckpt_base = load_model(str(reenact_base_w), 'reenactment', device=device, return_checkpoint=True)\n",
    "Gr_base.eval()\n",
    "print(f\"Base model loaded: {ckpt_base.get('arch', 'unknown')}\")\n",
    "\n",
    "# Load finetuned model\n",
    "print(\"\\n2. Loading FINETUNED model...\")\n",
    "finetuned_checkpoint = OUT_DIR / 'finetuned_models' / 'reenact_finetuned_epoch500.pth'\n",
    "\n",
    "if not finetuned_checkpoint.exists():\n",
    "    print(f\"Checkpoint not found: {finetuned_checkpoint}\")\n",
    "    print(\"Looking for any available checkpoint...\")\n",
    "    available_ckpts = sorted((OUT_DIR / 'finetuned_models').glob('*.pth'))\n",
    "    if available_ckpts:\n",
    "        finetuned_checkpoint = available_ckpts[-1]\n",
    "        print(f\"   Using: {finetuned_checkpoint.name}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"No finetuned checkpoints found!\")\n",
    "\n",
    "ckpt_finetuned = torch.load(str(finetuned_checkpoint), map_location=device)\n",
    "arch = ckpt_finetuned.get('arch', 'res_unet.MultiScaleResUNet(in_nc=101,out_nc=3)')\n",
    "Gr_finetuned_cmp = obj_factory(arch).to(device)\n",
    "Gr_finetuned_cmp.load_state_dict(ckpt_finetuned['state_dict'])\n",
    "Gr_finetuned_cmp.eval()\n",
    "print(f\"Finetuned model loaded from epoch {ckpt_finetuned.get('epoch', '?')}\")\n",
    "\n",
    "# Get available identities from dataset\n",
    "def analyze_dataset_identities(dataset):\n",
    "    \"\"\"Analyze dataset to find all identities.\"\"\"\n",
    "    label_to_indices = {}\n",
    "    for idx in range(len(dataset)):\n",
    "        _, label = dataset[idx]\n",
    "        if isinstance(label, torch.Tensor):\n",
    "            label = label.item()\n",
    "        if label not in label_to_indices:\n",
    "            label_to_indices[label] = []\n",
    "        label_to_indices[label].append(idx)\n",
    "    return label_to_indices\n",
    "\n",
    "train_identities = analyze_dataset_identities(train_dataset)\n",
    "available_labels = list(train_identities.keys())\n",
    "\n",
    "print(f\"\\n3. Dataset info:\")\n",
    "print(f\"Available identities: {len(available_labels)}\")\n",
    "print(f\"Will perform {NUM_COMPARISONS} swaps\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6a7f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nPerforming face swaps with both models...\\n\")\n",
    "\n",
    "all_sources = []\n",
    "all_targets = []\n",
    "all_base_results = []\n",
    "all_finetuned_results = []\n",
    "swap_info = []\n",
    "\n",
    "# Metrics storage\n",
    "metrics = {\n",
    "    'base_similarity': [],\n",
    "    'finetuned_similarity': [],\n",
    "    'improvement': []           # Similarity deltas\n",
    "}\n",
    "\n",
    "for i in range(NUM_COMPARISONS):\n",
    "    src_label, tgt_label = random.sample(available_labels, 2)\n",
    "    src_idx = random.choice(train_identities[src_label])\n",
    "    tgt_idx = random.choice(train_identities[tgt_label])\n",
    "    \n",
    "    src_img, _ = train_dataset[src_idx]\n",
    "    tgt_img, _ = train_dataset[tgt_idx]\n",
    "    \n",
    "    src_path = COMPARISON_DIR / f'temp_src_{i}.png'\n",
    "    tgt_path = COMPARISON_DIR / f'temp_tgt_{i}.png'\n",
    "    \n",
    "    src_np = ((src_img.permute(1, 2, 0).numpy() + 1) / 2 * 255).clip(0, 255).astype('uint8')\n",
    "    tgt_np = ((tgt_img.permute(1, 2, 0).numpy() + 1) / 2 * 255).clip(0, 255).astype('uint8')\n",
    "    \n",
    "    cv2.imwrite(str(src_path), cv2.cvtColor(src_np, cv2.COLOR_RGB2BGR))\n",
    "    cv2.imwrite(str(tgt_path), cv2.cvtColor(tgt_np, cv2.COLOR_RGB2BGR))\n",
    "    \n",
    "    print(f\"[{i+1}/{NUM_COMPARISONS}] Identity {src_label} -> Pose {tgt_label}\")\n",
    "    \n",
    "\n",
    "    if FACENET_AVAILABLE:\n",
    "        src_embedding = get_facenet_embedding(src_np)\n",
    "    \n",
    "\n",
    "    try:\n",
    "        base_out_path = COMPARISON_DIR / f'swap_{i}_base_id{src_label}_pose{tgt_label}.png'\n",
    "        result_base_bgr, _, _, _ = run_full_pipeline(\n",
    "            str(src_path), \n",
    "            str(tgt_path), \n",
    "            out_path=str(base_out_path),\n",
    "            reenact=True, \n",
    "            use_detector=True, \n",
    "            device=device,\n",
    "            crop_scale=1.2, \n",
    "            resolution=256,\n",
    "            G_model=Gr_base\n",
    "        )\n",
    "        result_base_rgb = cv2.cvtColor(result_base_bgr, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "\n",
    "        if FACENET_AVAILABLE:\n",
    "            base_embedding = get_facenet_embedding(result_base_rgb)\n",
    "            base_sim = cosine_similarity(src_embedding, base_embedding)\n",
    "            metrics['base_similarity'].append(base_sim)\n",
    "            print(f\"  ✓ Base model (similarity: {base_sim:.4f})\")\n",
    "        else:\n",
    "            print(f\"  ✓ Base model\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Base model failed: {e}\")\n",
    "        result_base_rgb = np.zeros_like(src_np)\n",
    "        if FACENET_AVAILABLE:\n",
    "            metrics['base_similarity'].append(0.0)\n",
    "    \n",
    "\n",
    "    try:\n",
    "        finetuned_out_path = COMPARISON_DIR / f'swap_{i}_finetuned_id{src_label}_pose{tgt_label}.png'\n",
    "        result_finetuned_bgr, _, _, _ = run_full_pipeline(\n",
    "            str(src_path), \n",
    "            str(tgt_path), \n",
    "            out_path=str(finetuned_out_path),\n",
    "            reenact=True, \n",
    "            use_detector=True, \n",
    "            device=device,\n",
    "            crop_scale=1.2, \n",
    "            resolution=256,\n",
    "            G_model=Gr_finetuned_cmp\n",
    "        )\n",
    "        result_finetuned_rgb = cv2.cvtColor(result_finetuned_bgr, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Compute identity similarity\n",
    "        if FACENET_AVAILABLE:\n",
    "            finetuned_embedding = get_facenet_embedding(result_finetuned_rgb)\n",
    "            finetuned_sim = cosine_similarity(src_embedding, finetuned_embedding)\n",
    "            metrics['finetuned_similarity'].append(finetuned_sim)\n",
    "            \n",
    "            # Calculate improvement\n",
    "            improvement = finetuned_sim - metrics['base_similarity'][-1]\n",
    "            metrics['improvement'].append(improvement)\n",
    "            \n",
    "            improvement_str = f\"+{improvement:.4f}\" if improvement > 0 else f\"{improvement:.4f}\"\n",
    "            print(f\"Finetuned model (similarity: {finetuned_sim:.4f}, Δ: {improvement_str})\")\n",
    "        else:\n",
    "            print(f\"Finetuned model\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Finetuned model failed: {e}\")\n",
    "        result_finetuned_rgb = np.zeros_like(src_np)\n",
    "        if FACENET_AVAILABLE:\n",
    "            metrics['finetuned_similarity'].append(0.0)\n",
    "            metrics['improvement'].append(0.0)\n",
    "    \n",
    "    all_sources.append(src_np)\n",
    "    all_targets.append(tgt_np)\n",
    "    all_base_results.append(result_base_rgb)\n",
    "    all_finetuned_results.append(result_finetuned_rgb)\n",
    "    swap_info.append(f\"ID {src_label} → Pose {tgt_label}\")\n",
    "    print()\n",
    "\n",
    "print(f\"Completed {NUM_COMPARISONS} comparisons\")\n",
    "print(f\"Results saved to: {COMPARISON_DIR}\")\n",
    "\n",
    "# Print summary statistics\n",
    "if FACENET_AVAILABLE and metrics['base_similarity']:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"IDENTITY SIMILARITY METRICS (FaceNet Cosine Similarity)\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Base Model Average:      {np.mean(metrics['base_similarity']):.4f} ± {np.std(metrics['base_similarity']):.4f}\")\n",
    "    print(f\"Finetuned Model Average: {np.mean(metrics['finetuned_similarity']):.4f} ± {np.std(metrics['finetuned_similarity']):.4f}\")\n",
    "    print(f\"Average Improvement:     {np.mean(metrics['improvement']):.4f}\")\n",
    "    print(f\"Improvements > 0:        {sum(1 for x in metrics['improvement'] if x > 0)}/{len(metrics['improvement'])} swaps\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64861dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "if FACENET_AVAILABLE and metrics['base_similarity']:\n",
    "    \n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    x = np.arange(NUM_COMPARISONS)\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[0].bar(x - width/2, metrics['base_similarity'], width, label='Base Model', color='steelblue', alpha=0.8)\n",
    "    axes[0].bar(x + width/2, metrics['finetuned_similarity'], width, label='Finetuned Model', color='seagreen', alpha=0.8)\n",
    "    axes[0].set_xlabel('Swap Index', fontsize=12)\n",
    "    axes[0].set_ylabel('Cosine Similarity', fontsize=12)\n",
    "    axes[0].set_title('Identity Preservation per Swap', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xticks(x)\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3, axis='y')\n",
    "    axes[0].set_ylim([0, 1])\n",
    "    \n",
    "    colors = ['green' if x > 0 else 'red' for x in metrics['improvement']]\n",
    "    axes[1].bar(x, metrics['improvement'], color=colors, alpha=0.7)\n",
    "    axes[1].axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
    "    axes[1].set_xlabel('Swap Index', fontsize=12)\n",
    "    axes[1].set_ylabel('Similarity Improvement (delta)', fontsize=12)\n",
    "    axes[1].set_title('Finetuned vs Base Improvement', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xticks(x)\n",
    "    axes[1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    box_data = [metrics['base_similarity'], metrics['finetuned_similarity']]\n",
    "    bp = axes[2].boxplot(box_data, labels=['Base', 'Finetuned'], patch_artist=True,\n",
    "                          boxprops=dict(facecolor='lightblue', alpha=0.7),\n",
    "                          medianprops=dict(color='red', linewidth=2))\n",
    "    axes[2].set_ylabel('Cosine Similarity', fontsize=12)\n",
    "    axes[2].set_title('Overall Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[2].grid(True, alpha=0.3, axis='y')\n",
    "    axes[2].set_ylim([0, 1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    metrics_plot_path = COMPARISON_DIR / 'identity_similarity_metrics.png'\n",
    "    plt.savefig(str(metrics_plot_path), dpi=150, bbox_inches='tight')\n",
    "    print(f\"\\n✓ Saved metrics visualization: {metrics_plot_path}\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f983021",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "fig, axes = plt.subplots(NUM_COMPARISONS, 4, figsize=(16, 4*NUM_COMPARISONS))\n",
    "\n",
    "if NUM_COMPARISONS == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for i in range(NUM_COMPARISONS):\n",
    "\n",
    "    axes[i, 0].imshow(all_sources[i])\n",
    "    axes[i, 0].set_title(f'Source\\n{swap_info[i].split(\"→\")[0].strip()}', fontsize=12, fontweight='bold')\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "\n",
    "    axes[i, 1].imshow(all_targets[i])\n",
    "    axes[i, 1].set_title(f'Target\\n{swap_info[i].split(\"→\")[1].strip()}', fontsize=12, fontweight='bold')\n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "\n",
    "    axes[i, 2].imshow(all_base_results[i])\n",
    "    if FACENET_AVAILABLE and i < len(metrics['base_similarity']):\n",
    "        sim_score = metrics['base_similarity'][i]\n",
    "        title = f'Base Model\\n{swap_info[i]}\\nSimilarity: {sim_score:.3f}'\n",
    "    else:\n",
    "        title = f'Base Model\\n{swap_info[i]}'\n",
    "    axes[i, 2].set_title(title, fontsize=11, color='blue')\n",
    "    axes[i, 2].axis('off')\n",
    "    \n",
    "\n",
    "    axes[i, 3].imshow(all_finetuned_results[i])\n",
    "    if FACENET_AVAILABLE and i < len(metrics['finetuned_similarity']):\n",
    "        sim_score = metrics['finetuned_similarity'][i]\n",
    "        improvement = metrics['improvement'][i]\n",
    "        improvement_str = f\"+{improvement:.3f}\" if improvement > 0 else f\"{improvement:.3f}\"\n",
    "        title = f'Finetuned Model\\n{swap_info[i]}\\nSimilarity: {sim_score:.3f} (Δ{improvement_str})'\n",
    "        title_color = 'darkgreen' if improvement > 0 else 'darkorange'\n",
    "    else:\n",
    "        title = f'Finetuned Model\\n{swap_info[i]}'\n",
    "        title_color = 'green'\n",
    "    axes[i, 3].set_title(title, fontsize=11, color=title_color)\n",
    "    axes[i, 3].axis('off')\n",
    "\n",
    "plt.suptitle('Model Comparison: Base vs Finetuned Face Swaps with Identity Metrics', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "viz_path = COMPARISON_DIR / 'comparison_visualization.png'\n",
    "plt.savefig(str(viz_path), dpi=150, bbox_inches='tight')\n",
    "print(f\"\\n✓ Saved comparison visualization: {viz_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab444695",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cf5d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import random\n",
    "\n",
    "NUM_REENACT_TESTS = 5\n",
    "REENACT_COMPARISON_DIR = OUT_DIR / 'reenactment_only_comparison'\n",
    "REENACT_COMPARISON_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"REENACTMENT-ONLY COMPARISON: Base vs Finetuned\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "if 'Gr_base' not in dir():\n",
    "    print(\"\\n1. Loading BASE reenactment model...\")\n",
    "    reenact_base_w = WEIGHTS_DIR / 'nfv_msrunet_256_1_2_reenactment_v2.1.pth'\n",
    "    Gr_base, ckpt_base = load_model(str(reenact_base_w), 'reenactment', device=device, return_checkpoint=True)\n",
    "    Gr_base.eval()\n",
    "    print(f\"   ✓ Base model loaded: {ckpt_base.get('arch', 'unknown')}\")\n",
    "\n",
    "if 'Gr_finetuned_cmp' not in dir():\n",
    "    print(\"\\n2. Loading FINETUNED reenactment model...\")\n",
    "    finetuned_checkpoint = OUT_DIR / 'finetuned_models' / 'reenact_finetuned_epoch500.pth'\n",
    "    \n",
    "    if not finetuned_checkpoint.exists():\n",
    "        print(f\"Checkpoint not found: {finetuned_checkpoint}\")\n",
    "        print(\"   Looking for any available checkpoint...\")\n",
    "        available_ckpts = sorted((OUT_DIR / 'finetuned_models').glob('*.pth'))\n",
    "        if available_ckpts:\n",
    "            finetuned_checkpoint = available_ckpts[-1]\n",
    "            print(f\"   Using: {finetuned_checkpoint.name}\")\n",
    "        else:\n",
    "            raise FileNotFoundError(\"No finetuned checkpoints found!\")\n",
    "    \n",
    "    ckpt_finetuned = torch.load(str(finetuned_checkpoint), map_location=device)\n",
    "    arch = ckpt_finetuned.get('arch', 'res_unet.MultiScaleResUNet(in_nc=101,out_nc=3)')\n",
    "    Gr_finetuned_cmp = obj_factory(arch).to(device)\n",
    "    Gr_finetuned_cmp.load_state_dict(ckpt_finetuned['state_dict'])\n",
    "    Gr_finetuned_cmp.eval()\n",
    "    print(f\"Finetuned model loaded from epoch {ckpt_finetuned.get('epoch', '?')}\")\n",
    "\n",
    "\n",
    "if 'L_frozen' not in dir():\n",
    "    print(\"\\n3. Loading landmarks model...\")\n",
    "    lms_w = WEIGHTS_DIR / 'hr18_wflw_landmarks.pth'\n",
    "    L_frozen, _ = load_model(str(lms_w), 'landmarks', device=device, return_checkpoint=True)\n",
    "    L_frozen.eval()\n",
    "    for param in L_frozen.parameters():\n",
    "        param.requires_grad = False\n",
    "    print(\"Landmarks model loaded\")\n",
    "\n",
    "print(f\"\\n4. Dataset: {len(available_labels)} identities available\")\n",
    "print(f\"Will perform {NUM_REENACT_TESTS} cross-person reenactment tests\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "reenact_sources = []\n",
    "reenact_targets = []\n",
    "reenact_base_outputs = []\n",
    "reenact_finetuned_outputs = []\n",
    "reenact_info = []\n",
    "\n",
    "\n",
    "reenact_metrics = {\n",
    "    'base_similarity': [],\n",
    "    'finetuned_similarity': [],\n",
    "    'improvement': [],\n",
    "    'pixel_l1_base': [],\n",
    "    'pixel_l1_finetuned': []\n",
    "}\n",
    "\n",
    "print(\"\\nPerforming direct reenactment tests (cross-person)...\\n\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(NUM_REENACT_TESTS):\n",
    "        src_label, tgt_label = random.sample(available_labels, 2)\n",
    "        \n",
    "        src_idx = random.choice(train_identities[src_label])\n",
    "        tgt_idx = random.choice(train_identities[tgt_label])\n",
    "        \n",
    "        src_img, _ = train_dataset[src_idx]\n",
    "        tgt_img, _ = train_dataset[tgt_idx]\n",
    "        \n",
    "        src_batch = src_img.unsqueeze(0).to(device)\n",
    "        tgt_batch = tgt_img.unsqueeze(0).to(device)\n",
    "        \n",
    "        print(f\"[{i+1}/{NUM_REENACT_TESTS}] Source ID {src_label} -> Target ID {tgt_label} (cross-person)\")\n",
    "        \n",
    "        input_list, tgt_normalized = prepare_reenactment_input(\n",
    "            src_batch, tgt_batch, L_frozen, n_levels, device\n",
    "        )\n",
    "        \n",
    "\n",
    "        src_np = ((src_img.permute(1, 2, 0).cpu().numpy() + 1) / 2 * 255).clip(0, 255).astype('uint8')\n",
    "        tgt_np = ((tgt_img.permute(1, 2, 0).cpu().numpy() + 1) / 2 * 255).clip(0, 255).astype('uint8')\n",
    "        \n",
    "        if FACENET_AVAILABLE:\n",
    "            src_embedding = get_facenet_embedding(src_np)\n",
    "        \n",
    "\n",
    "        output_base = Gr_base(input_list)\n",
    "        pred_base = output_base[-1] if isinstance(output_base, (list, tuple)) else output_base\n",
    "        pred_base_np = tensor2bgr(pred_base[0])\n",
    "        pred_base_rgb = cv2.cvtColor(pred_base_np, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "\n",
    "        if FACENET_AVAILABLE:\n",
    "            base_emb = get_facenet_embedding(pred_base_rgb)\n",
    "            base_sim = cosine_similarity(src_embedding, base_emb)\n",
    "            reenact_metrics['base_similarity'].append(base_sim)\n",
    "        \n",
    "\n",
    "        pixel_l1_base = F.l1_loss(pred_base, tgt_normalized).item()\n",
    "        reenact_metrics['pixel_l1_base'].append(pixel_l1_base)\n",
    "        \n",
    "\n",
    "        output_finetuned = Gr_finetuned_cmp(input_list)\n",
    "        pred_finetuned = output_finetuned[-1] if isinstance(output_finetuned, (list, tuple)) else output_finetuned\n",
    "        pred_finetuned_np = tensor2bgr(pred_finetuned[0])\n",
    "        pred_finetuned_rgb = cv2.cvtColor(pred_finetuned_np, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if FACENET_AVAILABLE:\n",
    "            finetuned_emb = get_facenet_embedding(pred_finetuned_rgb)\n",
    "            finetuned_sim = cosine_similarity(src_embedding, finetuned_emb)\n",
    "            reenact_metrics['finetuned_similarity'].append(finetuned_sim)\n",
    "            \n",
    "            improvement = finetuned_sim - base_sim\n",
    "            reenact_metrics['improvement'].append(improvement)\n",
    "            \n",
    "            improvement_str = f\"+{improvement:.4f}\" if improvement > 0 else f\"{improvement:.4f}\"\n",
    "            print(f\"  Identity similarity - Base: {base_sim:.4f}, Finetuned: {finetuned_sim:.4f}, Δ: {improvement_str}\")\n",
    "        \n",
    "        pixel_l1_finetuned = F.l1_loss(pred_finetuned, tgt_normalized).item()\n",
    "        reenact_metrics['pixel_l1_finetuned'].append(pixel_l1_finetuned)\n",
    "        \n",
    "        pixel_improvement = pixel_l1_base - pixel_l1_finetuned\n",
    "        pixel_improvement_str = f\"-{abs(pixel_improvement):.4f}\" if pixel_improvement > 0 else f\"+{abs(pixel_improvement):.4f}\"\n",
    "        print(f\"  Pixel L1 loss     - Base: {pixel_l1_base:.4f}, Finetuned: {pixel_l1_finetuned:.4f}, Δ: {pixel_improvement_str}\")\n",
    "        \n",
    "        reenact_sources.append(src_np)\n",
    "        reenact_targets.append(tgt_np)\n",
    "        reenact_base_outputs.append(pred_base_rgb)\n",
    "        reenact_finetuned_outputs.append(pred_finetuned_rgb)\n",
    "        reenact_info.append(f\"ID {src_label} → ID {tgt_label}\")\n",
    "        \n",
    "        cv2.imwrite(str(REENACT_COMPARISON_DIR / f'reenact_{i}_source_id{src_label}.png'), \n",
    "                    cv2.cvtColor(src_np, cv2.COLOR_RGB2BGR))\n",
    "        cv2.imwrite(str(REENACT_COMPARISON_DIR / f'reenact_{i}_target_id{tgt_label}.png'), \n",
    "                    cv2.cvtColor(tgt_np, cv2.COLOR_RGB2BGR))\n",
    "        cv2.imwrite(str(REENACT_COMPARISON_DIR / f'reenact_{i}_base_id{src_label}_to_id{tgt_label}.png'), \n",
    "                    pred_base_np)\n",
    "        cv2.imwrite(str(REENACT_COMPARISON_DIR / f'reenact_{i}_finetuned_id{src_label}_to_id{tgt_label}.png'), \n",
    "                    pred_finetuned_np)\n",
    "        \n",
    "        print()\n",
    "\n",
    "print(f\"Completed {len(reenact_sources)} cross-person reenactment tests\")\n",
    "print(f\"Results saved to: {REENACT_COMPARISON_DIR}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"REENACTMENT METRICS SUMMARY (Cross-Person)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if FACENET_AVAILABLE and reenact_metrics['base_similarity']:\n",
    "    print(\"\\nIdentity Preservation (FaceNet Cosine Similarity):\")\n",
    "    print(f\"  Base Model:      {np.mean(reenact_metrics['base_similarity']):.4f} ± {np.std(reenact_metrics['base_similarity']):.4f}\")\n",
    "    print(f\"  Finetuned Model: {np.mean(reenact_metrics['finetuned_similarity']):.4f} ± {np.std(reenact_metrics['finetuned_similarity']):.4f}\")\n",
    "    print(f\"  Avg Improvement: {np.mean(reenact_metrics['improvement']):.4f}\")\n",
    "    print(f\"  Better results:  {sum(1 for x in reenact_metrics['improvement'] if x > 0)}/{len(reenact_metrics['improvement'])} tests\")\n",
    "\n",
    "if reenact_metrics['pixel_l1_base']:\n",
    "    print(\"\\nPixel Reconstruction (L1 Loss with Target):\")\n",
    "    print(f\"  Base Model:      {np.mean(reenact_metrics['pixel_l1_base']):.4f} ± {np.std(reenact_metrics['pixel_l1_base']):.4f}\")\n",
    "    print(f\"  Finetuned Model: {np.mean(reenact_metrics['pixel_l1_finetuned']):.4f} ± {np.std(reenact_metrics['pixel_l1_finetuned']):.4f}\")\n",
    "    \n",
    "    pixel_improvements = [reenact_metrics['pixel_l1_base'][i] - reenact_metrics['pixel_l1_finetuned'][i] \n",
    "                         for i in range(len(reenact_metrics['pixel_l1_base']))]\n",
    "    print(f\"  Avg Improvement: {np.mean(pixel_improvements):.4f} (lower is better)\")\n",
    "    print(f\"  Better results:  {sum(1 for x in pixel_improvements if x > 0)}/{len(pixel_improvements)} tests\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a2f712",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "\n",
    "if reenact_sources:\n",
    "    n_tests = len(reenact_sources)\n",
    "    \n",
    "\n",
    "    fig, axes = plt.subplots(n_tests, 4, figsize=(16, 4*n_tests))\n",
    "    \n",
    "    if n_tests == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i in range(n_tests):\n",
    "\n",
    "        axes[i, 0].imshow(reenact_sources[i])\n",
    "        axes[i, 0].set_title(f'Source\\n{reenact_info[i]}', fontsize=12, fontweight='bold')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        axes[i, 1].imshow(reenact_targets[i])\n",
    "        axes[i, 1].set_title(f'Target (Driving)\\n{reenact_info[i]}', fontsize=12, fontweight='bold')\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "\n",
    "        axes[i, 2].imshow(reenact_base_outputs[i])\n",
    "        if FACENET_AVAILABLE and i < len(reenact_metrics['base_similarity']):\n",
    "            sim = reenact_metrics['base_similarity'][i]\n",
    "            l1 = reenact_metrics['pixel_l1_base'][i]\n",
    "            title = f'Base Reenactment\\nID Sim: {sim:.3f} | L1: {l1:.4f}'\n",
    "        else:\n",
    "            title = f'Base Reenactment\\n{reenact_info[i]}'\n",
    "        axes[i, 2].set_title(title, fontsize=11, color='blue')\n",
    "        axes[i, 2].axis('off')\n",
    "        \n",
    "        axes[i, 3].imshow(reenact_finetuned_outputs[i])\n",
    "        if FACENET_AVAILABLE and i < len(reenact_metrics['finetuned_similarity']):\n",
    "            sim = reenact_metrics['finetuned_similarity'][i]\n",
    "            l1 = reenact_metrics['pixel_l1_finetuned'][i]\n",
    "            imp = reenact_metrics['improvement'][i]\n",
    "            imp_str = f\"+{imp:.3f}\" if imp > 0 else f\"{imp:.3f}\"\n",
    "            title = f'Finetuned Reenactment\\nID Sim: {sim:.3f} (Δ{imp_str}) | L1: {l1:.4f}'\n",
    "            title_color = 'darkgreen' if imp > 0 else 'darkorange'\n",
    "        else:\n",
    "            title = f'Finetuned Reenactment\\n{reenact_info[i]}'\n",
    "            title_color = 'green'\n",
    "        axes[i, 3].set_title(title, fontsize=11, color=title_color)\n",
    "        axes[i, 3].axis('off')\n",
    "    \n",
    "    plt.suptitle('Direct Reenactment Comparison: Base vs Finetuned (No Pipeline)', \n",
    "                 fontsize=16, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    viz_path = REENACT_COMPARISON_DIR / 'reenactment_comparison_grid.png'\n",
    "    plt.savefig(str(viz_path), dpi=150, bbox_inches='tight')\n",
    "    print(f\"\\nSaved visualization: {viz_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    if FACENET_AVAILABLE and reenact_metrics['base_similarity']:\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "        \n",
    "        x = np.arange(n_tests)\n",
    "        width = 0.35\n",
    "        \n",
    "        axes[0].bar(x - width/2, reenact_metrics['base_similarity'], width, \n",
    "                   label='Base', color='steelblue', alpha=0.8)\n",
    "        axes[0].bar(x + width/2, reenact_metrics['finetuned_similarity'], width, \n",
    "                   label='Finetuned', color='seagreen', alpha=0.8)\n",
    "        axes[0].set_xlabel('Test Index', fontsize=12)\n",
    "        axes[0].set_ylabel('Identity Similarity', fontsize=12)\n",
    "        axes[0].set_title('Identity Preservation', fontsize=14, fontweight='bold')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3, axis='y')\n",
    "        axes[0].set_ylim([0, 1])\n",
    "        \n",
    "        axes[1].bar(x - width/2, reenact_metrics['pixel_l1_base'], width, \n",
    "                   label='Base', color='coral', alpha=0.8)\n",
    "        axes[1].bar(x + width/2, reenact_metrics['pixel_l1_finetuned'], width, \n",
    "                   label='Finetuned', color='lightgreen', alpha=0.8)\n",
    "        axes[1].set_xlabel('Test Index', fontsize=12)\n",
    "        axes[1].set_ylabel('L1 Loss', fontsize=12)\n",
    "        axes[1].set_title('Pixel Reconstruction (Lower = Better)', fontsize=14, fontweight='bold')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        colors = ['green' if x > 0 else 'red' for x in reenact_metrics['improvement']]\n",
    "        axes[2].bar(x, reenact_metrics['improvement'], color=colors, alpha=0.7)\n",
    "        axes[2].axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
    "        axes[2].set_xlabel('Test Index', fontsize=12)\n",
    "        axes[2].set_ylabel('Identity Similarity delta', fontsize=12)\n",
    "        axes[2].set_title('Identity Improvement', fontsize=14, fontweight='bold')\n",
    "        axes[2].grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        box_data = [\n",
    "            reenact_metrics['base_similarity'],\n",
    "            reenact_metrics['finetuned_similarity']\n",
    "        ]\n",
    "        bp = axes[3].boxplot(box_data, labels=['Base', 'Finetuned'], \n",
    "                            patch_artist=True,\n",
    "                            boxprops=dict(facecolor='lightblue', alpha=0.7),\n",
    "                            medianprops=dict(color='red', linewidth=2))\n",
    "        axes[3].set_ylabel('Identity Similarity', fontsize=12)\n",
    "        axes[3].set_title('Distribution', fontsize=14, fontweight='bold')\n",
    "        axes[3].grid(True, alpha=0.3, axis='y')\n",
    "        axes[3].set_ylim([0, 1])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        metrics_viz_path = REENACT_COMPARISON_DIR / 'reenactment_metrics.png'\n",
    "        plt.savefig(str(metrics_viz_path), dpi=150, bbox_inches='tight')\n",
    "        print(f\"Saved metrics visualization: {metrics_viz_path}\")\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b43da4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
