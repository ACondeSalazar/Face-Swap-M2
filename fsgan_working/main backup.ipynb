{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daeab10c",
   "metadata": {},
   "source": [
    "# fsgan quick model tests\n",
    "This notebook contains small, self-contained tests that: load model weights from `fsgan/weights/`,\n",
    "run a quick forward pass (or the provided inference pipeline) on two images `a.jpg` and `j.jpg` located\n",
    "in the repository root, and write outputs to an `outputs/` folder.\n",
    "\n",
    "Cells: 1) env & imports, 2) paths & helpers, 3) segmentation test, 4) landmarks test, 5) reenactment test, 6) full swap test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0923c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing packages: ['opencv_python']\n",
      "Installing missing packages into the current Python environment. This may take a few minutes.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Check and (optionally) install minimal dependencies\n",
    "import sys, subprocess, importlib\n",
    "pkgs = ['torch', 'torchvision', 'numpy', 'opencv_python', 'tqdm', 'matplotlib']\n",
    "missing = []\n",
    "for p in pkgs:\n",
    "    try:\n",
    "        importlib.import_module(p)\n",
    "    except Exception:\n",
    "        missing.append(p)\n",
    "\n",
    "if missing:\n",
    "    print('Missing packages:', missing)\n",
    "    print('Installing missing packages into the current Python environment. This may take a few minutes.')\n",
    "    cmd = [sys.executable, '-m', 'pip', 'install'] + missing\n",
    "    subprocess.check_call(cmd)\n",
    "else:\n",
    "    print('All minimal packages present')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e53a460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights directory: fsgan\\weights\n",
      "Expecting images at: j.jpg a.jpg\n",
      "Outputs will be saved to: outputs\n",
      "Using device cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: imports, paths and helpers\n",
    "import os, sys, traceback\n",
    "from pathlib import Path\n",
    "import torch, cv2, numpy as np\n",
    "from fsgan.utils.utils import load_model\n",
    "\n",
    "ROOT = Path('.')\n",
    "WEIGHTS_DIR = ROOT / 'fsgan' / 'weights'\n",
    "OUT_DIR = ROOT / 'outputs'\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "IMG_A = ROOT / 'j.jpg'\n",
    "IMG_J = ROOT / 'a.jpg'\n",
    "\n",
    "print('Weights directory:', WEIGHTS_DIR)\n",
    "print('Expecting images at:', IMG_A, IMG_J)\n",
    "print('Outputs will be saved to:', OUT_DIR)\n",
    "\n",
    "def load_image_as_tensor(p, size=256, device=None):\n",
    "    im = cv2.imread(str(p))\n",
    "    if im is None:\n",
    "        raise FileNotFoundError(f'Image not found: {p}')\n",
    "    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "    im = cv2.resize(im, (size, size), interpolation=cv2.INTER_AREA)\n",
    "    im = im.astype('float32') / 255.0\n",
    "    t = torch.from_numpy(im.transpose(2,0,1)).unsqueeze(0)\n",
    "    if device is not None:\n",
    "        t = t.to(device)\n",
    "    return t\n",
    "\n",
    "def save_mask(mask, outpath):\n",
    "    # mask: HxW uint8 or 2D numpy\n",
    "    cv2.imwrite(str(outpath), mask)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d79ea7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Loading segmentation model: \"celeba_unet_256_1_2_segmentation_v2.pth\"...\n",
      "Segmentation saved to outputs\\segmentation\\a_seg_mask.png\n",
      "Segmentation saved to outputs\\segmentation\\a_seg_mask.png\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Segmentation model quick test\n",
    "seg_w = WEIGHTS_DIR / 'celeba_unet_256_1_2_segmentation_v2.pth'\n",
    "out_seg_dir = OUT_DIR / 'segmentation'\n",
    "out_seg_dir.mkdir(exist_ok=True)\n",
    "try:\n",
    "    model_seg = load_model(str(seg_w), 'segmentation', device=device)\n",
    "    model_seg.eval()\n",
    "    t = load_image_as_tensor(IMG_A, size=256, device=device)\n",
    "    with torch.no_grad():\n",
    "        pred = model_seg(t)\n",
    "    # handle logits: if shape (B,C,H,W) -> argmax across channel\n",
    "    pred_np = pred.detach().cpu().numpy()\n",
    "    if pred_np.ndim == 4:\n",
    "        mask = pred_np.argmax(1)[0].astype('uint8') * 85  # 3 classes -> scale for visualization\n",
    "    else:\n",
    "        # fallback: save raw single-channel output\n",
    "        mask = (pred_np[0,0] * 255).astype('uint8')\n",
    "    out_path = out_seg_dir / 'a_seg_mask.png'\n",
    "    save_mask(mask, out_path)\n",
    "    print('Segmentation saved to', out_path)\n",
    "except Exception as e:\n",
    "    print('Segmentation test failed:', e)\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae9b9b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Loading landmarks model: \"hr18_wflw_landmarks.pth\"...\n",
      "Landmarks model forward output shape: torch.Size([1, 98, 64, 64])\n",
      "Saved example landmark heatmap to outputs\\landmarks\\a_landmark_heatmap_ch0.png\n",
      "Landmarks model forward output shape: torch.Size([1, 98, 64, 64])\n",
      "Saved example landmark heatmap to outputs\\landmarks\\a_landmark_heatmap_ch0.png\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Landmarks model quick test (shape/forward check)\n",
    "lms_w = WEIGHTS_DIR / 'hr18_wflw_landmarks.pth'\n",
    "out_lms_dir = OUT_DIR / 'landmarks'\n",
    "out_lms_dir.mkdir(exist_ok=True)\n",
    "try:\n",
    "    model_lms = load_model(str(lms_w), 'landmarks', device=device)\n",
    "    model_lms.eval()\n",
    "    t = load_image_as_tensor(IMG_A, size=256, device=device)\n",
    "    with torch.no_grad():\n",
    "        out = model_lms(t)\n",
    "    print('Landmarks model forward output shape:', getattr(out, 'shape', None))\n",
    "    # If output looks like heatmaps (B,C,H,W), save the first channel as an example heatmap\n",
    "    try:\n",
    "        out_np = out.detach().cpu().numpy()\n",
    "        if out_np.ndim == 4:\n",
    "            hm = out_np[0,0]\n",
    "            hm = (255 * (hm - hm.min()) / (hm.max() - hm.min() + 1e-8)).astype('uint8')\n",
    "            cv2.imwrite(str(out_lms_dir / 'a_landmark_heatmap_ch0.png'), hm)\n",
    "            print('Saved example landmark heatmap to', out_lms_dir / 'a_landmark_heatmap_ch0.png')\n",
    "    except Exception as e:\n",
    "        print('Could not save landmark heatmap:', e)\n",
    "except Exception as e:\n",
    "    print('Landmarks test failed:', e)\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5748a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "Reenact model: fsgan\\weights\\nfv_msrunet_256_1_2_reenactment_v2.1.pth\n",
      "Landmarks model: fsgan\\weights\\hr18_wflw_landmarks.pth\n",
      "Source: a.jpg Target: j.jpg\n",
      "=> Loading reenactment model: \"nfv_msrunet_256_1_2_reenactment_v2.1.pth\"...\n",
      "Checkpoint arch: res_unet.MultiScaleResUNet(in_nc=101,out_nc=3,flat_layers=(2,2,2,2),ngf=128)\n",
      "=> Loading landmarks model: \"hr18_wflw_landmarks.pth\"...\n",
      "Checkpoint arch: res_unet.MultiScaleResUNet(in_nc=101,out_nc=3,flat_layers=(2,2,2,2),ngf=128)\n",
      "=> Loading landmarks model: \"hr18_wflw_landmarks.pth\"...\n",
      "Loaded landmarks using utils.load_model()\n",
      "Generator n_local_enhancers = 1 ; building pyramid with 2 levels\n",
      "Pyramid levels: 2 shapes: [torch.Size([1, 3, 256, 256]), torch.Size([1, 3, 128, 128])]\n",
      "Input pyramid shapes for generator: [torch.Size([1, 101, 256, 256]), torch.Size([1, 101, 128, 128])]\n",
      "Loaded landmarks using utils.load_model()\n",
      "Generator n_local_enhancers = 1 ; building pyramid with 2 levels\n",
      "Pyramid levels: 2 shapes: [torch.Size([1, 3, 256, 256]), torch.Size([1, 3, 128, 128])]\n",
      "Input pyramid shapes for generator: [torch.Size([1, 101, 256, 256]), torch.Size([1, 101, 128, 128])]\n",
      "Saved reenactment to: outputs\\reenact_a_to_j.png\n",
      "Saved reenactment to: outputs\\reenact_a_to_j.png\n"
     ]
    }
   ],
   "source": [
    "# Cell 6 (updated): Robust minimal reenactment forward (single frame)\n",
    "import torch, cv2, torch.nn.functional as F, traceback\n",
    "from fsgan.utils import utils, img_utils, landmarks_utils\n",
    "from fsgan.models.hrnet import hrnet_wlfw\n",
    "from pathlib import Path\n",
    "\n",
    "# Use device already defined in cell 3 if present, else set one\n",
    "try:\n",
    "    device\n",
    "except NameError:\n",
    "    device, _ = utils.set_device(None)\n",
    "\n",
    "# Paths (prefer the variables defined in cell 3 if available)\n",
    "ROOT = Path('.')\n",
    "reenact_w = ROOT / 'fsgan' / 'weights' / 'nfv_msrunet_256_1_2_reenactment_v2.1.pth'\n",
    "lms_w = ROOT / 'fsgan' / 'weights' / 'hr18_wflw_landmarks.pth'\n",
    "src_path = ROOT / 'a.jpg'\n",
    "tgt_path = ROOT / 'j.jpg'\n",
    "out_path = ROOT / 'outputs' / 'reenact_a_to_j.png'\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('Device:', device)\n",
    "print('Reenact model:', reenact_w)\n",
    "print('Landmarks model:', lms_w)\n",
    "print('Source:', src_path, 'Target:', tgt_path)\n",
    "\n",
    "# Helper: read BGR image and convert to float tensor in [0,1]\n",
    "def read_bgr_tensor(p, device=device, size=256):\n",
    "    im = cv2.imread(str(p))\n",
    "    if im is None:\n",
    "        raise FileNotFoundError(f'Image not found: {p}')\n",
    "    # Resize to a consistent base resolution (models trained on 256x256)\n",
    "    if size is not None:\n",
    "        im = cv2.resize(im, (size, size), interpolation=cv2.INTER_AREA)\n",
    "    return img_utils.bgr2tensor(im, normalize=False).to(device)\n",
    "\n",
    "# Load reenactment generator (this checkpoint should contain 'arch' + 'state_dict')\n",
    "try:\n",
    "    Gr, ckpt = utils.load_model(str(reenact_w), 'reenactment', device=device, return_checkpoint=True)\n",
    "    Gr.eval()\n",
    "    # print architecture if available\n",
    "    try:\n",
    "        print('Checkpoint arch:', ckpt.get('arch', None))\n",
    "    except Exception:\n",
    "        pass\n",
    "except Exception as e:\n",
    "    print('Failed loading reenactment model:', e)\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "# Load landmarks model: try load_model first, fallback to hrnet_wlfw + raw state_dict\n",
    "try:\n",
    "    L = None\n",
    "    try:\n",
    "        L = utils.load_model(str(lms_w), 'landmarks', device=device)\n",
    "        print('Loaded landmarks using utils.load_model()')\n",
    "    except AssertionError:\n",
    "        # checkpoint is probably a raw state_dict -> fallback\n",
    "        print('Landmarks file looks like raw state_dict, falling back to hrnet factory')\n",
    "    if L is None:\n",
    "        L = hrnet_wlfw().to(device)\n",
    "        state_dict = torch.load(str(lms_w), map_location=device)\n",
    "        L.load_state_dict(state_dict)\n",
    "    L.eval()\n",
    "except Exception as e:\n",
    "    print('Failed loading landmarks model:', e)\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "# Read images as tensors [0,1] and resize to model base resolution\n",
    "base_res = 256\n",
    "try:\n",
    "    src_t = read_bgr_tensor(src_path, device=device, size=base_res)\n",
    "    tgt_t = read_bgr_tensor(tgt_path, device=device, size=base_res)\n",
    "except Exception as e:\n",
    "    print('Image loading error:', e)\n",
    "    raise\n",
    "\n",
    "# Determine the correct pyramid levels to match the generator's n_local_enhancers\n",
    "# The model expects len(pyd) == n_local_enhancers + 1\n",
    "try:\n",
    "    n_local = getattr(Gr, 'n_local_enhancers', None)\n",
    "    if n_local is None and hasattr(Gr, 'module'):\n",
    "        n_local = getattr(Gr.module, 'n_local_enhancers', None)\n",
    "    if n_local is None:\n",
    "        # fallback to 1 (common default)\n",
    "        n_local = 1\n",
    "    required_levels = int(n_local) + 1\n",
    "    print('Generator n_local_enhancers =', n_local, '; building pyramid with', required_levels, 'levels')\n",
    "except Exception as e:\n",
    "    print('Could not determine generator n_local_enhancers:', e)\n",
    "    required_levels = 3\n",
    "\n",
    "# Create pyramid with the required number of levels\n",
    "src_pyd = img_utils.create_pyramid(src_t, n=required_levels)\n",
    "print('Pyramid levels:', len(src_pyd), 'shapes:', [p.shape for p in src_pyd])\n",
    "\n",
    "# Normalization tensors (training uses these values)\n",
    "img_mean = torch.as_tensor([0.5,0.5,0.5], device=device).view(1,3,1,1)\n",
    "img_std  = torch.as_tensor([0.5,0.5,0.5], device=device).view(1,3,1,1)\n",
    "context_mean = torch.as_tensor([0.485,0.456,0.406], device=device).view(1,3,1,1)\n",
    "context_std  = torch.as_tensor([0.229,0.224,0.225], device=device).view(1,3,1,1)\n",
    "\n",
    "# Compute landmarks context from the target (hrnet expects ImageNet-normalized input)\n",
    "with torch.no_grad():\n",
    "    tgt_for_lms = (tgt_t - context_mean) / context_std\n",
    "    context = L(tgt_for_lms)\n",
    "    context = landmarks_utils.filter_landmarks(context)\n",
    "\n",
    "# Normalize source pyramid to [-1,1] as training did\n",
    "for i in range(len(src_pyd)):\n",
    "    src_pyd[i] = (src_pyd[i] - img_mean) / img_std\n",
    "\n",
    "# Build generator input: for each pyramid level concatenate image + resized context (training order preserved)\n",
    "inp = []\n",
    "for p in range(len(src_pyd)-1, -1, -1):\n",
    "    c = F.interpolate(context, size=src_pyd[p].shape[2:], mode='bicubic', align_corners=False)\n",
    "    inp.insert(0, torch.cat((src_pyd[p], c), dim=1))\n",
    "\n",
    "print('Input pyramid shapes for generator:', [x.shape for x in inp])\n",
    "\n",
    "# Run generator and save output (handle single-tensor and list outputs)\n",
    "with torch.no_grad():\n",
    "    try:\n",
    "        reenact_img = Gr(inp)\n",
    "    except Exception as e:\n",
    "        print('Generator forward failed:', e)\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "# If model returns a list/pyramid, take the last (highest res) or the tensor itself\n",
    "if isinstance(reenact_img, (list, tuple)):\n",
    "    out_tensor = reenact_img[-1]\n",
    "else:\n",
    "    out_tensor = reenact_img\n",
    "\n",
    "# Move to CPU and convert to BGR uint8 for saving\n",
    "out_bgr = img_utils.tensor2bgr(out_tensor[0].cpu())\n",
    "cv2.imwrite(str(out_path), out_bgr)\n",
    "print('Saved reenactment to:', out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d6ec545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running full pipeline: a.jpg -> j.jpg\n",
      "=> Loading reenactment model: \"nfv_msrunet_256_1_2_reenactment_v2.1.pth\"...\n",
      "=> Loading landmarks model: \"hr18_wflw_landmarks.pth\"...\n",
      "=> Loading landmarks model: \"hr18_wflw_landmarks.pth\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arthur\\Documents\\Github\\fsgan\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Arthur\\Documents\\Github\\fsgan\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Loading segmentation model: \"celeba_unet_256_1_2_segmentation_v2.pth\"...\n",
      "=> Loading completion model: \"ijbc_msrunet_256_1_2_inpainting_v2.pth\"...\n",
      "=> Loading completion model: \"ijbc_msrunet_256_1_2_inpainting_v2.pth\"...\n",
      "=> Loading blending model: \"ijbc_msrunet_256_1_2_blending_v2.pth\"...\n",
      "=> Loading blending model: \"ijbc_msrunet_256_1_2_blending_v2.pth\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arthur\\Documents\\Github\\fsgan\\.venv\\Lib\\site-packages\\torch\\functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3638.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full composited result to outputs\\reenact_full_composited.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arthur\\AppData\\Local\\Temp\\ipykernel_16692\\1023022259.py:18: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.figure(figsize=(6,6)); plt.axis('off'); plt.imshow(img); plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Cell 6b: Full pipeline inline (preprocessing + segmentation + inpainting + blending)\n",
    "from pathlib import Path\n",
    "from fsgan.notebook_helpers.reenact_preprocess import run_full_pipeline\n",
    "\n",
    "OUT_DIR = Path('outputs')\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "src = str(Path('a.jpg'))\n",
    "tgt = str(Path('j.jpg'))\n",
    "out_file = str(OUT_DIR / 'reenact_full_composited.png')\n",
    "print('Running full pipeline:', src, '->', tgt)\n",
    "# This may take some time while models are loaded onto the device\n",
    "result_bgr, intermediates, src_crop, tgt_crop = run_full_pipeline(src, tgt, out_path=out_file)\n",
    "print('Saved full composited result to', out_file)\n",
    "# If running inside notebook show via matplotlib (optional)\n",
    "try:\n",
    "    import matplotlib.pyplot as plt, cv2\n",
    "    img = cv2.cvtColor(result_bgr, cv2.COLOR_BGR2RGB)\n",
    "    plt.figure(figsize=(6,6)); plt.axis('off'); plt.imshow(img); plt.show()\n",
    "except Exception as e:\n",
    "    print('Could not display image inline:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4628d686",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arthur\\Documents\\Github\\fsgan\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Arthur\\Documents\\Github\\fsgan\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> using GPU devices: 0\n",
      "=> Loading face pose model: \"hopenet_robust_alpha1.pth\"...\n",
      "=> Loading face landmarks model: \"hr18_wflw_landmarks.pth\"...\n",
      "=> Loading face landmarks model: \"hr18_wflw_landmarks.pth\"...\n",
      "=> Loading face segmentation model: \"celeba_unet_256_1_2_segmentation_v2.pth\"...\n",
      "=> Loading face segmentation model: \"celeba_unet_256_1_2_segmentation_v2.pth\"...\n",
      "=> Loading face reenactment model: \"nfv_msrunet_256_1_2_reenactment_v2.1.pth\"...\n",
      "=> Loading face reenactment model: \"nfv_msrunet_256_1_2_reenactment_v2.1.pth\"...\n",
      "=> Loading face completion model: \"ijbc_msrunet_256_1_2_inpainting_v2.pth\"...\n",
      "=> Loading face completion model: \"ijbc_msrunet_256_1_2_inpainting_v2.pth\"...\n",
      "=> Loading face blending model: \"ijbc_msrunet_256_1_2_blending_v2.pth\"...\n",
      "=> Loading face blending model: \"ijbc_msrunet_256_1_2_blending_v2.pth\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arthur\\Documents\\Github\\fsgan\\.venv\\Lib\\site-packages\\torch\\__init__.py:1236: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\tensor\\python_tensor.cpp:436.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Detecting faces in video: \"j.jpg...\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  4.18frames/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Extracting sequences from detections in video: \"j.jpg\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 13189.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Cropping image sequences from image: \"j.jpg\"...\n",
      "=> Computing face poses for video: \"j_seq00.jpg\"...\n",
      "Swap pipeline failed: [WinError 2] Le fichier spécifié est introuvable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Arthur\\AppData\\Local\\Temp\\ipykernel_16692\\2496922654.py\", line 6, in <module>\n",
      "    swap_mod.main([str(IMG_A)], [str(IMG_J)], output=str(out_swap))\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Arthur\\Documents\\Github\\fsgan\\fsgan\\inference\\swap.py\", line 498, in main\n",
      "    face_swapping(source[0], target[0], output, select_source, select_target)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Arthur\\Documents\\Github\\fsgan\\fsgan\\inference\\swap.py\", line 239, in __call__\n",
      "    source_cache_dir, source_seq_file_path, _ = self.cache(source_path)\n",
      "                                                ~~~~~~~~~~^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Arthur\\Documents\\Github\\fsgan\\fsgan\\preprocess\\preprocess_video.py\", line 469, in cache\n",
      "    self.process_pose(input_path, output_dir, seq_file_path)\n",
      "    ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Arthur\\Documents\\Github\\fsgan\\fsgan\\preprocess\\preprocess_video.py\", line 247, in process_pose\n",
      "    in_vid = VideoInferenceDataset(curr_vid_path, transform=img_transforms)\n",
      "  File \"c:\\Users\\Arthur\\Documents\\Github\\fsgan\\fsgan\\datasets\\video_inference_dataset.py\", line 29, in __init__\n",
      "    self.width, self.height, self.total_frames, self.fps = get_video_info(vid_path)\n",
      "                                                           ~~~~~~~~~~~~~~^^^^^^^^^^\n",
      "  File \"c:\\Users\\Arthur\\Documents\\Github\\fsgan\\fsgan\\utils\\video_utils.py\", line 223, in get_video_info\n",
      "    return get_media_info(vid_path)\n",
      "  File \"c:\\Users\\Arthur\\Documents\\Github\\fsgan\\fsgan\\utils\\video_utils.py\", line 206, in get_media_info\n",
      "    probe = ffmpeg.probe(media_path)\n",
      "  File \"c:\\Users\\Arthur\\Documents\\Github\\fsgan\\.venv\\Lib\\site-packages\\ffmpeg\\_probe.py\", line 20, in probe\n",
      "    p = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
      "  File \"C:\\Users\\Arthur\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py\", line 1039, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                        pass_fds, cwd, env,\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "                        gid, gids, uid, umask,\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^\n",
      "                        start_new_session, process_group)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Arthur\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py\", line 1554, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\n",
      "                             # no special security\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<4 lines>...\n",
      "                             cwd,\n",
      "                             ^^^^\n",
      "                             startupinfo)\n",
      "                             ^^^^^^^^^^^^\n",
      "FileNotFoundError: [WinError 2] Le fichier spécifié est introuvable\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Full swap pipeline quick test (this will run reenact+segmentation+inpainting+blending)\n",
    "from fsgan.inference import swap as swap_mod\n",
    "out_swap = OUT_DIR / 'swap'\n",
    "out_swap.mkdir(exist_ok=True)\n",
    "try:\n",
    "    swap_mod.main([str(IMG_A)], [str(IMG_J)], output=str(out_swap))\n",
    "    print('Swap pipeline completed. Check', out_swap)\n",
    "except Exception as e:\n",
    "    print('Swap pipeline failed:', e)\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
