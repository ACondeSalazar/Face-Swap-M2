{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daeab10c",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e8b822",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ffmpeg-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0923c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, subprocess, importlib\n",
    "pkgs = ['torch', 'torchvision', 'numpy', 'opencv_python', 'tqdm', 'matplotlib']\n",
    "missing = []\n",
    "for p in pkgs:\n",
    "    try:\n",
    "        importlib.import_module(p)\n",
    "    except Exception:\n",
    "        missing.append(p)\n",
    "\n",
    "if missing:\n",
    "    print('Missing packages:', missing)\n",
    "    print('Installing missing packages into the current Python environment. This may take a few minutes.')\n",
    "    cmd = [sys.executable, '-m', 'pip', 'install'] + missing\n",
    "    subprocess.check_call(cmd)\n",
    "else:\n",
    "    print('All minimal packages present')\n",
    "\n",
    "\n",
    "import os\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from fsgan.utils.utils import load_model\n",
    "from fsgan.utils import img_utils, landmarks_utils\n",
    "from fsgan.models.hrnet import hrnet_wlfw\n",
    "from fsgan.inference import swap as swap_mod\n",
    "\n",
    "from fsgan.notebook_helpers.reenact_preprocess import run_full_pipeline\n",
    "\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "ROOT = Path('.')\n",
    "WEIGHTS_DIR = ROOT / 'fsgan' / 'weights'\n",
    "OUT_DIR = ROOT / 'outputs'\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "IMG_A = ROOT / 'j.jpg'\n",
    "IMG_J = ROOT / 'a.jpg'\n",
    "print('Weights directory:', WEIGHTS_DIR)\n",
    "print('Expecting images at:', IMG_A, IMG_J)\n",
    "print('Outputs will be saved to:', OUT_DIR)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device', device)\n",
    "\n",
    "def load_image_as_tensor(p, size=256, device=None):\n",
    "    im = cv2.imread(str(p))\n",
    "    if im is None:\n",
    "        raise FileNotFoundError(f'Image not found: {p}')\n",
    "    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "    im = cv2.resize(im, (size, size), interpolation=cv2.INTER_AREA)\n",
    "    im = im.astype('float32') / 255.0\n",
    "    t = torch.from_numpy(im.transpose(2,0,1)).unsqueeze(0)\n",
    "    if device is not None:\n",
    "        t = t.to(device)\n",
    "    return t\n",
    "\n",
    "def read_bgr_tensor(p, device=device, size=256):\n",
    "    im = cv2.imread(str(p))\n",
    "    if im is None:\n",
    "        raise FileNotFoundError(f'Image not found: {p}')\n",
    "    if size is not None:\n",
    "        im = cv2.resize(im, (size, size), interpolation=cv2.INTER_AREA)\n",
    "    return img_utils.bgr2tensor(im, normalize=False).to(device)\n",
    "\n",
    "def save_mask(mask, outpath):\n",
    "    cv2.imwrite(str(outpath), mask)\n",
    "\n",
    "print('Top-level imports and helpers are ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b92483c",
   "metadata": {},
   "source": [
    "# Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79ea7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seg_w = WEIGHTS_DIR / 'celeba_unet_256_1_2_segmentation_v2.pth'\n",
    "out_seg_dir = OUT_DIR / 'segmentation'\n",
    "out_seg_dir.mkdir(exist_ok=True)\n",
    "try:\n",
    "    model_seg = load_model(str(seg_w), 'segmentation', device=device)\n",
    "    model_seg.eval()\n",
    "    t = load_image_as_tensor(IMG_A, size=256, device=device)\n",
    "    with torch.no_grad():\n",
    "        pred = model_seg(t)\n",
    "\n",
    "    pred_np = pred.detach().cpu().numpy()\n",
    "    if pred_np.ndim == 4:\n",
    "        mask = pred_np.argmax(1)[0].astype('uint8') * 85 \n",
    "    else:\n",
    "\n",
    "        mask = (pred_np[0,0] * 255).astype('uint8')\n",
    "    out_path = out_seg_dir / 'a_seg_mask.png'\n",
    "    save_mask(mask, out_path)\n",
    "    print('Segmentation saved to', out_path)\n",
    "except Exception as e:\n",
    "    print('Segmentation test failed:', e)\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f85209e",
   "metadata": {},
   "source": [
    "# Landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9b9b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lms_w = WEIGHTS_DIR / 'hr18_wflw_landmarks.pth'\n",
    "out_lms_dir = OUT_DIR / 'landmarks'\n",
    "out_lms_dir.mkdir(exist_ok=True)\n",
    "try:\n",
    "    model_lms = load_model(str(lms_w), 'landmarks', device=device)\n",
    "    model_lms.eval()\n",
    "    t = load_image_as_tensor(IMG_A, size=256, device=device)\n",
    "    with torch.no_grad():\n",
    "        out = model_lms(t)\n",
    "    print('Landmarks model forward output shape:', getattr(out, 'shape', None))\n",
    "\n",
    "    try:\n",
    "        out_np = out.detach().cpu().numpy()\n",
    "        if out_np.ndim == 4:\n",
    "            hm = out_np[0,0]\n",
    "            hm = (255 * (hm - hm.min()) / (hm.max() - hm.min() + 1e-8)).astype('uint8')\n",
    "            cv2.imwrite(str(out_lms_dir / 'a_landmark_heatmap_ch0.png'), hm)\n",
    "            print('Saved example landmark heatmap to', out_lms_dir / 'a_landmark_heatmap_ch0.png')\n",
    "    except Exception as e:\n",
    "        print('Could not save landmark heatmap:', e)\n",
    "except Exception as e:\n",
    "    print('Landmarks test failed:', e)\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5748a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "reenact_w = ROOT / 'fsgan' / 'weights' / 'nfv_msrunet_256_1_2_reenactment_v2.1.pth'\n",
    "lms_w = ROOT / 'fsgan' / 'weights' / 'hr18_wflw_landmarks.pth'\n",
    "src_path = ROOT / 'input/a.jpg'\n",
    "tgt_path = ROOT / 'input/j.jpg'\n",
    "out_path = ROOT / 'outputs' / 'reenact_a_to_j.png'\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('Device:', device)\n",
    "print('Reenact model:', reenact_w)\n",
    "print('Landmarks model:', lms_w)\n",
    "print('Source:', src_path, 'Target:', tgt_path)\n",
    "\n",
    "# load reenactment generator\n",
    "try:\n",
    "    Gr, ckpt = load_model(str(reenact_w), 'reenactment', device=device, return_checkpoint=True)\n",
    "    Gr.eval()\n",
    "    try:\n",
    "        print('Checkpoint arch:', ckpt.get('arch', None))\n",
    "    except Exception:\n",
    "        pass\n",
    "except Exception as e:\n",
    "    print('Failed loading reenactment model:', e)\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "# load landmarks model\n",
    "try:\n",
    "    L = None\n",
    "    try:\n",
    "        L = load_model(str(lms_w), 'landmarks', device=device)\n",
    "        print('Loaded landmarks using utils.load_model()')\n",
    "    except AssertionError:\n",
    "        print('Landmarks file looks like raw state_dict, falling back to hrnet factory')\n",
    "    if L is None:\n",
    "        L = hrnet_wlfw().to(device)\n",
    "        state_dict = torch.load(str(lms_w), map_location=device)\n",
    "        L.load_state_dict(state_dict)\n",
    "    L.eval()\n",
    "except Exception as e:\n",
    "    print('Failed loading landmarks model:', e)\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "\n",
    "base_res = 256\n",
    "try:\n",
    "    src_t = read_bgr_tensor(src_path, device=device, size=base_res)\n",
    "    tgt_t = read_bgr_tensor(tgt_path, device=device, size=base_res)\n",
    "except Exception as e:\n",
    "    print('Image loading error:', e)\n",
    "    raise\n",
    "\n",
    "\n",
    "try:\n",
    "    n_local = getattr(Gr, 'n_local_enhancers', None)\n",
    "    if n_local is None and hasattr(Gr, 'module'):\n",
    "        n_local = getattr(Gr.module, 'n_local_enhancers', None)\n",
    "    if n_local is None:\n",
    "        n_local = 1\n",
    "    required_levels = int(n_local) + 1\n",
    "    print('Generator n_local_enhancers =', n_local, '; building pyramid with', required_levels, 'levels')\n",
    "except Exception as e:\n",
    "    print('Could not determine generator n_local_enhancers:', e)\n",
    "    required_levels = 3\n",
    "\n",
    "src_pyd = img_utils.create_pyramid(src_t, n=required_levels)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    tgt_for_lms = (tgt_t - torch.as_tensor([0.485,0.456,0.406], device=device).view(1,3,1,1)) / torch.as_tensor([0.229,0.224,0.225], device=device).view(1,3,1,1)\n",
    "    context = L(tgt_for_lms)\n",
    "    context = landmarks_utils.filter_landmarks(context)\n",
    "\n",
    "for i in range(len(src_pyd)):\n",
    "    src_pyd[i] = (src_pyd[i] - torch.as_tensor([0.5,0.5,0.5], device=device).view(1,3,1,1)) / torch.as_tensor([0.5,0.5,0.5], device=device).view(1,3,1,1)\n",
    "\n",
    "\n",
    "inp = []\n",
    "for p in range(len(src_pyd)-1, -1, -1):\n",
    "    c = torch.nn.functional.interpolate(context, size=src_pyd[p].shape[2:], mode='bicubic', align_corners=False)\n",
    "    inp.insert(0, torch.cat((src_pyd[p], c), dim=1))\n",
    "\n",
    "with torch.no_grad():\n",
    "    reenact_img = Gr(inp)\n",
    "\n",
    "out_tensor = reenact_img[-1] if isinstance(reenact_img, (list, tuple)) else reenact_img\n",
    "out_bgr = img_utils.tensor2bgr(out_tensor[0].cpu())\n",
    "cv2.imwrite(str(out_path), out_bgr)\n",
    "print('Saved reenactment to:', out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6877cb32",
   "metadata": {},
   "source": [
    "# preprocessing + segmentation + inpainting + blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6ec545",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "OUT_DIR = Path('outputs')\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "src = str(Path('input/a.jpg'))\n",
    "tgt = str(Path('input/juan2.jpg'))\n",
    "out_file = str(OUT_DIR / 'reenact_full_composited.png')\n",
    "\n",
    "\n",
    "result_bgr, intermediates, src_crop, tgt_crop = run_full_pipeline(src, tgt, out_path=out_file, reenact=True, use_detector=True, device=device,crop_scale=1.5, resolution=256)\n",
    "\n",
    "print('Saved full result to', out_file)\n",
    "\n",
    "try:\n",
    "    img = cv2.cvtColor(result_bgr, cv2.COLOR_BGR2RGB)\n",
    "    plt.figure(figsize=(6,6)); plt.axis('off'); plt.imshow(img); plt.show()\n",
    "except Exception as e:\n",
    "    print('Could not display image inline:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda2e847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Visualize intermediate steps: preprocessing, segmentation, inpainting, blending\n",
    "import math\n",
    "from fsgan.utils.img_utils import tensor2bgr\n",
    "\n",
    "def tensor_to_bgr_uint8(x):\n",
    "    import numpy as _np\n",
    "    import torch as _torch\n",
    "    if isinstance(x, _torch.Tensor):\n",
    "        t = x.detach().cpu()\n",
    "        if t.ndim == 4:\n",
    "            t = t[0]\n",
    "        # t expected in [-1,1] or [0,1]\n",
    "        if t.min() >= -1.1 and t.max() <= 1.1:\n",
    "            return tensor2bgr(t).astype('uint8')\n",
    "        else:\n",
    "            arr = (t.numpy().transpose(1,2,0) * 255.0).clip(0,255).astype('uint8')\n",
    "            return arr[:, :, ::-1]\n",
    "    else:\n",
    "        arr = x.copy()\n",
    "        if arr.dtype != _np.uint8:\n",
    "            arr = arr.astype('uint8')\n",
    "        return arr\n",
    "\n",
    "def seg_to_color(seg_arr):\n",
    "    import numpy as _np\n",
    "    h,w = seg_arr.shape\n",
    "    cmap = _np.array([[0,0,0],[0,255,0],[0,0,255],[255,0,0],[255,255,0]], dtype='uint8')\n",
    "    out = _np.zeros((h,w,3), dtype='uint8')\n",
    "    labels = _np.clip(seg_arr, 0, cmap.shape[0]-1)\n",
    "    for i in range(cmap.shape[0]):\n",
    "        out[labels==i] = cmap[i]\n",
    "    return out\n",
    "\n",
    "# Gather images to display\n",
    "imgs = []\n",
    "titles = []\n",
    "# Preprocessing crops\n",
    "try:\n",
    "    imgs.append(src_crop)\n",
    "    titles.append('Src crop')\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    imgs.append(tgt_crop)\n",
    "    titles.append('Tgt crop')\n",
    "except Exception:\n",
    "    pass\n",
    "# Reenact result\n",
    "try:\n",
    "    imgs.append(result_bgr)\n",
    "    titles.append('Final composited')\n",
    "except Exception:\n",
    "    pass\n",
    "# Reenactment-only (generator output) if available\n",
    "if 'reenact_tensor' in intermediates:\n",
    "    try:\n",
    "        imgs.append(tensor_to_bgr_uint8(intermediates['reenact_tensor']))\n",
    "        titles.append('Reenact')\n",
    "    except Exception:\n",
    "        pass\n",
    "# Segmentation maps\n",
    "if 'reenact_seg' in intermediates:\n",
    "    s = intermediates['reenact_seg']\n",
    "    try:\n",
    "        s_np = s.detach().cpu().numpy() if hasattr(s, 'detach') else s\n",
    "        if s_np.ndim == 4:\n",
    "            lab = s_np.argmax(1)[0].astype('int')\n",
    "        else:\n",
    "            lab = s_np.astype('int')\n",
    "        imgs.append(seg_to_color(lab)[:,:,::-1])\n",
    "        titles.append('Reenact seg')\n",
    "    except Exception:\n",
    "        pass\n",
    "if 'tgt_seg' in intermediates:\n",
    "    s = intermediates['tgt_seg']\n",
    "    try:\n",
    "        s_np = s.detach().cpu().numpy() if hasattr(s, 'detach') else s\n",
    "        if s_np.ndim == 4:\n",
    "            lab = s_np.argmax(1)[0].astype('int')\n",
    "        else:\n",
    "            lab = s_np.astype('int')\n",
    "        imgs.append(seg_to_color(lab)[:,:,::-1])\n",
    "        titles.append('Tgt seg')\n",
    "    except Exception:\n",
    "        pass\n",
    "# Source segmentation (for original source crop) if available\n",
    "if 'src_seg' in intermediates:\n",
    "    s = intermediates['src_seg']\n",
    "    try:\n",
    "        s_np = s.detach().cpu().numpy() if hasattr(s, 'detach') else s\n",
    "        if s_np.ndim == 4:\n",
    "            lab = s_np.argmax(1)[0].astype('int')\n",
    "        else:\n",
    "            lab = s_np.astype('int')\n",
    "        imgs.append(seg_to_color(lab)[:,:,::-1])\n",
    "        titles.append('Src seg')\n",
    "    except Exception:\n",
    "        pass\n",
    "# Masks: soft and eroded\n",
    "if 'soft_tgt_mask' in intermediates:\n",
    "    m = intermediates['soft_tgt_mask']\n",
    "    try:\n",
    "        m_np = m.detach().cpu().numpy()[0,0]\n",
    "        imgs.append((m_np*255).astype('uint8'))\n",
    "        titles.append('Soft mask')\n",
    "    except Exception:\n",
    "        pass\n",
    "if 'eroded_tgt_mask' in intermediates:\n",
    "    m = intermediates['eroded_tgt_mask']\n",
    "    try:\n",
    "        m_np = m.detach().cpu().numpy()[0,0]\n",
    "        imgs.append((m_np*255).astype('uint8'))\n",
    "        titles.append('Eroded mask')\n",
    "    except Exception:\n",
    "        pass\n",
    "# Completion / transfer / blend\n",
    "for key, label in [('completion','Completion'), ('transfer','Transfer'), ('blend_out','Blend out')]:\n",
    "    if key in intermediates:\n",
    "        try:\n",
    "            imgs.append(tensor_to_bgr_uint8(intermediates[key]))\n",
    "            titles.append(label)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# Plot grid\n",
    "n = len(imgs)\n",
    "if n == 0:\n",
    "    print('No intermediates available to display')\n",
    "else:\n",
    "    cols = min(4, n)\n",
    "    rows = math.ceil(n / cols)\n",
    "    plt.figure(figsize=(4*cols, 3*rows))\n",
    "    for i, im in enumerate(imgs):\n",
    "        plt.subplot(rows, cols, i+1)\n",
    "        plt.axis('off')\n",
    "        if im.ndim == 2:\n",
    "            plt.imshow(im, cmap='gray')\n",
    "        else:\n",
    "            plt.imshow(im[:,:,::-1])\n",
    "        plt.title(titles[i])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60080f9d",
   "metadata": {},
   "source": [
    "## Entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf8b982",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import utils\n",
    "import dataloader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "print(\"Cuda compatible : \", torch.cuda.is_available())\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abb72ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 256\n",
    "NB_IMAGES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4f4b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset, nb_classes = dataloader.make_dataset(\"../data/Face-Swap-M2-Dataset/dataset/smaller\", NB_IMAGES, IMAGE_SIZE, 0.8, crop_faces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1a2130",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EMBEDDING_SIZE = 128\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791c342b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351ac577",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"train samples: {len(train_dataset)}, test samples: {len(test_dataset)}, nb_classes: {nb_classes}\")\n",
    "\n",
    "class_counts = np.bincount([label for _, label in train_dataset])\n",
    "for cls, count in enumerate(class_counts):\n",
    "    print(f\"train Class {cls}: {count} samples\")\n",
    "\n",
    "class_counts = np.bincount([label for _, label in test_dataset])\n",
    "for cls, count in enumerate(class_counts):\n",
    "    print(f\"Test Class {cls}: {count} samples\")\n",
    "\n",
    "images, labels = next(iter(train_loader))\n",
    "images = images.cpu()\n",
    "labels = labels.cpu().numpy()\n",
    "\n",
    "n = min(8, images.size(0))\n",
    "imgs = images[:n]\n",
    "labs = labels[:n]\n",
    "\n",
    "fig, axs = plt.subplots(2, 4, figsize=(12, 6))\n",
    "axs = axs.flatten()\n",
    "for i, ax in enumerate(axs):\n",
    "    if i < n:\n",
    "        img = imgs[i].permute(1, 2, 0).numpy()\n",
    "        mi, ma = img.min(), img.max()\n",
    "        img_disp = (img - mi) / (ma - mi) if ma - mi > 1e-6 else img\n",
    "        ax.imshow(img_disp)\n",
    "        ax.set_title(f\"label: {labs[i]}\")\n",
    "    ax.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a349233a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Directory to save results\n",
    "random_swap_dir = OUT_DIR / 'random_swaps'\n",
    "random_swap_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Function to perform face swapping on a pair of images\n",
    "def swap_and_save(src_img, tgt_img, src_label, tgt_label, idx):\n",
    "    src_path = random_swap_dir / f'src_{idx}_label_{src_label}.png'\n",
    "    tgt_path = random_swap_dir / f'tgt_{idx}_label_{tgt_label}.png'\n",
    "    out_path = random_swap_dir / f'swap_{idx}_src_{src_label}_to_tgt_{tgt_label}.png'\n",
    "\n",
    "    # Convert tensors to NumPy arrays and scale to [0, 255]\n",
    "    src_img_np = ((src_img.permute(1, 2, 0).numpy() + 1) * 127.5).clip(0, 255).astype('uint8')\n",
    "    tgt_img_np = ((tgt_img.permute(1, 2, 0).numpy() + 1) * 127.5).clip(0, 255).astype('uint8')\n",
    "\n",
    "    # Convert RGB to BGR for OpenCV\n",
    "    src_img_bgr = cv2.cvtColor(src_img_np, cv2.COLOR_RGB2BGR)\n",
    "    tgt_img_bgr = cv2.cvtColor(tgt_img_np, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Save source and target images\n",
    "    cv2.imwrite(str(src_path), src_img_bgr)\n",
    "    cv2.imwrite(str(tgt_path), tgt_img_bgr)\n",
    "\n",
    "    # Run the face swap pipeline\n",
    "    result_bgr, _, _, _ = run_full_pipeline(\n",
    "        str(src_path), str(tgt_path), out_path=str(out_path),\n",
    "        reenact=True, use_detector=True, device=device, crop_scale=1.2, resolution=256\n",
    "    )\n",
    "\n",
    "    print(f'Saved face swap result to {out_path}')\n",
    "\n",
    "# Select random pairs of images from the dataloader\n",
    "num_pairs = 60  # Number of random pairs to process\n",
    "for idx in range(num_pairs):\n",
    "    src_batch, src_labels = next(iter(train_loader))\n",
    "    tgt_batch, tgt_labels = next(iter(test_loader))\n",
    "\n",
    "    # Randomly select a source and target image from the batches\n",
    "    src_idx = random.randint(0, src_batch.size(0) - 1)\n",
    "    tgt_idx = random.randint(0, tgt_batch.size(0) - 1)\n",
    "\n",
    "    src_img = src_batch[src_idx].cpu()\n",
    "    tgt_img = tgt_batch[tgt_idx].cpu()\n",
    "    src_label = src_labels[src_idx].item()\n",
    "    tgt_label = tgt_labels[tgt_idx].item()\n",
    "\n",
    "    # Perform face swap and save the result\n",
    "    swap_and_save(src_img, tgt_img, src_label, tgt_label, idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
