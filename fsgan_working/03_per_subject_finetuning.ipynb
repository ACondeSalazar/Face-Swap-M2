{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d34495c8",
   "metadata": {},
   "source": [
    "# Per-Subject Reenactment Finetuning\n",
    "\n",
    "This notebook performs intensive finetuning on a **single identity** for maximum face swap quality.\n",
    "\n",
    "**Purpose:** Overfit the model to one specific person for perfect quality on that individual.\n",
    "\n",
    "**When to use:**\n",
    "- You want the best possible quality for a specific person\n",
    "- You have at least 2-10 images of that person\n",
    "- You're willing to sacrifice generalization for quality\n",
    "\n",
    "**Contents:**\n",
    "1. Load dataset and select target identity\n",
    "2. Create single-identity dataset with augmentation\n",
    "3. Setup per-subject training\n",
    "4. Train with aggressive overfitting\n",
    "5. Evaluate results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21b2cc3",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5dff46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from fsgan.utils.utils import load_model\n",
    "from fsgan.utils.img_utils import bgr2tensor, create_pyramid, tensor2bgr\n",
    "from fsgan.utils.landmarks_utils import filter_landmarks\n",
    "from fsgan.criterions.vgg_loss import VGGLoss\n",
    "from fsgan.utils.obj_factory import obj_factory\n",
    "\n",
    "import dataloader\n",
    "\n",
    "ROOT = Path('.')\n",
    "WEIGHTS_DIR = ROOT / 'fsgan' / 'weights'\n",
    "OUT_DIR = ROOT / 'outputs'\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5465da1",
   "metadata": {},
   "source": [
    "## 2. Load Dataset\n",
    "\n",
    "Load your dataset first to see available identities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278142c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "IMAGE_SIZE = 256\n",
    "NB_IMAGES = 10\n",
    "DATASET_PATH = \"../data/Face-Swap-M2-Dataset/dataset/smaller\"\n",
    "\n",
    "# Load dataset\n",
    "train_dataset, test_dataset, nb_classes = dataloader.make_dataset(\n",
    "    DATASET_PATH, \n",
    "    NB_IMAGES, \n",
    "    IMAGE_SIZE, \n",
    "    0.8, \n",
    "    crop_faces=False\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Number of identities: {nb_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f685ae3",
   "metadata": {},
   "source": [
    "## 3. Analyze Available Identities\n",
    "\n",
    "See which identities are available and how many images each has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99154763",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset_identities(dataset):\n",
    "    \"\"\"Analyze dataset to find all identities and their image counts.\"\"\"\n",
    "    label_to_indices = {}\n",
    "    \n",
    "    for idx in range(len(dataset)):\n",
    "        _, label = dataset[idx]\n",
    "        if isinstance(label, torch.Tensor):\n",
    "            label = label.item()\n",
    "        if label not in label_to_indices:\n",
    "            label_to_indices[label] = []\n",
    "        label_to_indices[label].append(idx)\n",
    "    \n",
    "    return label_to_indices\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ANALYZING DATASET FOR PER-SUBJECT FINETUNING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train_identities = analyze_dataset_identities(train_dataset)\n",
    "print(f\"\\nTRAIN SET: {len(train_identities)} identities\")\n",
    "print(\"-\"*40)\n",
    "for label in sorted(train_identities.keys()):\n",
    "    count = len(train_identities[label])\n",
    "    status = \"✓\" if count >= 2 else \"✗ (need more)\"\n",
    "    print(f\"  Label {label}: {count} images {status}\")\n",
    "\n",
    "# Find best candidates\n",
    "sorted_by_count = sorted(train_identities.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "print(f\"\\n→ Recommended: Label {sorted_by_count[0][0]} ({len(sorted_by_count[0][1])} images)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e49505c",
   "metadata": {},
   "source": [
    "## 4. Configuration\n",
    "\n",
    "**Set the identity to finetune on here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91319698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SELECT WHICH IDENTITY TO FINETUNE ON\n",
    "# ============================================================\n",
    "SUBJECT_LABEL = 0  # <-- CHANGE THIS to the identity you want\n",
    "\n",
    "# Training hyperparameters - aggressive for overfitting\n",
    "SUBJECT_EPOCHS = 500           # Many epochs to overfit\n",
    "SUBJECT_LR = 1e-4              # Learning rate\n",
    "SUBJECT_BATCH_SIZE = 2         # Small batch\n",
    "SUBJECT_MIN_IMAGES = 2         # Minimum images needed\n",
    "\n",
    "# Loss weights - emphasize pixel accuracy\n",
    "SUBJECT_WEIGHT_PIXEL = 10.0    # Higher pixel weight\n",
    "SUBJECT_WEIGHT_PERC = 1.0      # Perceptual weight\n",
    "SUBJECT_SAVE_EVERY = 50        # Save checkpoints frequently\n",
    "\n",
    "# Data augmentation\n",
    "USE_AUGMENTATION = True\n",
    "\n",
    "print(\"Per-subject finetuning configuration:\")\n",
    "print(f\"  Target identity: {SUBJECT_LABEL}\")\n",
    "print(f\"  Epochs: {SUBJECT_EPOCHS}\")\n",
    "print(f\"  Learning rate: {SUBJECT_LR}\")\n",
    "print(f\"  Batch size: {SUBJECT_BATCH_SIZE}\")\n",
    "print(f\"  Augmentation: {USE_AUGMENTATION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752e9427",
   "metadata": {},
   "source": [
    "## 5. Create Single-Identity Dataset\n",
    "\n",
    "Extract one identity and apply heavy augmentation for few-shot learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f942e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleIdentityDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for per-subject finetuning using a single identity.\n",
    "    Creates pairs and supports heavy data augmentation.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_dataset, target_label, augment=True, num_augmentations=5):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.target_label = target_label\n",
    "        self.augment = augment\n",
    "        self.num_augmentations = num_augmentations if augment else 1\n",
    "        \n",
    "        # Find all indices for this identity\n",
    "        self.original_indices = []\n",
    "        for idx in range(len(base_dataset)):\n",
    "            _, label = base_dataset[idx]\n",
    "            if isinstance(label, torch.Tensor):\n",
    "                label = label.item()\n",
    "            if label == target_label:\n",
    "                self.original_indices.append(idx)\n",
    "        \n",
    "        if len(self.original_indices) == 0:\n",
    "            raise ValueError(f\"No images found for label {target_label}\")\n",
    "        \n",
    "        # Augmentation transforms\n",
    "        self.augment_transform = T.Compose([\n",
    "            T.RandomHorizontalFlip(p=0.5),\n",
    "            T.RandomAffine(\n",
    "                degrees=15,\n",
    "                translate=(0.1, 0.1),\n",
    "                scale=(0.9, 1.1),\n",
    "            ),\n",
    "            T.ColorJitter(\n",
    "                brightness=0.2,\n",
    "                contrast=0.2,\n",
    "                saturation=0.1,\n",
    "                hue=0.05\n",
    "            ),\n",
    "        ])\n",
    "        \n",
    "        print(f\"SingleIdentityDataset for label {target_label}:\")\n",
    "        print(f\"  Original images: {len(self.original_indices)}\")\n",
    "        print(f\"  With augmentation: {len(self)} pairs\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.original_indices) * self.num_augmentations\n",
    "    \n",
    "    def _get_image(self, idx):\n",
    "        img, _ = self.base_dataset[idx]\n",
    "        return img\n",
    "    \n",
    "    def _apply_augmentation(self, img_tensor):\n",
    "        if not self.augment:\n",
    "            return img_tensor\n",
    "        \n",
    "        # Convert to [0, 1] for transforms\n",
    "        img_01 = (img_tensor + 1) / 2\n",
    "        img_01 = torch.clamp(img_01, 0, 1)\n",
    "        \n",
    "        # Apply augmentation\n",
    "        img_aug = self.augment_transform(img_01)\n",
    "        \n",
    "        # Back to [-1, 1]\n",
    "        return img_aug * 2 - 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        base_idx = idx % len(self.original_indices)\n",
    "        real_idx = self.original_indices[base_idx]\n",
    "        \n",
    "        # Source: original or augmented\n",
    "        src_img = self._get_image(real_idx)\n",
    "        if self.augment and idx >= len(self.original_indices):\n",
    "            src_img = self._apply_augmentation(src_img)\n",
    "        \n",
    "        # Target: random OTHER image from same person\n",
    "        tgt_base_idx = base_idx\n",
    "        while tgt_base_idx == base_idx and len(self.original_indices) > 1:\n",
    "            tgt_base_idx = np.random.randint(len(self.original_indices))\n",
    "        tgt_real_idx = self.original_indices[tgt_base_idx]\n",
    "        tgt_img = self._get_image(tgt_real_idx)\n",
    "        \n",
    "        # Augment target too\n",
    "        if self.augment:\n",
    "            tgt_img = self._apply_augmentation(tgt_img)\n",
    "        \n",
    "        return src_img, tgt_img\n",
    "\n",
    "# Create dataset\n",
    "if SUBJECT_LABEL in train_identities:\n",
    "    num_images = len(train_identities[SUBJECT_LABEL])\n",
    "    print(f\"\\nCreating per-subject dataset for label {SUBJECT_LABEL}...\")\n",
    "    print(f\"  Found {num_images} images\")\n",
    "    \n",
    "    if num_images < SUBJECT_MIN_IMAGES:\n",
    "        print(f\"  ⚠ Warning: Only {num_images} images (minimum: {SUBJECT_MIN_IMAGES})\")\n",
    "    \n",
    "    # Adjust batch size and augmentation\n",
    "    effective_batch_size = min(SUBJECT_BATCH_SIZE, num_images)\n",
    "    \n",
    "    if num_images <= 5:\n",
    "        num_aug = 20  # Heavy augmentation\n",
    "    elif num_images <= 10:\n",
    "        num_aug = 10\n",
    "    else:\n",
    "        num_aug = 5\n",
    "    \n",
    "    subject_dataset = SingleIdentityDataset(\n",
    "        train_dataset,\n",
    "        target_label=SUBJECT_LABEL,\n",
    "        augment=USE_AUGMENTATION,\n",
    "        num_augmentations=num_aug\n",
    "    )\n",
    "    \n",
    "    subject_loader = DataLoader(\n",
    "        subject_dataset,\n",
    "        batch_size=effective_batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✓ Dataset ready:\")\n",
    "    print(f\"  Augmented pairs: {len(subject_dataset)}\")\n",
    "    print(f\"  Batch size: {effective_batch_size}\")\n",
    "    print(f\"  Batches per epoch: {len(subject_loader)}\")\n",
    "else:\n",
    "    print(f\"\\n✗ Label {SUBJECT_LABEL} not found!\")\n",
    "    print(f\"  Available: {list(train_identities.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035f6492",
   "metadata": {},
   "source": [
    "## 6. Load Models and Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78e5f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading pretrained models...\")\n",
    "\n",
    "# Load reenactment generator\n",
    "reenact_w = WEIGHTS_DIR / 'nfv_msrunet_256_1_2_reenactment_v2.1.pth'\n",
    "Gr_subject, ckpt = load_model(str(reenact_w), 'reenactment', device=device, return_checkpoint=True)\n",
    "Gr_subject.train()\n",
    "print(f\"✓ Reenactment generator\")\n",
    "\n",
    "# Load landmarks model (frozen)\n",
    "lms_w = WEIGHTS_DIR / 'hr18_wflw_landmarks.pth'\n",
    "L_subject, _ = load_model(str(lms_w), 'landmarks', device=device, return_checkpoint=True)\n",
    "L_subject.eval()\n",
    "for p in L_subject.parameters():\n",
    "    p.requires_grad = False\n",
    "print(\"✓ Landmarks model (frozen)\")\n",
    "\n",
    "# Get pyramid levels\n",
    "n_levels_subject = getattr(Gr_subject, 'n_local_enhancers', 1) + 1\n",
    "print(f\"Pyramid levels: {n_levels_subject}\")\n",
    "\n",
    "# Optimizer\n",
    "optimizer_subject = optim.Adam(\n",
    "    Gr_subject.parameters(), \n",
    "    lr=SUBJECT_LR, \n",
    "    betas=(0.5, 0.999)\n",
    ")\n",
    "print(f\"✓ Optimizer: Adam, LR={SUBJECT_LR}\")\n",
    "\n",
    "# Loss functions\n",
    "criterion_pixel = nn.L1Loss().to(device)\n",
    "try:\n",
    "    vgg_id_path = str(WEIGHTS_DIR / 'vggface2_vgg19_256_1_2_id.pth')\n",
    "    criterion_id = VGGLoss(vgg_id_path).to(device)\n",
    "    criterion_id.eval()\n",
    "    print(\"✓ VGG identity loss\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Could not load VGG: {e}\")\n",
    "    criterion_id = None\n",
    "\n",
    "try:\n",
    "    vgg_attr_path = str(WEIGHTS_DIR / 'celeba_vgg19_256_2_0_28_attr.pth')\n",
    "    criterion_attr = VGGLoss(vgg_attr_path).to(device)\n",
    "    criterion_attr.eval()\n",
    "    print(\"✓ VGG attribute loss\")\n",
    "except Exception:\n",
    "    criterion_attr = None\n",
    "\n",
    "# ImageNet normalization\n",
    "imagenet_mean = torch.tensor([0.485, 0.456, 0.406], device=device).view(1, 3, 1, 1)\n",
    "imagenet_std = torch.tensor([0.229, 0.224, 0.225], device=device).view(1, 3, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a762cdd",
   "metadata": {},
   "source": [
    "## 7. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0406979f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_reenactment_input(src_batch, tgt_batch, L_model, n_pyramid_levels, device):\n",
    "    # Ensure [-1, 1] range\n",
    "    if src_batch.min() >= 0:\n",
    "        src_normalized = src_batch * 2 - 1\n",
    "    else:\n",
    "        src_normalized = src_batch\n",
    "    \n",
    "    if tgt_batch.min() >= 0:\n",
    "        tgt_normalized = tgt_batch * 2 - 1\n",
    "    else:\n",
    "        tgt_normalized = tgt_batch\n",
    "    \n",
    "    # Prepare target for landmarks\n",
    "    tgt_01 = (tgt_normalized + 1) / 2\n",
    "    tgt_for_lms = (tgt_01 - imagenet_mean) / imagenet_std\n",
    "    \n",
    "    # Get landmarks\n",
    "    with torch.no_grad():\n",
    "        tgt_landmarks = L_model(tgt_for_lms)\n",
    "        tgt_landmarks = filter_landmarks(tgt_landmarks)\n",
    "    \n",
    "    # Build pyramid\n",
    "    src_pyd = create_pyramid(src_normalized, n_pyramid_levels)\n",
    "    \n",
    "    # Build input list\n",
    "    input_list = []\n",
    "    for p in range(len(src_pyd)):\n",
    "        pyd_h, pyd_w = src_pyd[p].shape[2:]\n",
    "        context = F.interpolate(tgt_landmarks, size=(pyd_h, pyd_w), mode='bilinear', align_corners=False)\n",
    "        context = filter_landmarks(context)\n",
    "        inp = torch.cat((src_pyd[p], context), dim=1)\n",
    "        input_list.append(inp)\n",
    "    \n",
    "    return input_list, tgt_normalized\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f7cbcb",
   "metadata": {},
   "source": [
    "## 8. Per-Subject Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcac620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_subject_finetune(model, dataloader, L_model, optimizer, \n",
    "                         epochs=300, save_dir='models/per_subject',\n",
    "                         weight_pixel=10.0, weight_perc=1.0):\n",
    "    \"\"\"\n",
    "    Per-subject finetuning for maximum quality.\n",
    "    Intentionally overfits to a single person.\n",
    "    \"\"\"\n",
    "    save_path = Path(save_dir)\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    model.train()\n",
    "    L_model.eval()\n",
    "    \n",
    "    history = {\n",
    "        'loss': [], 'loss_pixel': [], 'loss_id': [], \n",
    "        'loss_attr': [], 'loss_stepwise': []\n",
    "    }\n",
    "    \n",
    "    n_levels = getattr(model, 'n_local_enhancers', 1) + 1\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"PER-SUBJECT FINETUNING (Aggressive Overfitting)\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Epochs: {epochs}\")\n",
    "    print(f\"Batches per epoch: {len(dataloader)}\")\n",
    "    print(f\"Weight pixel: {weight_pixel}, Weight perceptual: {weight_perc}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_losses = {'total': 0, 'pixel': 0, 'id': 0, 'attr': 0, 'stepwise': 0}\n",
    "        n_batches = 0\n",
    "        \n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        for src_img, tgt_img in pbar:\n",
    "            src_img = src_img.to(device)\n",
    "            tgt_img = tgt_img.to(device)\n",
    "            \n",
    "            # Prepare inputs\n",
    "            input_list, tgt_normalized = prepare_reenactment_input(\n",
    "                src_img, tgt_img, L_model, n_levels, device\n",
    "            )\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(input_list)\n",
    "            if isinstance(output, (list, tuple)):\n",
    "                pred = output[-1]\n",
    "                # Stepwise consistency\n",
    "                loss_stepwise = torch.tensor(0.0, device=device)\n",
    "                for i, out_i in enumerate(output[:-1]):\n",
    "                    tgt_down = F.interpolate(tgt_normalized, size=out_i.shape[2:], \n",
    "                                            mode='bilinear', align_corners=False)\n",
    "                    loss_stepwise += criterion_pixel(out_i, tgt_down)\n",
    "                loss_stepwise /= len(output)\n",
    "            else:\n",
    "                pred = output\n",
    "                loss_stepwise = torch.tensor(0.0, device=device)\n",
    "            \n",
    "            # Losses\n",
    "            loss_pixel = criterion_pixel(pred, tgt_normalized)\n",
    "            \n",
    "            if criterion_id is not None:\n",
    "                loss_id = criterion_id(pred, tgt_normalized)\n",
    "            else:\n",
    "                loss_id = torch.tensor(0.0, device=device)\n",
    "            \n",
    "            if criterion_attr is not None:\n",
    "                loss_attr = criterion_attr(pred, tgt_normalized)\n",
    "            else:\n",
    "                loss_attr = torch.tensor(0.0, device=device)\n",
    "            \n",
    "            # Combined loss\n",
    "            loss_total = (\n",
    "                weight_pixel * loss_pixel + \n",
    "                weight_perc * (loss_id + loss_attr) +\n",
    "                weight_pixel * 0.5 * loss_stepwise\n",
    "            )\n",
    "            \n",
    "            # Backward\n",
    "            optimizer.zero_grad()\n",
    "            loss_total.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track\n",
    "            epoch_losses['total'] += loss_total.item()\n",
    "            epoch_losses['pixel'] += loss_pixel.item()\n",
    "            epoch_losses['id'] += loss_id.item() if criterion_id else 0\n",
    "            epoch_losses['attr'] += loss_attr.item() if criterion_attr else 0\n",
    "            epoch_losses['stepwise'] += loss_stepwise.item()\n",
    "            n_batches += 1\n",
    "            \n",
    "            pbar.set_postfix({'loss': f\"{loss_total.item():.4f}\"})\n",
    "        \n",
    "        # Epoch summary\n",
    "        avg_loss = epoch_losses['total'] / n_batches\n",
    "        avg_pixel = epoch_losses['pixel'] / n_batches\n",
    "        avg_id = epoch_losses['id'] / n_batches\n",
    "        avg_attr = epoch_losses['attr'] / n_batches\n",
    "        avg_stepwise = epoch_losses['stepwise'] / n_batches\n",
    "        \n",
    "        history['loss'].append(avg_loss)\n",
    "        history['loss_pixel'].append(avg_pixel)\n",
    "        history['loss_id'].append(avg_id)\n",
    "        history['loss_attr'].append(avg_attr)\n",
    "        history['loss_stepwise'].append(avg_stepwise)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Loss={avg_loss:.4f}, Pixel={avg_pixel:.4f}\")\n",
    "        \n",
    "        # Save best\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            best_path = save_path / 'per_subject_best.pth'\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'arch': ckpt.get('arch', 'unknown'),\n",
    "                'history': history,\n",
    "                'best_loss': best_loss,\n",
    "            }, best_path)\n",
    "            print(f\"  → Best model saved: {best_path}\")\n",
    "        \n",
    "        # Regular checkpoints\n",
    "        if (epoch + 1) % SUBJECT_SAVE_EVERY == 0 or epoch == epochs - 1:\n",
    "            ckpt_path = save_path / f'per_subject_epoch{epoch+1}.pth'\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'arch': ckpt.get('arch', 'unknown'),\n",
    "                'history': history,\n",
    "            }, ckpt_path)\n",
    "            print(f\"  → Checkpoint: {ckpt_path}\")\n",
    "    \n",
    "    print(f\"\\nBest loss: {best_loss:.4f}\")\n",
    "    return history\n",
    "\n",
    "print(\"✓ Training function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9779af92",
   "metadata": {},
   "source": [
    "## 9. Run Per-Subject Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91244f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'subject_loader' in dir() and subject_loader is not None:\n",
    "    print(f\"Starting per-subject finetuning with {len(subject_dataset)} pairs...\")\n",
    "    \n",
    "    subject_history = per_subject_finetune(\n",
    "        model=Gr_subject,\n",
    "        dataloader=subject_loader,\n",
    "        L_model=L_subject,\n",
    "        optimizer=optimizer_subject,\n",
    "        epochs=SUBJECT_EPOCHS,\n",
    "        save_dir=str(OUT_DIR / 'per_subject_models'),\n",
    "        weight_pixel=SUBJECT_WEIGHT_PIXEL,\n",
    "        weight_perc=SUBJECT_WEIGHT_PERC\n",
    "    )\n",
    "else:\n",
    "    print(\"Cannot run training: subject_loader not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e539d8",
   "metadata": {},
   "source": [
    "## 10. Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953c4a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "if 'subject_history' in dir() and subject_history:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    axes[0, 0].plot(subject_history['loss'], 'b-', linewidth=2)\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Total Loss')\n",
    "    axes[0, 0].set_title('Total Loss')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[0, 1].plot(subject_history['loss_pixel'], 'g-', linewidth=2)\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Pixel L1 Loss')\n",
    "    axes[0, 1].set_title('Pixel Loss')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1, 0].plot(subject_history['loss_id'], 'r-', linewidth=2, label='Identity')\n",
    "    axes[1, 0].plot(subject_history['loss_attr'], 'm-', linewidth=2, label='Attribute')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Perceptual Loss')\n",
    "    axes[1, 0].set_title('Perceptual Losses')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1, 1].plot(subject_history['loss_stepwise'], 'c-', linewidth=2)\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Stepwise Loss')\n",
    "    axes[1, 1].set_title('Stepwise Consistency')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Per-Subject Training Progress', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(str(OUT_DIR / 'per_subject_training_curves.png'), dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nFinal losses:\")\n",
    "    print(f\"  Total: {subject_history['loss'][-1]:.4f}\")\n",
    "    print(f\"  Pixel: {subject_history['loss_pixel'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e97c1c",
   "metadata": {},
   "source": [
    "## 11. Test Per-Subject Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e3c6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_per_subject_model(model, dataloader, L_model, num_samples=6):\n",
    "    model.eval()\n",
    "    n_levels = getattr(model, 'n_local_enhancers', 1) + 1\n",
    "    \n",
    "    all_src, all_tgt, all_pred = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for src_batch, tgt_batch in dataloader:\n",
    "            if len(all_src) >= num_samples:\n",
    "                break\n",
    "                \n",
    "            src_batch = src_batch.to(device)\n",
    "            tgt_batch = tgt_batch.to(device)\n",
    "            \n",
    "            input_list, tgt_normalized = prepare_reenactment_input(\n",
    "                src_batch, tgt_batch, L_model, n_levels, device\n",
    "            )\n",
    "            \n",
    "            output = model(input_list)\n",
    "            pred = output[-1] if isinstance(output, (list, tuple)) else output\n",
    "            \n",
    "            for i in range(min(src_batch.shape[0], num_samples - len(all_src))):\n",
    "                all_src.append(src_batch[i].cpu())\n",
    "                all_tgt.append(tgt_batch[i].cpu())\n",
    "                all_pred.append(pred[i].cpu())\n",
    "    \n",
    "    def to_np(t):\n",
    "        return ((t.numpy().transpose(1, 2, 0) + 1) / 2 * 255).clip(0, 255).astype('uint8')\n",
    "    \n",
    "    src_np = [to_np(t) for t in all_src]\n",
    "    tgt_np = [to_np(t) for t in all_tgt]\n",
    "    pred_np = [to_np(t) for t in all_pred]\n",
    "    \n",
    "    # Visualization\n",
    "    n = len(src_np)\n",
    "    fig, axes = plt.subplots(n, 3, figsize=(12, 4*n))\n",
    "    if n == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    total_psnr = 0\n",
    "    for i in range(n):\n",
    "        axes[i, 0].imshow(src_np[i])\n",
    "        axes[i, 0].set_title('Source', fontsize=11)\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        axes[i, 1].imshow(tgt_np[i])\n",
    "        axes[i, 1].set_title('Target (GT)', fontsize=11)\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        mse = np.mean((tgt_np[i].astype(float) - pred_np[i].astype(float)) ** 2)\n",
    "        psnr = 10 * np.log10(255**2 / (mse + 1e-10))\n",
    "        total_psnr += psnr\n",
    "        \n",
    "        axes[i, 2].imshow(pred_np[i])\n",
    "        axes[i, 2].set_title(f'Prediction (PSNR: {psnr:.1f} dB)', fontsize=11)\n",
    "        axes[i, 2].axis('off')\n",
    "    \n",
    "    plt.suptitle('Per-Subject Model Results', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(str(OUT_DIR / 'per_subject_test_results.png'), dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    avg_psnr = total_psnr / n\n",
    "    print(f\"\\nAverage PSNR: {avg_psnr:.2f} dB\")\n",
    "    \n",
    "    model.train()\n",
    "    return avg_psnr\n",
    "\n",
    "# Run test\n",
    "if 'Gr_subject' in dir() and 'subject_loader' in dir():\n",
    "    test_psnr = test_per_subject_model(\n",
    "        Gr_subject,\n",
    "        subject_loader,\n",
    "        L_subject,\n",
    "        num_samples=6\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80ce621",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook:\n",
    "- ✓ Extracted a single identity from your dataset\n",
    "- ✓ Applied heavy data augmentation for few-shot learning\n",
    "- ✓ Trained with aggressive overfitting (intentional!)\n",
    "- ✓ Achieved maximum quality for that specific person\n",
    "\n",
    "**The best model is saved as `per_subject_best.pth`**\n",
    "\n",
    "**Use this model when:**\n",
    "- You need the highest quality for a specific person\n",
    "- Source and target are both this person\n",
    "- You have similar poses to training data"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
