{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bfb8755",
   "metadata": {},
   "source": [
    "# Face Swap Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e891f0bf",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583104ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac9cce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IMAGE_SIZE = 256\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 200  \n",
    "LR = 1e-4\n",
    "\n",
    "TARGET_CLASS = \"Andrew\"\n",
    "\n",
    "CUSTOM_DATASET_PATH = \"../data/Face-Swap-M2-Dataset/dataset/smaller\"\n",
    "CELEBA_PATH = \"C:/Users/Arthur/.cache/kagglehub/datasets/jessicali9530/celeba-dataset/versions/2/img_align_celeba/img_align_celeba\"\n",
    "\n",
    "\n",
    "NUM_CELEBA_IMAGES = 2000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea18b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_target_images(dataset_path, target_class, image_size):\n",
    "    \"\"\"Load all images from the target class folder.\"\"\"\n",
    "    target_dir = os.path.join(dataset_path, target_class)\n",
    "    images = []\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # [-1, 1]\n",
    "    ])\n",
    "    \n",
    "    for fname in os.listdir(target_dir):\n",
    "        fpath = os.path.join(target_dir, fname)\n",
    "        try:\n",
    "            img = Image.open(fpath).convert('RGB')\n",
    "            img_tensor = transform(img)\n",
    "            images.append(img_tensor)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {fpath}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Loaded {len(images)} images for target class '{target_class}'\")\n",
    "    return torch.stack(images)\n",
    "\n",
    "target_images = load_target_images(CUSTOM_DATASET_PATH, TARGET_CLASS, IMAGE_SIZE)\n",
    "target_images = target_images.to(device)\n",
    "print(f\"Target images shape: {target_images.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e604f775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CelebA Dataset for content images\n",
    "class CelebADataset(Dataset):\n",
    "    def __init__(self, celeba_path, num_images, image_size):\n",
    "        self.celeba_path = celeba_path\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        # Get all image files and sample\n",
    "        all_files = sorted([f for f in os.listdir(celeba_path) if f.endswith('.jpg')])\n",
    "        self.files = random.sample(all_files, min(num_images, len(all_files)))\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "        \n",
    "        print(f\"CelebA dataset: {len(self.files)} images\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.celeba_path, self.files[idx])\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        return self.transform(img)\n",
    "\n",
    "celeba_dataset = CelebADataset(CELEBA_PATH, NUM_CELEBA_IMAGES, IMAGE_SIZE)\n",
    "celeba_loader = DataLoader(celeba_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4917e24",
   "metadata": {},
   "source": [
    "# Model Architecture\n",
    "\n",
    "The Transformation Network transforms any input face to look like the target identity.\n",
    "Based on the multiscale architecture from Korshunova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dce6029",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, 3, padding=1),\n",
    "            nn.InstanceNorm2d(channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels, channels, 3, padding=1),\n",
    "            nn.InstanceNorm2d(channels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "\n",
    "class TransformationNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 7, padding=3),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(128, 256, 3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        # Residual blocks\n",
    "        self.residual = nn.Sequential(\n",
    "            ResidualBlock(256),\n",
    "            ResidualBlock(256),\n",
    "            ResidualBlock(256),\n",
    "            ResidualBlock(256),\n",
    "            ResidualBlock(256),\n",
    "            ResidualBlock(256),\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(64, 3, 7, padding=3),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.residual(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# VGG for perceptual loss\n",
    "class VGGFeatures(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        vgg = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1).features\n",
    "        \n",
    "        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
    "        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
    "        \n",
    "        self.slice1 = nn.Sequential(*list(vgg.children())[:4])   # relu1_2\n",
    "        self.slice2 = nn.Sequential(*list(vgg.children())[4:9])  # relu2_2\n",
    "        self.slice3 = nn.Sequential(*list(vgg.children())[9:18]) # relu3_4\n",
    "        self.slice4 = nn.Sequential(*list(vgg.children())[18:27]) # relu4_4\n",
    "        \n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def normalize(self, x):\n",
    "        x = (x + 1) / 2\n",
    "        return (x - self.mean) / self.std\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.normalize(x)\n",
    "        h1 = self.slice1(x)\n",
    "        h2 = self.slice2(h1)\n",
    "        h3 = self.slice3(h2)\n",
    "        h4 = self.slice4(h3)\n",
    "        return [h1, h2, h3, h4]\n",
    "\n",
    "\n",
    "print(\"Models defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3b52dd",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01e511c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from facenet_pytorch import InceptionResnetV1\n",
    "\n",
    "class FaceSwapLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, target_images, content_weight=1.0, style_weight=1e3, \n",
    "                 identity_weight=5.0, tv_weight=1e-6):\n",
    "        super().__init__()\n",
    "        self.vgg = VGGFeatures().to(device)\n",
    "        self.content_weight = content_weight\n",
    "        self.style_weight = style_weight\n",
    "        self.identity_weight = identity_weight\n",
    "        self.tv_weight = tv_weight\n",
    "        \n",
    "        print(\"Loading FaceNet for identity loss...\")\n",
    "        self.facenet = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "        for param in self.facenet.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        print(\"Computing target identity embedding...\")\n",
    "        with torch.no_grad():\n",
    "            target_resized = F.interpolate(target_images, size=(160, 160), mode='bilinear', align_corners=False)\n",
    "            target_normalized = (target_resized + 1) / 2\n",
    "            target_embeddings = self.facenet(target_normalized)\n",
    "            self.target_embedding = target_embeddings.mean(dim=0, keepdim=True)\n",
    "            print(f\"  Target embedding shape: {self.target_embedding.shape}\")\n",
    "        \n",
    "        print(\"Pre-computing target style features...\")\n",
    "        with torch.no_grad():\n",
    "            target_feats = self.vgg(target_images)\n",
    "            self.target_grams = []\n",
    "            for layer_idx in [0, 1]: \n",
    "                gram = self.gram_matrix(target_feats[layer_idx])\n",
    "                avg_gram = gram.mean(dim=0, keepdim=True)\n",
    "                self.target_grams.append(avg_gram)\n",
    "                print(f\"  Layer {layer_idx}: Gram mean={avg_gram.mean().item():.4f}\")\n",
    "        print(\"Target features computed!\")\n",
    "    \n",
    "    def gram_matrix(self, x):\n",
    "        b, c, h, w = x.size()\n",
    "        features = x.view(b, c, h * w)\n",
    "        gram = torch.bmm(features, features.transpose(1, 2))\n",
    "        return gram / (h * w)\n",
    "    \n",
    "    def content_loss(self, output_features, input_features):\n",
    "        loss = 0\n",
    "        for out_feat, in_feat in zip(output_features[2:], input_features[2:]):\n",
    "            loss += F.mse_loss(out_feat, in_feat)\n",
    "        return loss\n",
    "    \n",
    "    def style_loss(self, output_features):\n",
    "        loss = 0\n",
    "        for layer_idx in [0, 1]:\n",
    "            out_gram = self.gram_matrix(output_features[layer_idx])\n",
    "            target_gram = self.target_grams[layer_idx].expand(out_gram.size(0), -1, -1)\n",
    "            loss += F.l1_loss(out_gram, target_gram)\n",
    "        return loss\n",
    "    \n",
    "    def identity_loss(self, output):\n",
    "        output_resized = F.interpolate(output, size=(160, 160), mode='bilinear', align_corners=False)\n",
    "        output_normalized = (output_resized + 1) / 2  \n",
    "        \n",
    "        output_embedding = self.facenet(output_normalized)\n",
    "        target_embedding = self.target_embedding.expand(output_embedding.size(0), -1)\n",
    "        \n",
    "        cos_sim = F.cosine_similarity(output_embedding, target_embedding)\n",
    "        return (1 - cos_sim).mean()\n",
    "    \n",
    "    def tv_loss(self, x):\n",
    "        diff_h = x[:, :, 1:, :] - x[:, :, :-1, :]\n",
    "        diff_w = x[:, :, :, 1:] - x[:, :, :, :-1]\n",
    "        return diff_h.abs().mean() + diff_w.abs().mean()\n",
    "    \n",
    "    def forward(self, output, input_img):\n",
    "        output_features = self.vgg(output)\n",
    "        input_features = self.vgg(input_img)\n",
    "        \n",
    "        c_loss = self.content_loss(output_features, input_features)\n",
    "        s_loss = self.style_loss(output_features)\n",
    "        id_loss = self.identity_loss(output)\n",
    "        tv_loss = self.tv_loss(output)\n",
    "        \n",
    "        total_loss = (self.content_weight * c_loss + \n",
    "                      self.style_weight * s_loss + \n",
    "                      self.identity_weight * id_loss +\n",
    "                      self.tv_weight * tv_loss)\n",
    "        \n",
    "        return total_loss, c_loss.item(), s_loss.item(), id_loss.item()\n",
    "\n",
    "\n",
    "print(\"Loss functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd292e4",
   "metadata": {},
   "source": [
    "# Initialize Model and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad63ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = TransformationNetwork().to(device)\n",
    "\n",
    "criterion = FaceSwapLoss(\n",
    "    target_images=target_images,\n",
    "    content_weight=1.0,\n",
    "    style_weight=100.0,\n",
    "    identity_weight=50.0,\n",
    "    tv_weight=1e-6\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.999))\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Transformation Network has {num_params:,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3c8518",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c63819",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    content_losses = 0\n",
    "    style_losses = 0\n",
    "    identity_losses = 0 \n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch_idx, content_imgs in enumerate(dataloader):\n",
    "        content_imgs = content_imgs.to(device)\n",
    "        \n",
    "        output = model(content_imgs)\n",
    "        \n",
    "        loss, c_loss, s_loss, id_loss = criterion(output, content_imgs)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        content_losses += c_loss\n",
    "        style_losses += s_loss\n",
    "        identity_losses += id_loss\n",
    "        \n",
    "        if (batch_idx + 1) % 50 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"  Batch {batch_idx+1}/{len(dataloader)} - \"\n",
    "                  f\"Loss: {loss.item():.4f} (C:{c_loss:.4f}, S:{s_loss:.4f}, ID:{id_loss:.4f}) - \"\n",
    "                  f\"Time: {elapsed:.1f}s\")\n",
    "    \n",
    "    n_batches = len(dataloader)\n",
    "    return {\n",
    "        'total': total_loss / n_batches,\n",
    "        'content': content_losses / n_batches,\n",
    "        'style': style_losses / n_batches,\n",
    "        'identity': identity_losses / n_batches\n",
    "    }\n",
    "\n",
    "\n",
    "def visualize_results(model, dataloader, target_images, num_samples=4):\n",
    "    \"\"\"Visualize face swap results.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    content_imgs = next(iter(dataloader))[:num_samples].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(content_imgs)\n",
    "    \n",
    "    def denorm(x):\n",
    "        return (x + 1) / 2\n",
    "    \n",
    "    fig, axes = plt.subplots(3, num_samples, figsize=(4 * num_samples, 12))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        axes[0, i].imshow(denorm(content_imgs[i]).cpu().permute(1, 2, 0).clamp(0, 1))\n",
    "        axes[0, i].set_title(\"Input\")\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        axes[1, i].imshow(denorm(output[i]).cpu().permute(1, 2, 0).clamp(0, 1))\n",
    "        axes[1, i].set_title(\"Swapped\")\n",
    "        axes[1, i].axis('off')\n",
    "\n",
    "        target_idx = i % target_images.size(0)\n",
    "        axes[2, i].imshow(denorm(target_images[target_idx]).cpu().permute(1, 2, 0).clamp(0, 1))\n",
    "        axes[2, i].set_title(\"Target Identity\")\n",
    "        axes[2, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    model.train()\n",
    "\n",
    "\n",
    "print(\"Training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff6a605",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history = {'total': [], 'content': [], 'style': [], 'identity': []}\n",
    "\n",
    "EPOCHS = 50\n",
    "\n",
    "print(f\"Training face swap model for {EPOCHS} epochs...\")\n",
    "print(f\"Target identity: {TARGET_CLASS}\")\n",
    "print(f\"Training on {len(celeba_dataset)} CelebA images\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "    \n",
    "    losses = train_epoch(model, celeba_loader, criterion, optimizer, epoch)\n",
    "    scheduler.step()\n",
    "    \n",
    "    for key in history:\n",
    "        history[key].append(losses[key])\n",
    "    \n",
    "    print(f\"  Total Loss: {losses['total']:.4f} | \"\n",
    "          f\"Content: {losses['content']:.4f} | \"\n",
    "          f\"Style: {losses['style']:.4f} | \"\n",
    "          f\"Identity: {losses['identity']:.4f}\")\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        visualize_results(model, celeba_loader, target_images)\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        checkpoint_path = f\"models/faceswap_{TARGET_CLASS}_epoch{epoch+1}.pth\"\n",
    "        os.makedirs(\"models\", exist_ok=True)\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': losses['total'],\n",
    "        }, checkpoint_path)\n",
    "        print(f\"  Saved checkpoint: {checkpoint_path}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5f60b4",
   "metadata": {},
   "source": [
    "# Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992ea9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(history['total'], label='Total Loss')\n",
    "axes[0].set_title('Total Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(history['content'], label='Content Loss', color='green')\n",
    "axes[1].set_title('Content Loss')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "\n",
    "axes[2].plot(history['style'], label='Style Loss', color='orange')\n",
    "axes[2].set_title('Style Loss')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Loss')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd258cb1",
   "metadata": {},
   "source": [
    "# Final Results and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c56b03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "visualize_results(model, celeba_loader, target_images, num_samples=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9165a377",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def swap_single_image(model, image_path, image_size=256):\n",
    "    \"\"\"Apply face swap to a single image.\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    \n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(img_tensor)\n",
    "    \n",
    "    # Denormalize\n",
    "    def denorm(x):\n",
    "        return (x + 1) / 2\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    \n",
    "    axes[0].imshow(denorm(img_tensor[0]).cpu().permute(1, 2, 0).clamp(0, 1))\n",
    "    axes[0].set_title(\"Input\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(denorm(output[0]).cpu().permute(1, 2, 0).clamp(0, 1))\n",
    "    axes[1].set_title(f\"Swapped to {TARGET_CLASS}\")\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    axes[2].imshow(denorm(target_images[0]).cpu().permute(1, 2, 0).clamp(0, 1))\n",
    "    axes[2].set_title(\"Target Identity\")\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "sample_img_path = os.path.join(CELEBA_PATH, celeba_dataset.files[100])\n",
    "swap_single_image(model, sample_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89813c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_model_path = f\"models/faceswap_{TARGET_CLASS}_final.pth\"\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'target_class': TARGET_CLASS,\n",
    "    'image_size': IMAGE_SIZE,\n",
    "    'history': history\n",
    "}, final_model_path)\n",
    "print(f\"Final model saved to: {final_model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
